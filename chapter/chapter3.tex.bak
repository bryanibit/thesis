\chapter{基于无人机航拍图像的三维构建}

\section{引言}
大部分无人车辆行驶依赖环境地图导航与感知\cite{Mattyus-Enhancing-road-maps}，这限制了无人车在未知环境中的行驶。为了解决该问题，当前许多技术利用车载传感器构建大范围地图，例如SLAM(Simultaneous Localization and Mapping) 和第二章提到的用车载Ladybug构建地图的方法。然而车载传感器仅能获取车辆周边环境信息且易被阻挡，而且在野外或灾害地区等环境中，车辆可通行区域变少。然而无人机拥有比无人车更广阔的视野，不受地面障碍物的影响，可以灵活地从空中俯瞰地面。本章介绍了应用无人机构建三维环境地图的方法，为无人车导航提供先验信息。

三维重建涉及到的内容可以分为三部分：稀疏点云的获取，点云稠密化与点云的网格渲染。其中稀疏点云的获取使用的是Structure from Motion(SfM)，在技术层面上与当前热门的Visual-SLAM(VSLAM)比较类似，只是SfM 侧重于三维重建，而SLAM侧重于实时定位，并需要预测下一时刻的位置。SfM可以分为视觉前端与优化后端，前端涉及到的是相机模型，坐标转换，特征提取与匹配以及多视图几何相关的内容，最后得到粗略的相机位姿与3D点坐标，后端涉及到的是优化前端得到的相机位姿与3D点坐标，去除错误的结果等。点云稠密使用的技术称为Multi-View Stereo(MVS)，MVS的方法很多，本文使用的是深度图融合的方法。网格渲染主要用的技术为三角剖分，网格渲染后的三维重建结果比稠密点云存储量小，更易于后续的仿真与处理。
\section{相机模型}
数码相机拍摄的过程中，实际上是一个光学成像的过程，这涉及到摄像机最基本的原件――透镜――的成像原理，如图~\ref{len}所示，这是最基本的透镜成像原理：$Z$是物体距透镜光心的距离，简称物距；$f$是焦距；$b$是相距，即成像平面与透镜光心的距离。三者满足式~\ref{len-equation}
\begin{equation}
\frac{1}{f} = \frac{1}{Z} + \frac{1}{b}
\label{len-equation}
\end{equation}

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter3/focal.png}
	\caption{透镜成像原理}
	\label{len}
\end{figure}
在机器视觉中，利用摄像机可以将三维场景记录在二维图形上，不同的相机模型导致不同的成像效果，比如第二章中Ladybug使用的球面成像模型，使不同相机的拼接简化为绕光心的旋转。但是常见的相机使用的模型还是小孔成像模型，如图~\ref{pinhole}所示。数码相机的镜头相当于一个凸透镜，感光元件就处在这个凸透镜的焦点附近，将焦距近似为凸透镜中心到感光元件的距离时就成为小孔成像模型。
这可以类比为艺术家画一幅画的过程，将眼睛放在图~\ref{pinhole}光心$C$处，在物体与眼睛之间放置一个画布（图~\ref{pinhole}中虚像位置），按照光线直线传播的原则，则物体反射的光线与眼睛的连线交画布一点，如此物体可以在画布上成像。只是相机的成像在光心后面，所以说存在一个虚拟成像平面。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter3/pinhole.png}
	\caption{小孔成像模型}
	\label{pinhole}
\end{figure}

在三维空间中，物体反射的光线经过小孔形成倒立二维图像，根据其数学模型，可以得到
\begin{equation}
\frac{-x}{X} = \frac{f}{Z}
\label{pinhole-equation}
\end{equation}

其中，$f$是小孔到成像平面的距离，即焦距；$Z$是物体距光心的距离，简称物距；$x$是物体在成像平面的投影长度；$X$是物体实际长度。为了简化以上模型，用图~\ref{firstperson}表示小孔成像的等价模型，其中$C$是相机中心，$P(X,Y,Z)$为空间中的3D点，$p(x,y,1)$表示3D点成像在感光元件上的点，小孔成像近似后，成像平面与相机中心的距离为焦距$f$。该图也表示了两个坐标系，分别为图像坐标系和像素坐标系，在下节中详细介绍。
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter3/firstperson.png}
	\caption{小孔成像等价模型}
	\label{firstperson}
\end{figure}

\section{相关坐标系}
从上一节知道，仅仅相机内部就存在多个坐标系，那么摄像机在空间中涉及到的坐标系转换问题将在这节详细探讨。相机成像的过程是一个3D物体显示在2D成像平面的过程，在此涉及物体，真实空间与相机三者的位置关。物体在真实空间的位置即是物体在世界坐标系中的位置坐标表达，在图~\ref{firstperson}中，$(X,Y,Z)$为3D点$P$在相机坐标系（$X_c,y_c,Z_c$）下的坐标表示，这里就存在两个三维坐标的转换，即同一个点在世界坐标系与相机坐标系下表达方式的转换关系，如式\ref{R-t-equation}，该变换可以看做三维坐标系的刚体变换。
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
^{c}P = ^{c}R_w ^w P + ^c T_w
\label{R-t-equation}
\end{equation}
其中$c$表示相机坐标系，$w$表示世界坐标系，$^w P$表示世界坐标系下$P$点3D坐标，$^c P$是相机坐标系下$P$点3D坐标，$^{c}R_w$和$^c T_w$分别表示由世界坐标系到相机坐标系的旋转矩阵和平移矩阵。

将世界坐标系下的3D点表示为齐次坐标$(X,Y,Z,1)$，式\ref{R-t-equation}可以改写为：
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
^{c}P=\left[\begin{array}{ccc}
^{c}R_w & ^c T_w \\
0^{1\times 3} & 1 \\
\end{array}
\right]_{4\times 4}
\left[\begin{array}{ccc}
X_w \\
Y_w \\
Z_w \\
1 \\
\end{array}
\right]
\label{Rt-matrix-equation}
\end{equation}
其中$^{c}R_w$是$3\times 3$矩阵，三个列向量俩俩正交；$^c T_w$是$3\times 1$矩阵，代表两个坐标系原点连线的向量，$^{c}P$坐标表示为$(x_c,y_c,z_c)$。

从图\ref{firstperson}中可以看到成像平面内的二维坐标系$x^\prime ,y^\prime$，该二维坐标系称为图像坐标系，可以推导相机坐标系与图像坐标系（3D到2D）变换的公式表示：
\begin{equation}
x\prime = f\frac{x}{z}
\label{camera2img-equation}
\end{equation}
\begin{equation}
y\prime = f\frac{y}{z}
\label{camera2img-equation}
\end{equation}
改为矩阵表达形式为
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
z_c \left[\begin{array}{ccc}
x\prime \\
y\prime \\
1 \\
\end{array}
\right]=\left[\begin{array}{cccc}
f & 0 & 0 & 0 \\
0 & f & 0 & 0 \\
0 & 0 & 1 & 0 \\
\end{array}
\right]
\left[\begin{array}{ccc}
x_c \\
y_c \\
z_c \\
1 \\
\end{array}
\right]
\label{camera2img-matrix-equation}
\end{equation}

相机在生成图片的过程中，由于成像平面由许多CCD感光元件组成，所以成像的横纵坐标是不连续的，引入像素的概念，同时也存在从图像坐标系到像素坐标系（2D到2D）的转换关系：
\begin{equation}
\left[\begin{array}{ccc}
u \\
v \\
1 \\
\end{array}
\right]=\left[\begin{array}{ccc}
\frac{1}{s_x} & s & p_x \\
0 & \frac{1}{s_y} & p_y \\
0 & 0 & 1 \\
\end{array}
\right]
\left[\begin{array}{ccc}
x\prime \\
y\prime \\
1 \\
\end{array}
\right]
\label{camera2pixel-equation}
\end{equation}
其中，$dx$是u轴方向单像素的宽度，$dy$是v轴方向单像素的宽度，当前大部分ccd的像素宽度，$dx=dy$，只有以前老式电视机使用的是矩形ccd，导致$dx\neq dy$。$p_x,p_y$是主点（图~\ref{firstperson}）与图像左边界的距离与$p_y$ 是主点与图像上边界的距离。$s$是衡量主光轴与成像平面的倾斜程度，如果主光轴垂直于成像平面，则$s = 0$。这些参数被称作相机内参，一般可以通过查询相机手册得到包括焦距在内的相机内参。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=4.0in]{chapter3/transform.png}
	\caption{坐标转换流程图}
	\label{transform}
\end{figure}
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter3/transform-coordinate.png}
	\caption{坐标转换关系图}
	\label{transform-coordinate}
\end{figure}
至此我们可以得到从世界坐标系到像素坐标系变化过程，如图~\ref{transform}和~\ref{transform-coordinate}。
图~\ref{transform}过程用公式表达为~\ref{transform-equation}
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
z_c \left[\begin{array}{ccc}
u \\
v \\
1 \\
\end{array}
\right]=\left[\begin{array}{ccc}
\frac{1}{s_x} & s & p_x \\
0 & \frac{1}{s_y} & p_y \\
0 & 0 & 1 \\
\end{array}
\right]
\left[\begin{array}{cccc}
f & 0 & 0 & 0 \\
0 & f & 0 & 0 \\
0 & 0 & 1 & 0 \\
\end{array}
\right]
\left[\begin{array}{ccc}
^{c}R_w & ^c T_w \\
0^{1\times 3} & 1 \\
\end{array}
\right]_{4\times 4}
\left[\begin{array}{cccc}
X_w \\
Y_w \\
Z_w \\
1 \\
\end{array}
\right]
\label{transform-equation}
\end{equation}
至此我们知道了相机成像过程中某点如何在不同坐标系下转换，从而完成摄像过程。式~\ref{transform-equation}涉及到相机参数$f,p_x,p_y$以及空间变换相关的参数（外参$R,t$），实际上在相机成像过程中除了这些参数，我们还会遇到无法线性建模的参数：像Gopro等鱼眼镜头中，我们可以看到世界中的直线不再是直线，如图。。。。
这些弯曲的线有一些特别的特性：他们都是被关于中心扭曲的 我们称这种畸变为径向畸变。这意味着像素点是沿着从中心放射的径向成比例扭曲的。为了去除相机的畸变，需要对畸变用多项式进行建模，如式~\ref{radial-equation}。

\begin{subequations}
\label{radial-equation}
\begin{align}
u^{dist} &= u(1+k_1 r+k_2 r^2+ k_3 r^3 + ...)  \\
v^{dist} &= v(1+k_1 r+k_2 r^2+ k_3 r^3 + ...)  \\
where \quad & r^2 = u^2 + v^2
\end{align}
\end{subequations}
其中，$k_1,k_2,k_3$是未知参数，需要标定才可以得到，而实际校正径向畸变时一般也只用到2个或3个参数，如图~\ref{distortion}所示，左图为存在径向畸变的图片，右图为经过校正后的图片。
\begin{figure}[!htbp]
	\centering
    \subfigure{
	\includegraphics[width=2.5in]{chapter3/radial-distortion.PNG}
    }
    \subfigure{
	\includegraphics[width=2.5in]{chapter3/non-radial-distortion.PNG}
    }
	\caption{径向畸变的校正}
	\label{distortion}
\end{figure}

在相机校正的过程中，实际上还需要校正图像的切向失真，这是由于摄像机安装时成像平面与透镜没有平行导致的，如果说径向畸变是坐标点距离中心点的长度放生了变化，那么切向畸变就是坐标点与中心点的水平夹角发生了变化。对切向畸变的建模如式~\ref{tangential-equation}，一般使用$p_1,p_2$两个参数。
\begin{subequations}
\label{tangential-equation}
\begin{align}
u^{dist} &= u + 2p_1 uv + p_2(r^2 + 2u^2) \\
u^{dist} &= u + 2p_2 uv + p_1(r^2 + 2v^2) \\
where \quad & r^2 = u^2 + v^2
\end{align}
\end{subequations}

综上所述，本节详细介绍了相机模型与相机相关坐标系转换关系，以及相机成像的校正等相关内容，这部分属于图像处理的基础，熟悉该部分内容将为下面几节多视图几何相关内容作铺垫。以下3.4-3.7节的内容概况了本文SfM的主要流程，如图~\ref{flow-sparse}所示，输入为无序单目航拍图像，输出为稀疏点云与相机位姿，以下将分步介绍各个环节。

\begin{figure}[H]
\centering
\includegraphics[width=5.4in]{chapter3/flow-sparse.png}
\caption{SfM流程图}
\label{flow-sparse}
\end{figure}

\section{特征点提取}
SfM首先需要找到图片集中俩俩图片间的几何关系，这样才能估算相机间的运动与成像的三维点坐标。为了找到图片之间的几何关系，现今有许多方法，例如光流法，特征点法等，光流法适用于匀速运送的视频流中，要求帧间运动比较小，从而找出帧间运动关系；特征点法指对图像特征的提取与存储，是图像中比较有代表性的特征。由于本文使用的数据集为无序的航拍图像，特征点法较为适宜使用。特征点提取更通俗的理解为将二维矩阵描述的图像降维为一维特征点描述的图像，突出图像的重点，去除冗余信息的过程。

最简单的图像特征为图像的角点，角点就是图像中线的交点，例如大厦最高处的顶点。两幅图像的角点不会随着图像移动而改变，但是角点会随着离相机的距离太近而变得无法识别，所以许多研究者设计了精巧而互有优劣的特征检测与描述方法，比较著名的有SIFT(Scale Invariant Feature Transform)\cite{Lowe-DistinctiveImageFeatures}，SURF\cite{Bay-Speeded-UpRobustFeatures}，ORG\cite{Rublee-ORB:Anefficient}等。相对于简单的角点，这些特征检测具有以下优点\cite{slam14}:
\begin{itemize}
\item 可重复：相同的特征可以在不同的图片中找到
\item 可区别：不同的特征有不同的表示
\item 高效率：特征点数远远小于像素数
\end{itemize}

实际上，特征点可以分为局部特征点和全局特征点，像上面举例的三个特征点实际上是局部特征点，局部特征点顾名思义就是基于图像的突出区域明显的区分图像的特征，一般而言，局部特征需要具有旋转不变性，光线明暗的鲁棒性等。因此图像可以被提取到的一系列称为感兴趣区域的特征描述。相反全局特征是将图像表示为一个向量，向量的值为图像的各个方面，如颜色，纹理或形状。例如像区分图像，一些是海洋，一些是森林，一个全局颜色描述子可以很好的对其归类。在这种情况下，全局描述子涉及图像所有像素的特定属性，这个属性可以是颜色直方图，纹理，边缘等。如图~\ref{global-local}所示，使用哪种特征描述子取决于图像处理的情景。由于地理空间的航拍图像在颜色或者纹理等全局特征上，图片的相似度极高，不易辨识，难以找到图片之间的匹配关系，所以此文选用局部特征描述子：SIFT。
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/global-local.jpg}
\caption{全局特征与局部特征描述}
\label{global-local}
\end{figure}

特征点提取包括特征点检测与特征点描述两部分，在此以SIFT特征点为例，简单介绍一下检测子与描述子的使用方法。检测子有很多，使用较多的有Harris Detector\cite{Harris-corner}，FAST Detector\cite{Rosten-FAST}。Harris检测子结合边缘与角点检测方法得到图像不同方向的自相关系数变化率，例如强度变化率等，该方法将局部特征描述为对称的自相关系数矩阵。Fast检测子考虑某像素周围一定大小的圆，院上像素强度与该像素比较，如果大于或小于某一阈值则定义该像素为角点，这两种检测子对尺度变化都不具有不变性，不适合直接应用在拍摄高度不同的航拍图形的特征检测中。而SIFT作为一种具有尺度、放缩不变，对不同光线鲁棒的特定，很适合应用于环境复杂的航拍图形的特征特取与描述中。SIFT算法由四个步骤组成：尺度空间极值检测，兴趣点（关键点）定位，方向计算以及兴趣点描述。第一个阶段是使用Difference of Gaussian(DoG)确认潜在的兴趣点，这里使用DoG代替Laplacian of Gaussian (LoG)算子以提高计算速度。

Laplacian of Gaussian (LoG)是二阶导数的线性组合，也是检测斑点(blob)的检测子，给出一个图片$I(x,y)$,尺度空间$L(x,y,\sigma)$是由图片$I(x,y)$与不同大小的高斯核$G(x,y,\sigma)$卷积得到的，表示为

\begin{equation}
L(x,y,\sigma) = G(x,y,\sigma)*I(x,y)
%\label{laplacian}
\end{equation}
其中，
\begin{equation}
G(x,y,\sigma) = \frac{1}{2\pi \sigma ^2}e^{\frac{-(x^2 + y^2)}{2 {\sigma}^2}}
%\label{laplacian}
\end{equation}

据此可计算Laplacian算子为
\begin{equation}
{\nabla}^2 L(x,y,\sigma)=L_{xx}(x,y,\sigma)+L_{yy}(x,y,\sigma)
%\label{laplacian}
\end{equation}

这对于大小为$\sqrt{2\sigma}$的斑点有最强的响应，然而该算子严重依赖图像中斑点大小与用来平滑图像的高斯核大小，即大小不同的高斯核函数被用来寻找对应大小的blob。为了在一幅图像中自动检测不同大小的斑点，一种多尺度的方法被提出来\cite{Lindeberg-Feature detection}，其通过尺度不同的归一化的Laplacian算子对图片进行平滑从而在不同尺度空间找到大小不同的blob，尺度归一化的高斯核函数为式~\ref{nor-lap-equation}。

\begin{equation}
{\nabla}^{2}_{norm} L(x,y,\sigma)={\sigma}^2 L_{xx}(x,y,\sigma)+L_{yy}(x,y,\sigma)
\label{nor-lap-equation}
\end{equation}

值较大的$\sigma$对应图片的粗略特征，较小$\sigma$对应图片的精细特征。同时LoG算子是对称的，所以它具有blob旋转对其没有影响，但是LoG算子耗费计算资源，为了加速计算并在尺度空间检测到稳定的兴趣点，Lowe\cite{Lowe-DistinctiveImageFeatures}提出了高斯差分尺度空间，简称DoG scale-space。如式~\ref{gaussian}所示，利用不同尺度的搞死差分核函数与图像卷积生成。
\begin{equation}
\begin{split}
D(x,y,\sigma) &= (G(x,y,k\sigma)-G(x,y,\sigma)*I(x,y)) \\
&= L(x,y,k\sigma)-L(x,y,\sigma)
\label{gaussian}
\end{split}
\end{equation}

计算DoG的关键步骤是构建图像金字塔：对于某个图像，使用大小不同的高斯核与该图像卷积，可以得到模糊程度不同的但是图像长宽一致的图像，这些图像构成了一个八度(octave)，而后对这一系列图像进行降采样，降采样就是对octave中的图像隔行隔列采集像素，最后图像尺度(scale)变为原来大小的$\frac{1}{4}$，构成下一个octave。这样就避免了对不同尺度空间下的图像进行卷积，减少了计算。为了寻找尺度空间的极值点，在DoG尺度空间中，每个采样点要和其周围所有相邻点（8邻域中，一个像素点周围共有26个像素点）比较，若该点是极大值或者极小值则定位为兴趣点。整个过程可以用图~\ref{octave}描述。
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.4in]{chapter3/octave.jpg}
\caption{图像金字塔的构建}
\label{octave}
\end{figure}

检测到特征点后，需要定量的表示各个特征点，以为特征点匹配做准备。Lowe在其论文中提出用一个128维度的向量表示每个特征点，具体做法为：如图~\ref{siftDescriptor}，首先对每个关键点周围的$16\times 16$ 邻域内所有像素计算其梯度的幅值与方向，用直方图统计邻域像素的梯度方向，梯度直方图将$0~360^{\circ}$分为8个区间，对得到的直方图进行高斯平滑，以减少突变带来的影响。选取直方图的峰值作为该关键点处邻域梯度的主方向。所以在每个$4\times 4$的象限内，将每个像素的主方向加权到直方图的8个方向区间中的一个，计算一个梯度方向直方图，这样对于每个特征点可以形成$4\times 4 \times 8 = 128$ 维的描述子。

%%在此需要插入4张sift检测的图片，甚至是sift与surf特征点的比较框

至此基本介绍了本文使用的SIFT特征点提取的方法，详细介绍了特征的提取与描述方法，以上是使用SIFT特征点检测多张图像的结果，其中特征点主方向以及特征点的强度分别表示在图中的。。。
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/siftDescriptor.jpg}
\caption{SIFT描述子的构建}
\label{siftDescriptor}
\end{figure}


\begin{figure}[!htbp]
\centering
 \subfigure{
	\includegraphics[width=2.6in]{chapter3/testPho/1_res.png}
    }
 \subfigure{
	\includegraphics[width=2.6in]{chapter3/testPho/2_res.png}
    }
 \subfigure{
	\includegraphics[width=2.6in]{chapter3/testPho/3_res.png}
    }
 \subfigure{
	\includegraphics[width=2.6in]{chapter3/testPho/4_res.png}
    }
%\includegraphics[width=5.0in]{chapter3/siftDescriptor.jpg}
\caption{SIFT特征点检测结果}
\label{sift-result}
\end{figure}

\section{特征点匹配}
上面提到本文使用SIFT特征点作为关键点检测的方法，特征点提取并描述后，就应该找出各个图像中以特征点为依据的关系，即每张图像中的特征点如何与其他图像的特征点进行匹配，找到相近的特征点并去除错误的匹配的特征点。特征点匹配解决航拍图像之间的数据关联问题，将无序的图片数据集建立关联关系，为下文通过图像之间的关系，计算相机位姿与还原特征点的三维坐标提供基础。很好理解，一对匹配点就是实际空间的一个点，如果将所有图片对应同一个真实点的特征点连在一起，将使数据集中所有无序图片“有序化”。相同的，如果匹配点中有大量的错误匹配，那对后续的位姿估计与还原三维坐标都产生不利影响，所以如果去除错误匹配更是至关重要的问题。下面就这两方面展开讨论。

假设在图片集$I = \{I_i | i=1...N_I\}$中找到的局部特征点为$F_i=\{(x_j,f_j) | j=1...N_{F_i}\}$，其中$x_j$表示特征点位置坐标，$f_j$表示特征点描述子，如果是SIFT特征点，则$f_j$是128维向量。对于$F_i$与$F_j$的特征点匹配的方法，最简单的是使用暴力匹配，即每个$(x_j,f_j) | j=1...N_{F_i}$与$(x_j,f_j) | j=1...N_{F_j}$中特征点测量两个向量之间的距离，需要进行$N_{F_i} \times N_{F_j}$次距离比较。对于像SIFT这样的浮点描述子，一般采用欧拉距离作为衡量依据，而对于像BRIEF（ORB特征使用的描述子）二进制描述子，则一般使用汉明距离作为度量依据。

%插入一组SIFT匹配的图片，对应欧拉距离那句话

但是当图片长宽较大，分辨率较高时，图片特征点会很多，这时采用暴力匹配会导致效率很低，一般实际使用中采用近似最近邻(Approximate Nearest Neighbor, ANN)匹配方法。事实上，无论SIFT特征匹配还是数据库检索本质上是相同的，都是使用距离函数在高维矢量空间中寻找相近对象的过程。常用的方法试K近邻查询，设置查询点与正整数K，从另一个数据集中根据距离公式找到距离最近的K个数据，如果K=1，改方法变为最近邻查询算法。K近邻查询算法是通过构建KD-Tree或者R-Tree等实现的。在此不做详细介绍。

在此简单介绍近似最近邻的快速库（Fast Library for Approximate Nearest Neighbors, FLANN），FLANN具有一种内部机制，该机制可以根据数据本身选择合适的算法来处理数据集，基于FLANN的匹配非常准确、快速。FLANN在使用的时候需要配置两个参数：indexParams和searchParams。使用FLANN时，indexParams可以选择为LinearIndex、KTreeIndex、KMeansIndex、CompositeIndex和AutotuneIndex，其中KTreeIndex灵活且可被并行处理。对于searchParams，用来指定索引树被遍历的次数，即check值，该值越大则匹配的时间越长，当然也越准确。

当然即使用上面的方法，也无法保证所有的匹配都是正确的，Lowe\cite{Lowe-ObjectRecognition}在1999提出一种简单的方法可以剔除大部分的错误匹配。对于某个特征点向量，如果查询数据集中与其距离最近的特征点向量与次近的特征点向量的距离之比大于0.7，可以避免接近90\%的错误匹配，但是正确的匹配也因此变少。使用FLANN并采用Lowe提出的方法可以去掉大部分的错误匹配，图~\ref{matches-single}展示了随机选取10个匹配对的结果。
\begin{figure}[!htbp]
\centering
\includegraphics[width=4.5in]{chapter3/matches/matches.jpg}
\caption{FLANN匹配}
\label{matches-single}
\end{figure}

以上提到的方法，匹配后仍然存在错误的匹配点，由于刚刚的匹配是单纯基于特征描述子的向量值，无法保证匹配的两个特征点对应相同的场景点。因此SfM使用投影几何的知识估计两张图像的特征点变换。依赖图像对的空间配置，不同投影描述图片间不同的几何关系。例如，单应性变换（又称射影变换\cite{Richard-MultipleView}）描述相机拍摄的二维图片之间纯旋转和平移变换。对极几何通过基本矩阵（Essential matrix）$E$和基础矩阵（Fundamental matrix）$F$ 描述移动相机的关系，并可以通过三焦张量扩展到三视图的关系。无论哪种变换关系，有效的变换可以满足大部分匹配点的几何关系，那么这种变换或者匹配点被认为是有效的。本文在删除错误匹配时，使用的是单应性变换，一下简单介绍一下单应性变换的详细内容。

单应性又称射影变换、保线变换，与坐标系无关。映射$h$是射影变换的充要条件是：存在一个$3\times 3$非奇异矩阵$H$，使得任意一个矢量$X$表示的坐标点都满足$h(x)=HX$。即
\begin{equation}
\left[\begin{array}{ccc}
x_{1}^{\prime} \\
x_{2}^{\prime} \\
x_{3}^{\prime} \\
\end{array}
\right]=\left[\begin{array}{ccc}
h_{11} & h_{12} & h_{13} \\
h_{21} & h_{22} & h_{23} \\
h_{31} & h_{32} & h_{33} \\
\end{array}\right]
\left[\begin{array}{ccc}
x_{1} \\
x_{2} \\
x_{3} \\
\end{array}\right]
\label{homography-equation}
\end{equation}

矩阵$H$是一个齐次矩阵，在$H$的九个元素中有八个独立比率，即交比(cross radio)$h_{11}:h_{12}:h_{13}:h_{21}:h_{22}:h_{23}:h_{31}:h_{32}:h_{33}$不会因为$H$因为乘以非零因子而变化，所以单应性变化有八个自由度，而每个二维特征点可提供2个方程，故需要解算$H$需要至少4对匹配点。已知4对匹配点，使用直接线性法解算$H$，示意图如图~\ref{homography}，已知两个视图四对匹配的特征点坐标$X_1,X_2$，求H 的过程。
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/homography.png}
\caption{单应性变换}
\label{homography}
\end{figure}
线性变换表示为$X_2=HX_1$，这是齐次矢量方程，三维矢量$X_2$和$HX_1$不相等，实际可以写为$\lambda X_2 = HX_1$，线性变换可以变形为$X_2 \times HX_1=0$（矢量与自身外积为零），记为$[X_2]_\times HX_1=0$，设$h_i$为$H$的第$i$ 行，

\begin{equation}
\begin{split}
\left[\begin{array}{ccc}
u_2 \\
v_2 \\
1 \\
\end{array}
\right]_\times
\left[\begin{array}{ccc}
h_1 \\
h_2 \\
h_3 \\
\end{array}\right]X_1 &=
\left[\begin{array}{ccc}
u_2 \\
v_2 \\
1 \\
\end{array}
\right]_\times
\left[\begin{array}{ccc}
h_1 X_1 \\
h_2 X_1 \\
h_3 X_1 \\
\end{array}\right]=
\left[\begin{array}{ccc}
u_2 \\
v_2 \\
1 \\
\end{array}
\right]_\times
\left[\begin{array}{ccc}
X_1^T h_1^T \\
X_1^T h_2^T \\
X_1^T h_3^T \\
\end{array}\right] \\
&=
\left[\begin{array}{ccc}
0 & -1 & v_2 \\
1 & 0 & -u_2 \\
-v_2 & u_2 & 0 \\
\end{array}\right]
\left[\begin{array}{ccc}
X_1^T & 0_{1\times 3} & 0_{1\times 3} \\
0_{1\times 3} & X_1^T & 0_{1\times 3} \\
0_{1\times 3} & 0_{1\times 3} & X_1^T \\
\end{array}\right]
\left[\begin{array}{ccc}
h_1^T \\
h_2^T \\
h_3^T \\
\end{array}\right]
\end{split}
\label{homography-tuidao-equation}
\end{equation}
最后一步是由线性代数的知识得到，即对3维矢量$x=(x_1,x_2,x_3)$的$3\times 3$的反对称矩阵为式~\ref{inverse-sym-equation}
\begin{equation}
[a]_\times=\left[\begin{array}{ccc}
0 & -a_3 & a_2 \\
a_3 & 0 & -a_1 \\
-a_2 & a_1 & 0 \\
\end{array}\right]
\label{inverse-sym-equation}
\end{equation}

合并~\ref{homography-tuidao-equation}最后一步的前两项得到下式为
\begin{equation}
=\left[\begin{array}{ccc}
0_{1\times 3} & -X_1^T & v_2 X_1^T \\
X_1^T & 0_{1\times 3} & -u_2 X_1^T \\
-v_2 X_1^T & u_2 X_1^T & 0_{1\times 3} \\
\end{array}\right]
\left[\begin{array}{ccc}
h_1^T \\
h_2^T \\
h_3^T \\
\end{array}\right]=0
\label{inverse-sym-equation}
\end{equation}

上式简写为$Ab=0$，其中$A$为$3\times 9$矩阵，$rank(A)=2$，这是因为每队匹配点只提供两个量：$u$和$v$，也可以从$A$矩阵的值看出，将$A$的第一行乘以$u_2$与第二行乘以$v_2$相加得到第三行。$b$为$9\times 1$ 矩阵，由于只需要维持$b$ 各项的交比不变即可，所以有8 个自由度，故需要4 对匹配点即可在这个线性系统上获得足够的条件计算得到单应性矩阵$H$的各项。由以上分析我们可以知道当给定的匹配点为4个时，并将$||H||=1$，方程有精确解，。但是大部分的匹配问题匹配点的数目远不止4 对，如果多于4对，那么$Ab=0$是超定的，如果所有的匹配点的位置是精确的，那么$Ab=0$的解仍然是精确的，但是通常情况匹配点中有噪声，存在错误匹配等问题，所以这时候方程存在最小二乘解（超定解）。除了零解可以忽略外，方程解可以使用SVD找到其近似解。

SVD又称奇异值分解，在此不详细介绍，SVD可以将一个矩阵，无论是否满秩，均可以分解为$A=UDV^T$形式，其中$UU_T=I$和$V V_T=I$，即$U$和$V$是正交矩阵，$D$是对角阵，对角元素非负。实际上SVD可以看做特征值分解的推广，SVD在主成分分析，求解方程的最小二乘解中广泛应用。回到求解单应性变换的问题来，根据SVD求解最小二乘解的原理，可以得到：最小二乘解是$A^T A$的最小特征值的特征向量。具体讲，假设有n个匹配点，则$A$为$2n\times 9$的矩阵，且$A=UDV^T$，且$D$对角阵元素按降序排列，那么$b$是$V$的最后一列。这是由于$Ab=0$的解，即$A$的零空间，又由于$rank(A)=n-1$，所以$A$的零空间只有一个基向量，即SVD中最小特征值对应的特征向量$V$的最后一列。

奇异值分解是广泛用于图像处理中的一种算法，不仅在求单应性矩阵时使用，下文亦有涉及。但是这里要介绍一种本文使用到的另一种方法――RANSAC(Random Sample Consensus)，该方法在图像领域中也广泛应用，其根本原理是使用统计原理，可以通过多次迭代估计含有噪声的数据模型的参数。RANSAC中文译为随机抽样一致性，其一般过程为从一组含有外点（噪声）的数据中随机选取数个数据，估计模型参数，然后将估计得到的模型应用于剩余的其他数据，从而得到该模型的误差，重复以上过程，直到误差达到某设定阈值或迭代次数达到设定最大次数，停止迭代，选取最优的一组模型作为最终结果。根据最终结果与事先设定的阈值可以过滤一部分外点并得到较为精确的模型。由以上内容知，估计单应性变换需要4对匹配点即可计算一个单应性变换，所以RANSAC每次选取4对匹配点迭代计算得到单应性矩阵，而后将单应性矩阵应用于剩余的匹配点，计算误差，过滤外点。如图~\ref{Homography+RANSAC}所示，其中$a,b$是仅根据SIFT描述子通过最近邻匹配过滤的结果。$c,d$是使用RANSAC方法通过单应性变换过滤的结果，从图中可以直观看出使用RANSAC可以过滤掉一部分错误的匹配，同样从表~\ref{table:matches}中也可以看出，其中对角数字表示图~\ref{Homography+RANSAC}中三张图片提取到的SIFT特征点数量，其他表格数目，例如$55/32$表示使用RANSAC方法前得到的匹配点数目，以及使用RANSAC后过滤剩下的匹配点。

\begin{figure}[!htbp]
\centering
 \subfigure[]{
	\includegraphics[width=2.95in]{chapter3/matches/000400_H_000415.png}
    }
\subfigure[]{
	\includegraphics[width=2.95in]{chapter3/matches/000400_U_000415.png}
    }
 \subfigure[]{
	\includegraphics[width=2.95in]{chapter3/matches/000400_H_000424.png}
    }
\subfigure[]{
	\includegraphics[width=2.95in]{chapter3/matches/000400_U_000424.png}
    }
 \subfigure[]{
	\includegraphics[width=2.95in]{chapter3/matches/000415_H_000424.png}
    }
 \subfigure[]{
	\includegraphics[width=2.95in]{chapter3/matches/000415_U_000424.png}
    }
%\includegraphics[width=5.0in]{chapter3/siftDescriptor.jpg}
\caption{RANSAC筛选的单应性变换结果}
\label{Homography+RANSAC}
\end{figure}

\begin{table}[!htbp]
	\caption{匹配点过滤}
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		\diagbox[dir=SE]{\bfseries 图片名}{\bfseries 特征点}{\bfseries 图片名} & \bfseries KITTI-0400 & \bfseries KITTI-0415 & \bfseries KITTI-0424\\
		\hline
%		\hline
		\bfseries KITTI-0400&  $861$&  $55/32$ & $75/46$\\
		\hline
		\bfseries KITTI-0415 & $ 空 $ & $370$ & $19/10$ \\
		\hline
        \bfseries KITTI-0424 & $空$ & $空$ & $599$ \\
		\hline
	\end{tabular}
	\label{table:matches}
\end{table}

经过各种方法，得到的图片匹配点基本上是正确的，基于此我们可以对数据集中无序图片构建它们的关联结构，对于多张图片对应的同一个匹配点，设置统一标号，称为track。例如图片一中第125个特征点，图片二中第259个特征点，图片三中第6个特征点...匹配，实际上它们对应实际地理空间的同一个点，所以可以使用同一个track序号标记这些匹配点，这样做使整个数据集通过特征点匹配构成关联关系，为下面的多视图重建打下基础。

\section{多视图重建}
本节内容主要介绍如何通过多视图几何的只是，恢复上节匹配的特征点的三维空间结构，以及摄像机位姿等相关信息，本节内容采用增量式SfM的方法，首次从track点最多的两张图片入手，恢复两视图匹配特征点的三维空间位置与两视图对应的摄像机的相关关系，而后加入第三张图片，计算第三张图片与前两张图片得到的3D点的关系，使用三角测量方法还原第三张图片的特征点3D位置坐标，如此循环。其中每加入一张图片使用光束平差法（Bundle Adjustment，简称BA）优化3D点与相机位姿。最终得到较为理想的点云信息。

\subsection{两视图重建}
在式~\ref{transform-equation}中，$z_c$为每个像素的深度，在单目成像时，$z_c$是未知的。所以当知道某点的世界坐标系时，可以很容易得到该点的像素坐标，但是反之无法已知像素坐标得到世界坐标。所以从单张图像中无法还原场景的三维结构，所以下文介绍从两张存在特征点匹配的图片中恢复场景的三维结构。两视图几何最核心的原理是对极约束，在阐述堆积约束相关知识之前，我们先复习一下3.3节中关于摄像机成像过程中涉及到的相关坐标系。如果三维空间的一个点$X$，其在相机所成的像坐标为$x$，则有$\lambda x = PX=k[R\quad t]X$。其中$\lambda$表示每个像素的深度；$[R\quad t]$，表示摄像机坐标系与世界坐标系刚体变换。了解这些后下面将详细阐述对极约束的相关知识。如图~\ref{epipolor} 所示，其中$C_1,C_2$ 表示两个摄像头同时看到三维世界中同一点，该点在$C_1$为原点的坐标系的三维坐标为$X_1$，在$C_2$ 为原点的坐标系的三维坐标为$X_2$。在该对极几何中，世界坐标原点选为$C_1$，坐标轴与该相机坐标系相同，所以对于$C_1$表示的相机成像矩阵$P=k[I\quad 0]$，而对于$C_2$表示的相机成像矩阵$P=k[R\quad t]$，所以$X_2=RX_1 +t$，表示$C_1$为原点的坐标系的点经过$R,t$可以转换到$C_2$为原点的坐标系表示的过程。在$C_2$为原点的坐标系中，我们可以得到如图~\ref{epipolor}中红色三角形三边对应的的矢量为$t,X_2,X_2 -t$，其中$X_2 -t = RX_1$，由于三边共面有向量的混合积如式~\ref{epipolor-equation}所示，当然式中$[t]_\times X_2 = t\times X_2$表示垂直于图~\ref{epipolor}红色平面的向量，由绿色箭头显示。最后我们可以得到简洁的形式$0 = -X_2^T EX_1$，其中$E=[t]_\times R$，$E$被Longuet Higgins称为本质矩阵，他首先发现的这一关系。本质矩阵是$3\times 3$的矩阵，它将旋转与平移的复杂关系转换为简单的形式，下文将介绍如何计算$E$与如何从$E$中恢复旋转、平移信息。

这里还有几个概念需要介绍一下，所有由3D点，相机中心$C_1,C_2$组成的平面交成像平面于两条直线，如图~\ref{epipolor}中蓝色直线所示，$C_1$成像平面中一点$\lambda X_1$对应的极线为红色平面与$C_2$成像平面的交线，即蓝色线$Fx_1$。如果改变3D点的位置形成另外一个红色平面，其与成像平面的交线为另外一条极线，所有形成的极线交于一点，该点称为极点，如图中$e_1,e_2$，所示。从极线约束的关系看$0 = -X_2^TEX_1$，结合齐次坐标的性质点$x$在直线上的充要条件是$x^T L=0$，所以$EX_1$是过$X_2$的直线，也就是与$C_2$成像平面的极线平行的直线，由于所有的极线$Ex_1$一直通过同一个点（极点$e_2$），所以$e_2^T E = 0, E e_1=0$。极点与极线的关系在图~\ref{epipolor-line}中展示出来，图a中粉色点表示选取的两个特征点，而图b中对应的图a两点的两条极线标注为红色，这两条线的交点为极点。

\begin{figure}[!htbp]
\centering
\includegraphics[width=5.4in]{chapter3/epipolor.png}
\caption{对极几何约束}
\label{epipolor}
\end{figure}

\begin{equation}
\begin{split}
0 =(X_2-t)^T\cdot [t]_\times X_2 = (RX_1)^T\cdot [t]_\times X_2 &= X_1^TR^T[t]_\times X_2=-X_2^T[t]_\times RX_1 = -X_2^T EX_1 \\
0 &=-X_2EX_1 \\
\end{split}
\label{epipolor-equation}
\end{equation}

\begin{figure}[!htbp]
\centering
\subfigure[a]{
\includegraphics[width=5.4in]{chapter3/epipolorLine/points.png}
}
\subfigure[b]{
\includegraphics[width=5.4in]{chapter3/epipolorLine/lines.png}
}
\caption{极点与极线的关系}
\label{epipolor-line}
\end{figure}

已知$KX_1 = x_1$，其中$K$为内参矩阵，$X_1$表示$C_1$为原点的坐标系中三维点坐标，$x_1$表示$C_1$的像素坐标系中二维点坐标。这里和上文相同都是其次坐标，所以是在齐次意义上相等。带入$-X_2^TEX_1=0$，中有式~\ref{fundunmental-equation}所示，其中$F$称为基本矩阵，$rank(F) = 2$，$F$的自由度为8。根据式~\ref{fundunmental-equation}，一对匹配点可以得到一个方程，所以至少需要8对匹配点才能计算得到矩阵$F$。如式~\ref{fundunmental-tuidao-equation}所示，可以简写为$AX=0$，与上文计算单应性矩阵的方法相同，由于匹配点远不止8对，所以此时方程存在最小二乘解，使用SVD对左边矩阵分解得$UDV^T$，则基本矩阵即为$V$的最后一列重新排列成$3\times 3$的形式，当然最后还需要保证$rank(F)=2$，这可以通过将$F$进行SVD分解并使最小奇异值为零得到秩为2的$F$。至此我们可以根据至少8对匹配点解算出基本矩阵$F$，该方法称为八点法，在两视图重建中被广泛使用。另外一种方法是使用RANSAC代替SVD分解，避免噪声带来的影响。

\begin{equation}
\begin{split}
X_2^TEX_1 &=0 \\
x_2^Tk^{-T} E k^{-1} x――1 &=0 \\
x_2^TFx_1 &=0 \\
\end{split}
\label{fundunmental-equation}
\end{equation}

基于式~\ref{fundunmental-equation}可以得到下式
\begin{equation}
\left[\begin{array}{ccc}
u_i^1 & v_i^1 & 1
\end{array}\right]
\left[\begin{matrix}
f_{11} & f_{12} & f_{13} \\
f_{21} & f_{22} & f_{23} \\
f_{31} & f_{32} & f_{33}
\end{matrix}\right]
\left[\begin{array}{c}
u_i^1 \\
v_i^1 \\
1
\end{array}\right] =0
\label{fundunmental-tuidao-equation}
\end{equation}

\begin{equation}
\left[\begin{matrix}
u_1^1u_1^2 & u_1^1v_1^2 & u_1^1 & v_1^1u_1^2 & v_1^1 & u_1^2 & v_1^2 & 1 \\
u_2^1u_2^2 & u_2^1v_2^2 & u_2^1 & v_2^1u_2^2 & v_2^1 & u_2^2 & v_2^2 & 1 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
u_8^1u_8^2 & u_8^1v_8^2 & u_8^1 & v_8^1u_8^2 & v_8^1 & u_8^2 & v_8^2 & 1 \\
\end{matrix}\right]
\left[\begin{array}{c}
f_{11} \\
f_{21} \\
f_{31} \\
f_{12} \\
f_{22} \\
f_{32} \\
f_{31} \\
f_{32} \\
f_{33}
\end{array}\right] =0
\label{fundunmental-tuidao-equation}
\end{equation}

在得到基本矩阵$F$后，根据式~\ref{fundunmental-equation}以及已知相机内参矩阵$K$的情况下，可以得到本质矩阵$E$，从本质矩阵可以恢复相机之间的旋转平移关系。
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/essentialMatrix.png}
\caption{本质矩阵恢复旋转平移关系}
\label{essentialMatrix}
\end{figure}

由于$P_2 \left[\begin{matrix}
0 \\
1
\end{matrix}\right]=[R | t]\left[\begin{matrix}
0 \\
1
\end{matrix}\right] = t$，所以$t^TE=0$，由SVD知$t$是$E$的左零空间，也是第二视图极点$e_2^T E=0$，所以$t=U[:,-1]$，即$E$svd分解后$U$的最后一列，即$t=-u_3,u_3$

\begin{equation}
\begin{split}
& E=U \left[\begin{matrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{matrix}\right]
V^T=[t]_\times |R=U
\left[\begin{matrix}
0 & 1 & 0 \\
-1 & 0 & 0 \\
0 & 0 & 0 \\
\end{matrix}\right]
U^T | UYV^T \\
& where \quad
U=\left[\begin{array}{ccc}
u_1 & u_2 & t \\
\end{array}\right]
\label{Rt-fromE-equation}
\end{split}
\end{equation}

据此我们可以得到

\begin{equation}
\left[\begin{matrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{matrix}\right]
=
\left[\begin{matrix}
0 & 1 & 0 \\
-1 & 0 & 0 \\
0 & 0 & 0 \\
\end{matrix}\right]
Y
\label{result-y-equation}
\end{equation}

所以可以得到矩阵$Y$的值，$Y / Y^T=\left[\begin{matrix}
0 & 1 & 0 \\
-1 & 0 & 0 \\
0 & 0 & 0 \\
\end{matrix}\right]$所以$R$有两种可能值，这是因为$R=UYV^T$。

\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/essentialfromRT.png}
\caption{分解本质矩阵得到的四个解}
\label{essentialfromRT}
\end{figure}

综上通过$E$恢复旋转矩阵$R$和平移矩阵$t$有四种可能的解，图~\ref{essentialfromRT}形象地显示了分解本质矩阵得到四个解的过程。保持成像平面的点（红点）不变的情况下，可以画出四种可能的情况，但是只有(a)是正确的，因为其有正深度，所以恢复旋转与平移后需要将3D点带入图中四个解中，检测该点在两个相机中的深度，即可确定$R$和$t$。实际上利用$E$的内在性质，其只有五个自由度：旋转(3)，平移(1)，相机中心(1)。但是使用五点法形式复杂，所以本文还是采用上文介绍的八点法计算旋转与平移关系。


\subsection{三角定位法}
在得到相机的位姿后，需要恢复匹配点的三维坐标信息，该方法称为三角测量（triangulation），三角测量时指不同视角观察同一个点的夹角，确定该点的位置。三角测量最早由高斯提出并应用于天文学中。在SfM中，我们使用三角测量估计匹配点的像素深度。

\begin{figure}[!htbp]
\centering
\includegraphics[width=5.2in]{chapter3/triangulation.png}
\caption{三角测量示意图}
\label{triangulation}
\end{figure}

小孔相机成像模型简化为$\lambda \left[\begin{array}{c}
x_1\\
1
\end{array}\right]=P_1\left[\begin{array}{c}
X_1\\
1
\end{array}\right]$，即$\left[\begin{array}{c}
x_1\\
1
\end{array}\right]_\times P_1\left[\begin{array}{c}
X_1\\
1
\end{array}\right]=0$，由于一个视图可以提供两个方程，所以只需要两个视图即可计算得到3D点坐标，但是由于噪声的影响，空间中的两视图射线不一定交于一点，所以使用两视图恢复出来的三维点坐标存在不准确的问题，所以一般情况下使用多视图进行三角测量恢复多视图匹配的3D点坐标，如图~\ref{triangulation}所示，多个视图匹配的同一个3D点在不同视图成像平面的投影为$x_1,x_2,...x_n$，这些投影点具有相同的track序号（第三章第二节末提到的定义），当从多视图中恢复3D位置坐标，该问题又归为对式~\ref{triangulation-equation}进行奇异值分解求最小二乘解的问题。由于$rank([\quad]_{3n\times 4})=2$所以对$[\quad]$进行SVD分解，$V$的最后一列就是$[\quad]$的右零空间，也就是3D点的位置坐标。

三角测量的多视图必须存在平移关系，否则单纯的旋转无法使用三角测量，因为此时对极约束永远满足。在平移存在的情况下，三角测量存在不确定性，当平移很小时，射线夹角很小，计算得到的像素深度不确定性很大；但是平移太大后，图像变换会较大，导致匹配失效。所以三角测量时，需要考虑多视图夹角，也就是多视图平移大小的影响。

\begin{equation}
\centering
\left[\begin{array}{c}
\left[\begin{array}{c}
x_1\\
1
\end{array}\right]_\times P_1 \\
\left[\begin{array}{c}
x_2\\
1
\end{array}\right]_\times P_2 \\
\vdots
\end{array}\right]_{3n\times 4}
\left[\begin{array}{c}
X\\
1
\end{array}\right]_{4\times 1}=0
\label{triangulation-equation}
\end{equation}

\subsection{2D-3D位姿求解}
经过以上的计算，我们可以得到初始的两视图匹配点的3D位置坐标，也就是我们通过两视图匹配关系得到场景的特征点3D位置坐标，或者称为稀疏点云，以及两视图的空间几何关系，至此我们加入第三张匹配的图片，这时就存在如何计算第三张图片对应的相机与刚刚三角测量得到的点云的空间位置关系，完成这一步才能继续增加其他图片，完成对数据集的增量式重建过程。

PnP(Perspective-n-Point)是求解3D到2D点对运动的方法。新图片能被通过解决PnP问题与当前的点云模型（两视图或者多视图得到的）配准。PnP问题被用来估计相机位姿，包括未矫正相机的内参矩阵。我们知道对极几何估计2D-2D位置关系，使用八点法，但也存在纯旋转等问题。然而，如果两张图像其中一张特征点的3D位置已知，那么最小只需要三对匹配点就可以估计相机运动。特征点的3D位置可以有上文的三角测量或者深度传感器确定。3D-2D方法不需要使用对极约束，其有多种解法，直接线性方法，RANSAC或者非线性优化构建最小二乘问题并接待求解的Bundle Adjustment。

\begin{equation}
\begin{split}
&\left[\begin{array}{ccc}
u \\
v \\
1 \\
\end{array}
\right]_\times
\left[\begin{array}{ccc}
P_1 \\
P_2 \\
P_3 \\
\end{array}\right]X =
\left[\begin{array}{ccc}
u \\
v \\
1 \\
\end{array}
\right]_\times
\left[\begin{array}{ccc}
P_1 X \\
P_2 X \\
P_3 X \\
\end{array}\right] \\
&=
\left[\begin{array}{ccc}
0 & -1 & v \\
1 & 0 & -u \\
-v & u & 0 \\
\end{array}\right]
\left[\begin{array}{ccc}
X^T & 0_{1\times 4} & 0_{1\times 4} \\
0_{1\times 4} & X^T & 0_{1\times 4} \\
0_{1\times 4} & 0_{1\times 4} & X^T \\
\end{array}\right]
\left[\begin{array}{ccc}
P_1^T \\
P_2^T \\
P_3^T \\
\end{array}\right]\\
&=
\left[\begin{array}{ccc}
0_{1\times 4} & -X^T & vX^T \\
X^T & 0_{1\times 4} & -uX^T \\
-vX^T & uX^T & 0_{1\times 4} \\
\end{array}\right]
\left[\begin{array}{ccc}
P_1^T \\
P_2^T \\
P_3^T \\
\end{array}\right]=0\\
&Which\quad is\quad A_1b=0
\end{split}
\label{pnp-tuidao1-equation}
\end{equation}

下面仅介绍使用直接线性方法结合SVD分解，其他的方法与此原理相似不一一展开，首先回忆最经典的小孔成像模型$\lambda \left[\begin{array}{c}
x_1\\
1
\end{array}\right]=P_1\left[\begin{array}{c}
X_1\\
1
\end{array}\right]$，即$\left[\begin{array}{c}
x_1\\
1
\end{array}\right]_\times P_1\left[\begin{array}{c}
X_1\\
1
\end{array}\right]=0$，其中$P$为未知量，2D坐标$x$与3D坐标$X$都是已知的，参照求单应性矩阵的方法，我们可以把推导写成如式~\ref{pnp-tuidao1-equation}所示，其中$rank(A)=2$，这很容易理解，因为每对匹配点仅能提供两个方程，$A$为$3\times 12$矩阵，$b$为$12\times 1$的矩阵，所以需要至少6对匹配点才能计算得到位置关系矩阵$b$。此时等式简写为~\ref{pnp-svd-equation}的组合矩阵，其中每对3D-2D匹配点可以写成类似$A_1$的形式。此时可以对式~\ref{pnp-svd-equation}中最左边的组合矩阵运用SVD求解。对于实际中匹配点大于6对时，SVD求得的是最小二乘解。

\begin{equation}
\left[\begin{array}{c}
A_1\\
A_2\\
\cdots\\
A_6
\end{array}\right]b = 0
\end{equation}

通过以上方法，我们可以得到矩阵$P$，由于$P=K[R|t]$，所以旋转矩阵$R=K^{-1}P_{[1:3]}$，$P_{[1:3]}$表示$P$的前三列，由于旋转阵$R$是正交矩阵，为了保证这一性质，所以旋转阵取式~\ref{main-orthogonal-equation}中$R_+$
\begin{equation}
\begin{split}
&R_+ = UV^T\\
&where\quad UDV^T=R
\label{main-orthogonal-equation}
\end{split}
\end{equation}

那么该如何恢复平移阵$t$呢？如式~\ref{main-scale-equation}所示，最后$P=K[R_+ t]$
\begin{equation}
\begin{split}
&t=K^{-1}P_4/\sigma_1\\
&where\quad D=diag(\sigma_1,\sigma_2,\sigma_3)\\
&and\quad \sigma_1>\sigma_2>\sigma_3
\label{main-scale-equation}
\end{split}
\end{equation}

\section{三维重建中的优化}
上面介绍了如何使用多视图几何的知识计算相机位姿与特征点的3D坐标，一般是先计算相机位姿，后计算特征点3D坐标，而本章要介绍的非线性优化的问题，将相机位姿与特征点3D位置放在一起优化，实际上SfM中structure就是指特征点3D坐标，而motion指相机位姿，但是以上内容将图像配准（image registration）和三角测量作为独立的步骤，实际上它们是紧密关联的：相机姿态的不确定将导致三角测量的不确定性，反之亦然。没有优化操作，SfM将很快离散到难以恢复的状态。本文一下介绍同时优化相机位姿与3D点坐标的常用方法，称为光束平差法(Bundle Adjustment,BA)，主要手段是最小化重投影误差。
\subsection{光束平差法}
BA是对相机位姿矩阵$P_c$与3D点$X_k$的非线性优化方法，其最小化重投影误差如式~\ref{repro-equation}，其中函数$f(x)$表示3D点的重投影误差，函数$\pi$是3D点在成像平面的投影的像坐标，$\rho_j$是损失函数，用来降低外点带来的影响。
\begin{equation}
E=arg\mathop{{min}}\limits_{x} \sum_{j=1}^{k}\left \|f_j(x) - x_j \right \|
=\sum_{j}\rho_j(\left \| \pi(P_c,X_k)-x_j \right \|_2^2)
\label{repro-equation}
\end{equation}
为解决该问题，使用最多的方法是梯度下降法，该算法是解决非线性最小二乘问题的通用方法，而BA实际上就是非线性最小二乘问题。实际操作中，我们一般使用Levenberg-Marquardt\cite{Hartley-MultipleView}，简称LM，LM将非线性问题转变为一系列正则线性问题。设$J(x)$ 是$f(x)$ 的雅各比矩阵，最小化重投影误差$E$ 如式~\ref{repro-tuidao-equation}
\begin{equation}
\mathop{{min}}\limits_{x} \left \|f(x)-b\right\|^2=\mathop{{min}}\limits_{x} (f(x)-b)^T(f(x)-b)=\mathop{{min}}\limits_{x} f(x)^Tf(x)-2b^Tf(x)
\label{repro-tuidao-equation}
\end{equation}
对式~\ref{repro-tuidao-equation}求导数有
\begin{equation}
\frac{\partial E}{\partial x}|_{x^*}=2\frac{\partial f(x)}{\partial x}^Tf(x)-2\frac{\partial f(x)}{\partial x}b=0
\label{repro-tuidao2-equation}
\end{equation}
其中
\begin{equation}
J=\frac{\partial f(x)}{\partial x}=
\left[\begin{matrix}
\frac{\partial f_1}{\partial x_1} & \vdots & \frac{\partial f_1}{\partial x_n}\\
\frac{\partial f_2}{\partial x_1} & \vdots & \frac{\partial f_2}{\partial x_n}\\
\cdots & \cdots & \cdots\\
\frac{\partial f_n}{\partial x_1} & \vdots & \frac{\partial f_m}{\partial x_n}\\
\end{matrix}
\right]
\label{repro-tuidao3-equation}
\end{equation}
雅各比矩阵的$m$一般不等于$n$，$m$表示组成$f(x)$的等式个数，$n$表示与$f(x)$相关的变量的个数，下文会详细解释在光束平差法中各个变量代表的含义。雅各比矩阵反映存在$m$个约束条件的误差函数的变化率。对式\label{repro-tuidao3-equation}中的$f(x)$泰勒展开如式~\ref{repro-tuidao4-equation}，最终结果可简写为$H\Delta x = -J^T\nabla$，其中$H$称为Hessian矩阵。
\begin{equation}
\begin{split}
\frac{\partial E}{\partial x}|_{x^*}=2\frac{\partial f(x)}{\partial x}^T(f(x) & +\frac{\partial f(x)}{\partial x}\Delta x)-2\frac{\partial f(x)}{\partial x}b=0\\
\frac{\partial f(x)}{\partial x}^T\frac{\partial f(x)}{\partial x}\Delta x &= \frac{\partial f(x)}{\partial x}^T(b-f(x))\\
\Delta x &=(J^TJ)^{-1}J^T(b-f(x))
\label{repro-tuidao4-equation}
\end{split}
\end{equation}

式~\ref{repro-tuidao4-equation}的推导结果称为高斯牛顿法，LM是基于高斯牛顿法，只是在迭代步长时不使用$\Delta x=(J^TJ)^{-1}J^T(b-f(x))$，而改为$\Delta x=(J^TJ+\lambda D(x)^TD(x))^{-1}J^T(b-f(x))$，其中$D(x)$是非负对角矩阵，是$J^TJ$的对角元素的平方根，也可以简写为$H_\lambda\Delta x = -J^T\nabla$，式中$H_\lambda$被称为增广Hessian矩阵。相较高斯牛顿法，LM的优点是可以动态调节$\Delta x$，当下降太快时，使用较小的$\lambda$，反之亦然。经过上面的计算我们知道实际上光束平差法是一种非线性最小二乘法，关键是如何计算$f(x)$的雅各比矩阵$J$，即$f(x)$ 的梯度，并最终计算步长$\Delta x$，直至误差$E$ 收敛至最小。下面介绍一下，BA 中针对多个相机多个3D 点使用的LM 算法计算雅各比矩阵的形式。最小化误差函数存在$6(F-1)$个运动约束和$3N-1$ 个结构约束，$F$指相机个数，$N$ 指3D点的个数，这可以理解为第一个相机为基准，所以不计入考虑，而每个相机有6 个约束条件，即旋转（3）、相机中心（3），每个3D 点都是3 个约束条件，但是缺失一个尺度，所以减去$1$。Hessian 矩阵$J^TJ$ 的维数为$(6F+3N-7)\times (6F+3N-7)$。既然如此，由于3D 点与相机非常多，所以BA 得到的雅各比矩阵与Hessian 矩阵将非常大。如图~\ref{jagebi-shiyi}所示，两视图、同一个3D点对应的矩阵为式~\ref{jagebi}中的$J$
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.4in]{chapter3/ba.png}
\caption{雅各比矩阵}
\label{jagebi}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=5.4in]{chapter3/jag.png}
\caption{雅各比矩阵示意图}
\label{jagebi-shiyi}
\end{figure}

\begin{equation}
\left[\begin{matrix}
U_\lambda & W \\
W^T & V_\lambda
\end{matrix}\right]
\left[\begin{matrix}
\Delta x_c \\
\Delta x_p
\end{matrix}\right]=-
\left[\begin{matrix}
J_c^T(b-f)\\
J_p^T(b-f)
\end{matrix}\right]
\label{simple-j-equation}
\end{equation}
为了下文分析方便，我们设$U=J_c^TJ_c$，$V=J_p^TJ_p$，$U_\lambda=U+\lambda D_c^TD_c$，$V_\lambda=V+\lambda D_p^TD_p$，$W=J_c^TJ_p$，下标$c$表示与相机参数有关的向量，下标$p$表示与3D坐标点参数有关的向量，可以将式$\Delta x=(J^TJ+\lambda D(x)^TD(x))^{-1}J^T(b-f(x))$写为分块矩阵形式，如式~\ref{simple-j-equation} 所示。$U_\lambda$和$V_\lambda$是分块对角矩阵，对此可以采用Schur Complement高效求解该方程。考虑求解线性系统$M\left[\begin{matrix}
x_1 \\
x_2
\end{matrix}\right]=\left[\begin{matrix}
b_1 \\
b_2
\end{matrix}\right]$，式~\ref{schur-equation}表示分块矩阵$M$的分解
\begin{equation}
\begin{split}
&M = \left[\begin{matrix}
A & B\\
C & D
\end{matrix}\right]=
\left[\begin{matrix}
1 & 0\\
CA^{-1} & 1
\end{matrix}\right]\left[\begin{matrix}
A & 0\\
0 & \bar{D}
\end{matrix}\right]\left[\begin{matrix}
1 & A^{-1}B\\
0 & 1
\end{matrix}\right]\\
& where\quad \bar{D}=D-CA^{-1}B
\label{schur-equation}
\end{split}
\end{equation}
其中，$A$必须是非奇异方阵，$D$称为$A$在$M$中的Schur Complement。
所以使用矩阵$\left[\begin{matrix}
1 & 0\\
-CA^{-1} & 1
\end{matrix}\right]$左乘该线性系统，有$\left[\begin{matrix}
A & B\\
0 & \bar{D}
\end{matrix}\right]\left[\begin{matrix}
x_1\\
x_2
\end{matrix}\right]=\left[\begin{matrix}
b_1\\
\bar{b}_2
\end{matrix}\right]$，其中$\bar{b_2}=b_2-CA^{-1}b_1$，至此我们可以得到一个降阶系统$\bar{D}x_2=\bar{b_2}$，求解$x_2$后，回带解算$x_1$。

回到光束平差法问题中，可以得到降阶系统~\ref{reduce-system-equation}和~\ref{reduce-system2-equation}，矩阵$S=(V_\lambda-WU_\lambda^{-1}W^T)\Delta x_p$是Schur Complement。
\begin{equation}
(U_\lambda-WV_\lambda^{-1}W^T)\Delta x_c=-J_c(b-f)+W^TV_\lambda ^{-1}J_p^T(b-f)
\label{reduce-system-equation}
\end{equation}
\begin{equation}
\Delta x_p=-V_\lambda^{-1}(J_p^T(b-f)+W^T\Delta x_c)
\label{reduce-system2-equation}
\end{equation}

$S$是对称正定矩阵，使用Cholesky分解可以求解式~\ref{reduce-system-equation}。以上求解BA问题的方法之所以有效，是由于相机数量要远小于3D点数量，所以可以先求解$\Delta x_c$，再求解$\Delta x_p$。
\subsection{最小化相机中心位置误差}
上节介绍了如何使用Schur Complement简化线性系统，求解使重投影误差最小的$\Delta x_c,\Delta x_p$，其中涉及到的变量有旋转、相机中心（平移）、3D点坐标，而本节将介绍一种优化方法：最小化相机中心位置误差。由于从图片中，我们可以提取相机的EXIF(Exchangeable image file format)信息，从而得到相机的经纬度坐标。EXIF是由数码相机制造商在图片、声音上标记的相机、图片、声音等相关内容的标准格式文件。EXIF包含的信息丰富，可以从中找到经纬高信息，也可以从中提取相机模型，包括焦距等，也包括相机制造厂商，甚至相机位姿等。

对于同一个3D点在两视图中的左视图的相机坐标系中坐标为$X_1$，在右视图的相机坐标系中坐标为$X_2$，已知两视图相机位姿关系$R,t$，则坐标可以表示为$RX_1+t=X_2$。对于右视图相机中心这一3D点有$X_2=0$，则该点在左视图的相机坐标系中的坐标为$X_1=-R^Tt$，旋转矩阵是正交矩阵有$R^T=R^{-1}$。所以在多视图中每个视图与第一个视图的位姿关系已知为$R_i,t_i$，则这些视图对应的相机中心坐标在第一视图的相机坐标系中的坐标为$-R_i^Tt_i$。这样我们就建立了每个视图的经纬高$pos=(Lon,Lat,High)$与相对于第一视图的位姿关系$-R_i^Tt_i$，基于此我们可以建立最小二乘问题如式~\ref{min-position-equation}。
\begin{equation}
E_{position}=arg\mathop{{min}}\limits_{R,t} \sum_{i=1}\left \|f_j(x) - x_j \right \|
=\sum_{i}(\left \| -R_i^Tt_i-pos \right \|_2^2)
\label{min-position-equation}
\end{equation}

在此不详细介绍如何解算上式，相对于最小化重投影误差的过程，该过程只调整相机相关参数，没有涉及3D点相关的调整，所以只能作为BA的辅助步骤，也可以认为这属于光束平差法的一部分，下文不再过多强调。

\begin{figure}[!htbp]
\centering
\subfigure[]{
\includegraphics[width=3.0in]{chapter3/BA/before_small.jpg}}
\subfigure[]{\includegraphics[width=3.0in]{chapter3/BA/after_small.jpg}
}
\caption{BA对比结果图}
\label{BA}
\end{figure}

本节介绍了基本的稀疏点云生成的过程，包括多视图几何知识从匹配点恢复结构，而后增量式的重建其他图片；在每加入一张图片时，使用BA优化相机位姿与3D点的位置，在此我加上最小化相机中心位置误差，作为BA的辅助步骤。图~\ref{BA}(b)是使用CeresSolver库迭代三次优化重投影误差的结果，初始代价函数结果为$3.483778e+01$，BA优化后的代价为$1.732320e+01$。从~\ref{BA}两张图对比可以看出BA最小化重投影误差的作用:(a) 仅使用多视图几何计算相机位姿并使用三角测量恢复3D点位置，而(b)在(a)的基础上增加BA环节，图中红色点是提取的2D特征点坐标，绿色点是3D根据相机$R,t$关系投影回成像平面的点，从中可以明显看出BA对重投影误差有改善作用，可以将误差平摊到所有点上。

\section{稀疏点云的稠密化}
通过SfM方法，不仅可以得到相机位姿，也可以恢复特征点的三维坐标得到稀疏点云，但是由于点云过于稀疏难以用于无人车导航与规划，所以本文使用Multiple View Stereo(MVS)技术将点云稠密化，MVS可以分为基于体素（voxel），基于表面演化（surface evolution）、基于特征点生长（feature point growing）和基于深度图融合（depth-map merging）四种类型\cite{Shen-Accurate}。本文采用深度图融合的方法\cite{Furukawa-Accurate} 得到稠密点云，该方法包括深度图计算与深度图融合两部分，可以用图~\ref{flow-dense}表示整个过程。

\begin{figure}[H]
\centering
\includegraphics[width=4.9in]{chapter3/flow-dense.png}
\label{flow-dense}
\caption{点云稠密化流程图}
\end{figure}

\subsection{深度图计算}
基于SfM方法，我们可以得到图像数据集下所有图片对应相机的位姿关系与特征点的3D坐标，利用这些位姿关系，可以完成接下来的深度图计算过程。对于单张图片无法计算深度图，深度图的计算首先需要为每张图片选取其参考图（Reference Images），而后根据参考图与自身的旋转平移关系（通过SfM得到），计算每张图片的深度图。对于双目相机，通过双目图片得到深度图，双目成像互为参考图；然而对于无序的图片集，参考图的选取至关重要。本文参考Shen\cite{shen-Accurate}的方法设置参考图阈值，假设有$n$张图片，$\theta_{ij}, j=1,...,n$表示第$i$张图片与其他图片$1,..,n$的相机主轴夹角，$\theta_{ij}$用$i$ 与$j$图片匹配的特征点之间夹角的平均值表示。同时每张图片与其参考图对应相机的距离称为基线，记为$d_{ij}$，基线的距离太长则两张图片重叠区域太小，太短则使重建精度降低，据此满足$5^{\circ}<\theta_{ij}<60^\circ$，$d_{ij}<0.05\bar{d}$或$d_{ij}>2\bar{d}$的图片作为参考图，按照$\theta_{ij}\dot d_{ij}$降序排序参考图，取前10张（如果有）作为参考图。

根据Bleyer\cite{Bleyer-PatchMatch}的方法使用上文刷选的参考图计算深度图，主要思想是遍历图片的每个像素，为其找到一个支持面，使该支持面在参考图中有最小汇总匹配代价，整个过程如图~\ref{patch}所示。其中支持面$f$由法线$n_i$与3D点$X_i$决定，两视图几何中，以一个相机的相机坐标系作为世界坐标系，则有$P=[I_{3\times3} | 0_3]$，$P^\prime=[R|t]$，空间平面的齐次表达式为$\pi^T X=0$，其中$\pi=(V^T,1)^T,V^T=(V_1,V_2,V_3)$，由于$X_i$在面$\pi$上，所以有$\left|V^T\right|=\left|\frac{1}{X_i}\right|$，而方向与$n_i$相反，故$V^T=-\frac{n_i^T}{n_i^TX_i}$。

相机$i$由${K_i,R_i,C_i}$表示，参考图对应的相机用${K_j,R_j,C_j}$表示，现在设世界坐标系中心为相机$i$，则相机坐标系间的旋转平移关系变为$P_i=K_i[I_{3\times3} | 0_3]$，$P_j=K_j[R_jR_i^{-1}|R_j(C_i-C_j)]$
根据吴福朝\cite{wu-jisuanji} 有两视图间的单应性可以用式~\ref{homodouble-equation} 表示。
\begin{equation}
H=K_j(R-tV^T)K_i^{-1}=K_j(R_jR_i^{-1}+R_j\frac{(C_i-C_j)n_i^T)}{n_i^TX_i}K_i^{-1}
\label{homodouble-equation}
\end{equation}

\begin{figure}[!htbp]
\centering
\includegraphics[width=4.0in]{chapter3/mvs/patch.png}
\caption{最小汇总匹配代价计算过程}
\label{patch}
\end{figure}

图~\ref{patch}中支持面$f$（patch）以$p$为中心，大小为$7\times 7$像素的正方形块，对于patch中的每个像素通过式~\ref{homodouble-equation}转换到参考图中，记为$H_{ij}$，此时可以求出$7\times7$所有像素$q$的代价和$m(p,f_p)$，称为去均值的归一化互相关（NCC, Normalized Cross Correlation），如式~\ref{matchingCost-equation}所示，其中上划线表示所有像素点的平均值。

\begin{equation}
m(p,f_p)=1-\frac{\sum\limits_{q\in patch}(q-\bar{q})(H_{ij}(q)-\overline{H_{ij}(q)})}{\sqrt{\sum\limits_{q\in patch}(q-\bar{q})^2\sum\limits_{q\in patch}(H_{ij}(q)-\overline{H_{ij}(q)})^2}}
\label{matchingCost-equation}
\end{equation}

据此可计算数据集中每张图片的深度图与图片中每个像素对应的NCC值，如图~\ref{depth}所示，图$(b),(e)$为深度图，图$(c),(f)$为NCC值图，图中使用冷暖色表示像素值的大小，颜色越暖表示像素值越大。图~\ref{depth-3} 较冷（蓝色）的区域表示用来计算深度图的图片与参考图对应patch区域像素误差较大，当大于一定阈值时，证明该区域的深度计算是不准确的，故对应图~\ref{depth-5}的深度设为零，为最浅色（蓝色）部分。对于深度图而言颜色越暖证明深度越大，图~\ref{depth-5}很好反映了一面竖直墙导致图片深度的变化情况。

\begin{figure}[!htbp]
\centering
    \subfigure[]{
    \label{depth-1}
    \includegraphics[width=1.8in]{chapter3/mvs/DJI_0056.JPG}
    }
    \subfigure[]{
    \label{depth-2}
    \includegraphics[width=1.8in]{chapter3/mvs/DJI_0056_depth.jpg}
    }
\subfigure[]{
\label{depth-3}
    \includegraphics[width=1.8in]{chapter3/mvs/DJI_0056_score.jpg}
}
\subfigure[]{
\label{depth-4}
    \includegraphics[width=1.8in]{chapter3/mvs/DJI_0100.JPG}
    }
    \subfigure[]{
    \label{depth-5}
    \includegraphics[width=1.8in]{chapter3/mvs/DJI_0100_depth.jpg}
    }
\subfigure[]{
\label{depth-6}
    \includegraphics[width=1.8in]{chapter3/mvs/DJI_0100_score.jpg}
}
\caption{深度图与NCC值}
\label{depth}
\end{figure}

\subsection{深度图融合}
在得到每张图片的深度图之后，需要将各个深度图融合得到完成的场景图，由于图片存在重叠区域，所以直接融合深度图会在重叠区域产生深度冗余，所以深度图的融合就是去除冗余深度的过程。将图$i$每个像素点$x$通过上文计算得到的深度重新投影到3D空间为$X=\lambda R_i^TK_i^{-1}x + C_i$，而$i$的参考图$j=1,..h$也可以得到对应图$i$中每个像素的深度，这个深度可以通过参考图的深度图计算得到，记为$\lambda(X,j)$，对于依赖图$i$的深度图得到的坐标$X$可以转换到参考图的相机坐标系中得到$X$对应的像素深度$d(X,i)$，如果$d(X,i)<\lambda(X,j)$或$\frac{\left|d(X,i)-\lambda(X,j)\right|}{\lambda(X,j)}$，则移除参考图中该像素的深度。
\section{本章小结}
本章介绍了如何将点云稠密化的过程，首先采用基于块的多视图理论，以去均值的NCC作为块匹配代价函数得到数据集中每张图片的每个像素的深度，由于图片存在重叠区域，所以深度图的融合也存在冗余深度的问题，所以精炼深度图，深度设为零（图~\ref{depth-2}蓝色区域），从而去除冗余深度，从而完成深度图的融合，得到稠密点云。
