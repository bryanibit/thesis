\chapter{基于全景图像的高精度地图构建}
\section{引言}
全景图像（Panorama）是指水平角度包含完整$360^\text{o}$的相机得到的二维平面图像。相对于普通单目相机，全景相机可以更完整地呈现无人车周围环境。
本章通过全景相机与高精度定位系统的数据融合提出一种车道级别电子地图的构建方法，以该地图为基础，结合运动规划技术和跟踪控制技术可以为UGV在结构化或非结构化道路环境下自主行驶提供保障。本章主要包括数据采集系统设计、全景图像处理及大范围电子地图构建。

\section{传感器数据采集}
\subsection{传感器选取}
在开始研究之前，先讨论本研究所涉及到的传感器选型问题。如图\ref{sensors}所示是UGV通常使用的视觉传感器、3D激光雷达。图$(a)$是双目相机，双目相机可以通过左右摄像头计算得到场景深度图，但双目相机的基线较短，当物体距相机较远时，计算得到的物体深度图误差较大。图$(b)$是全景相机，可以获取周围$360^\text{o}$ 场景，视野开阔，但相机模型较复杂。图$(c)$激光雷达可以直接得到3D点云，且精度高，但激光雷达价格昂贵限制了其广泛应用。图$(d)$ 是最常见的单目相机，相机的成像可以近似抽象为小孔成像模型，处理其成像较全景相机更方便，虽然其无法直接得到场景的深度图，但可以通过针对同一场景的多张图像计算该场景的深度图。
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=4.7in]{chapter1/sensors.png}
	\caption{传感器示例}
	\label{sensors}
\end{figure}

针对以上分析，结合实际使用需要，本文设计的构建二维地图方法旨在为UGV导航提供先验信息，UGV通过定位信息匹配到二维地图的某区域后，即可通过地图获取当前车道、限速、甚至距离路口远近以及是否有红绿灯等信息，这些先验信息将为UGV导航规划提供重要帮助。由于全景相机相比单目相机可以获得更广视野的图像信息，且本章只需要制作二维地图，不需要选择双目相机或价格昂贵的激光雷达，所以在二维地图构建中倾向于选择视野开阔的全景相机。

Ladybug全景相机具有6个独立的摄像头，如图\ref{sensors}中$(b)$所示，可以在球面坐标系下将6个相机图像实时合并为一张全景图，呈现在二维平面空间中。Ladybug相机提供$360^\text{o}$视频串流功能，可以覆盖90\% 的可视球面，具有图像采集、处理和校正功能。

由于本章提出的方法是基于定位信息将变换后的全景图像拼接为大范围地图，所以在此选用高精度组合定位系统，其由差分GNSS（Global Navigation Satellite System）接收机与高精度惯性导航系统构成。在GNSS接收机锁星状态良好时，该组合定位系统可以达到厘米级别精度。

\subsection{定位信息编码}
为了同时采集Ladybug与高精度组合定位系统的数据信息，采用将车辆的定位信息编码为大小一定的QR Code（Quick-Response Code），放置于每帧全景图像左上角位置，完成系统的信息采集任务。

QR Code作为一种目前使用较多的矩阵式二维码，具有空间利用率高，存储数据量大，解码速度快，可以包含字符、数字等不同内容等优点。如图\ref{qrcodefig} 所示，二维码可分为两部分：功能区和编码区。功能区的二维码主要是用来定位二维码的位置，校正二维码的形变；编码区的二维码包含该二维码的数据与校验纠错等信息。
按照QR Code的规则，可以很方便地将定位信息编码为固定像素大小的二维码，进而将其放置在每帧全景图像中，完成信息的采集过程。当回放该视频时，可以很容易地对大小固定的编码区进行解码，读取到每帧全景图对应的车辆当前的位置与姿态信息等，为图像拼接做准备。
\begin{figure}[!htbp]
	\centering
    \includegraphics[width=4.0in]{chapter2/qrcode.png}
	\caption{QR Code示意图}
    \label{qrcodefig}
\end{figure}

在此介绍一下二维码的编码与解码过程。由于Ladybug图像的帧率为7Hz，高精度组合定位系统的帧率为20Hz，考虑拼接图像的精度，在没有使用硬件触发的条件下，本文采用最近邻原则，即当收到一帧Ladybug图像时，查找距当前时刻最近的高精度组合定位系统的输出信息，将其编码为二维码。由于编码的信息有时间、经纬度、方位角、偏航角与俯仰角6 个整数，所以需要的编码信息量很小，本文采用$25\times25$像素的二维码编码定位信息。

\section{全景图像处理}
在得到Ladybug相机的全景图后，使用逆透视投影（Inverse Perspective Mapping, IPM）将全景图像投影成俯视图。而后将车辆的高精度定位信息编码为二维码，放置于对应的逆透视投影得到的图片的左下角，从而完成图像与其对应的位置采集与处理，方便后续构建大范围地图。
%\begin{figure}[!htbp]
%	\centering
%	\caption{Ladybug全景相机}
%	\label{ladybug}
%\end{figure}
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter2/para.png}
	\caption{逆透视坐标变换关系}
	\label{para}
\end{figure}
由于Ladybug成像描述在球面坐标系中，IPM投影需要表示在车体坐标系中，地图拼接需要在世界坐标系中完成，在此简单介绍全景图像处理过程涉及的相关坐标系的变换关系。如图\ref{para}所示，代表Ladybug全景相机的球面坐标系（图中$c$表示）与车体坐标系（图中$w$表示）的关系，其中在车体坐标系中，绕自身$x$，$y$和$z$旋转的角度分别为俯仰角，横滚角和方位角，图中道路代表车辆行驶方向；在全景球面坐标系中，球面狭缝与全景图分块成对应关系（图中绿色区域所示），如图中虚线箭头所示，将球面坐标展开成二维平面得到全景图像，其长宽分别代表相对于车体的方位角$\mu$与俯仰角$\nu$。

为了得到全景图像的IPM俯视图，需要标定全景相机Ladybug相对于车辆的外参：俯仰角$\theta$，横滚角$\phi$和方位角$\varphi$，以及Ladybug相机光心与地面的高度$h$。现定义车体坐标系为$\{F_\omega\}=\{X_\omega,Y_\omega,Z_\omega\}$，坐标原点位于车辆后轴中点；定义全景相机坐标系为$\{F_c\}=\{X_c,Y_c,Z_c\}$；定义全景图像坐标为$\{F_i\}=\{\mu,\nu\}$，则车体坐标系中任一点$P_c$与全景相机坐标系中点$P_w$的对应关系为：
\begin{equation}
\label{coordiante relation}
P_c = R_\phi R_\theta R^T _\varphi (P_\omega + T)
\end{equation}
其中
\begin{equation}
R_\phi=\left[\begin{array}{ccc}
cos\phi & 0 & -sin\phi \\
0 & 1 & 0 \\
sin\phi & 0 & cos\phi \\
\end{array}
\right]
\label{R-phi}
\end{equation}

\begin{equation}
R_\theta=\left[\begin{array}{ccc}
1 & 0 & 0 \\
0 & cos\theta & sin\theta \\
0 & -sin\theta & cos\theta \\
\end{array}
\right]
\label{R-theta}
\end{equation}

\begin{equation}
R_\varphi=\left[\begin{array}{ccc}
cos(\varphi-\frac{\pi}{2}) & sin(\varphi-\frac{\pi}{2}) & 0 \\
-sin(\varphi-\frac{\pi}{2}) & cos(\varphi-\frac{\pi}{2}) & 0 \\
0 & 0 & 1 \\
\end{array}
\right]
\label{R-varphi}
\end{equation}

\begin{equation}
T=\left[\begin{array}{ccc}
0 \\ 0 \\ -h \\
\end{array}
\right]
\label{h-move}
\end{equation}
进而可以得到全景图像坐标系中的坐标$P_i = [\mu_i, \nu_i]^T$:
\begin{equation}
\label{p-i}
P_i=\left[\begin{array}{ccc}
arctan \frac{y_c}{x_c} \\ arccos \frac{z_c}{||P_c||} \\
\end{array}
\right]
\end{equation}
其中$||P_c||=\sqrt{{x_{c}}^2 + {y_{c}}^2 + {z_{c}}^2}$，即$P_c$的二范数。通过以上算法可以将全景图像（图\ref{raw-para}）变换得到车体坐标系下的IPM图，显示效果如图\ref{IPM}所示，图中长宽在真实地理空间中均为$30m$。

\begin{figure}[!htbp]
	\centering
    \subfigure{
	\includegraphics[width=2.8in]{chapter2/RAW927090958.png}
    }
    \subfigure{
    \includegraphics[width=2.8in]{chapter2/RAW1001103021.png}
    }
	\caption{基于球面坐标的全景图}
	\label{raw-para}
\end{figure}

\begin{figure}[!htbp]
	\centering
    \subfigure{
	\includegraphics[width=2.4in]{chapter2/IPM927090958.png}
    }
    \subfigure{
    \includegraphics[width=2.4in]{chapter2/IPM1001103021.png}
    }
	\caption{逆透视变换结果}
	\label{IPM}
\end{figure}
%\subsubsection{球面坐标系}
%\subsubsection{世界坐标系}
%\subsubsection{车体坐标系}
%\subsubsection{球体相机坐标系}


\section{地图的构建与使用}
使用以上方法可以得到全景图像的逆透视投影图，以及每张投影图对应的此时车辆位置与方位等信息。基于此可以将局部小区域的逆透视投影图按照其对应的位置拼接为大范围地图，并添加车道、限速等信息，生成UGV行驶地图，为UGV规划与感知提供先验信息。
\subsection{图像拼接}
图像拼接的历史可以追溯到上个世纪，拼接方法总体上可以分为两类，一类是基于图像特征匹配的拼接方法，另一类是基于图片间相对位置关系的拼接方法。由于采用高精度组合定位系统，在无遮挡、多路径效应较弱的空旷场地，实验精度达到$5cm$ 以内，所以本文采用基于图片间相对位置关系的方法。具体就是将视频流中每帧逆透视投影图片按照二维码解算的航向旋转一定角度，再按照解算的位置进行平移，对所有视频按照上述方法处理，最终得到完整的城区地图。

对于每帧IPM俯视图，车身为无效信息且占据图像大量位置，对地图拼接造成干扰，所以在拼接地图前，需要将车身移除。由于全景图变换为IPM投影图时，将IPM俯视图变换到车体坐标系中，所以任何一张IPM俯视图的车辆位置与方位是固定的：车辆后轴中心位于图片横轴的中心，纵轴靠下的$1/3$位置处，航向朝上。如图\ref{carmaskfig}(a)所示。据此设计一个掩码（Mask），在拼接过程中将车辆去掉，如图\ref{carmaskfig}(b)所示，仅拼接剩下的像素不为零的部分。
\begin{figure}[!htbp]
    \centering
    \subfigure[]{
	\includegraphics[width=2.4in]{chapter2/IPM927090958.png}}
    \subfigure[]{
	\includegraphics[width=2.4in]{chapter2/res.png}}
    \caption{使用掩码移除车辆干扰}
    \label{carmaskfig}
\end{figure}

在拼接过程中，因为所需拼接的地理空间过大，图片过多，而内存限制了单张图片的大小，所以本文设计了区号标记的方法。具体做法为：将视频流中图片按照位置和航向拼接为多张$28000\times7000$像素的图片（图\ref{carmask}中橙色方框），由于每个像素代表实际地理空间的大小为$0.06m$，即每张拼接后的图片代表的物理空间大小为$1680m\times420m$。设置整个地图的坐标原点（图\ref{carmask}红色圆点所示）为某一固定经纬度坐标，IPM图对应的经纬度与该点对比后，计算出该帧IPM图应该放置于哪个区，而后可以拼接成多张橙色大小的图片。一个橙色方框代表一个区域，对应一个区号，相邻区号代表的区域存在大小一定的重叠区域。这是因为每帧IPM图大小一定，而定位信息表示Ladybug的光心位置，即每张IPM图的左右$1/2$，上下$1/3$位置处，在图\ref{carmask}中右图黑色方框虚线交点位置，如果区号划分的区域（图\ref{carmask}中两子图橙色方框）间不存在重叠区域，则某些定位信息虽然属于某个橙色区域，但其对应的IPM图的某些像素可能落在该橙色区域之外，导致无法拼接到橙色区域中。这种存在重叠区域的区域划分方法有效地解决了图像拼接过程中边缘缺失的问题，重叠区域的大小至少为IPM边长的$5/6$，如图\ref{carmask}中左图黑色方框所示。

\begin{figure}[!htbp]
	\centering
    \subfigure{
	\includegraphics[width=1.8in]{chapter2/overlap.png}
    }
    \subfigure{
	\includegraphics[width=3.3in]{chapter2/area_num.png}
    }
	\caption{IPM图拼接处理过程}
	\label{carmask}
\end{figure}

%边缘缺失是指系统拼接得到的橙色图的边缘没有道路像素，这是由于每帧IPM图的旋转中点无法通过平移放置于接近橙色图的边缘位置，因为这样IPM 图的其他像素就落在橙色图区域之外，重叠区域的存在可以有效解决该问题。
最后只需要保留上图\ref{carmask}的绿色框表示的区域和该区域的区号，这样就可以按照区号将拼接的图片在Photoshop中拼接为更大的图片，但是Photoshop 拼接图片受限于计算机内存，所以当地理空间过大，图片像素过多时，仍然无法将整个地理空间拼接为一张图片。为此可以采用GIS数据库软件，将Photoshop得到的每张图片作为GIS数据库中的一个图层，构建大范围地图数据库的底图，如图\ref{gistuceng}所示，其中每张图片均为GIS中的一个图层。

\begin{figure}[!htbp]
	\centering
    \subfigure{
	\includegraphics[width=1.8in]{chapter2/GIS_illustrate/1_.png}
    }
    \subfigure{
	\includegraphics[width=1.8in]{chapter2/GIS_illustrate/2_.png}
    } \\
    \subfigure{
	\includegraphics[width=1.8in]{chapter2/GIS_illustrate/3_.png}
    }
    \subfigure{
	\includegraphics[width=1.8in]{chapter2/GIS_illustrate/4_.png}
    }
	\caption{GIS中的图层}
	\label{gistuceng}
\end{figure}

\subsection{基于GIS数据库生成无人驾驶车辆行驶地图}

在得到数张Photoshop拼接的更大的图片后，按照上图\ref{carmask}的分区原则可以得到每张图片四个顶点的地理坐标。依托SuperMap(GIS)软件完成拼接地图的金字塔图生成，作为GIS数据库的一个图层。这样构建的地图并不能直接用于UGV导航或规划。将得到的底图作为GIS数据库的一个图层，根据底图可以得到车道级别的道路拓扑结构，该拓扑作为GIS数据库的另一个图层，在线段与线段的节点上添加各种属性，例如车道数量，限速，红绿灯和路口等，即可实现全局路径规划等功能。图\ref{road-cross}中左图为拼接的路口图片，右图中绿色线段为GIS数据库的道路拓扑。

\begin{figure}[!htbp]
	\centering
    \subfigure{
	\includegraphics[width=2.8in]{chapter2/road_cross.png}
    }
    \subfigure{
	\includegraphics[width=2.8in]{chapter2/road_net.png}
    }
	\caption{拼接路口在GIS数据库显示结果}
	\label{road-cross}
\end{figure}

\section{本章小结}
鉴于高精度地图在UGV自主导航与决策规划中的重要作用，本章提出了一种新的高精度地图构建方法，并给出了地图构建的实现流程。在该方法中，高精度组合导航系统的位置航向输出编码为固定大小的二维码，嵌入全景图像视频中，以保证全景图像与定位信息的同步。而后在离线拼接地图过程中，将全景图像转换到车体坐标系中，完成IPM过程，并将每个IPM图像根据车辆的高精度定位信息，旋转、平移拼接为范围较大的地图。但由于地理空间广阔，计算机内存限制等原因，本章将采集的视频流拼接为多个大小一致的图像，并根据定位信息将该图像编号，据此可以拼接更大范围的地图，并导入GIS 数据库生成多个图层构成完整的道路地图。最后在GIS数据库中添加道路拓扑与道路属性等信息。第5章实验环节展示了本章提出的方法在实际无人驾驶比赛的效果，验证了此方法的有效性。
但是本章获取的二维地图存在肉眼可见变形、包含地面信息较少的问题，所以下文将介绍另外一种构建地图方法，可以通过二维图片恢复地理空间的三维结构。
