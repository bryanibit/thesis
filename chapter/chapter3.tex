\chapter{基于无人机航拍图像的三维地图构建}

\section{引言}
大部分UGV依赖环境地图导航与感知\cite{Mattyus-Enhancing-road-maps}，这限制了UGV在未知环境的自主导航规划。为了解决该问题，当前许多技术利用车载传感器构建大范围地图，例如SLAM和第二章提到的用车载Ladybug构建地图的方法。然而车载传感器仅能获取车辆周边环境信息且易被阻挡，而且在野外或灾害地区等环境中，车辆可通行区域变少。无人机拥有比UGV更广阔的视野，不受地面障碍物的影响，可以灵活地从空中俯瞰地面。本章介绍了应用无人机航拍图像构建三维环境地图的方法，为UGV自主导航提供先验信息。

三维重建可以分为两部分：稀疏点云与相机位姿的获取以及点云稠密化。其中稀疏点云与相机位姿的获取使用的是Structure from Motion（SfM），在技术上与当前热门的Visual-SLAM （V-SLAM）比较类似，只是SfM侧重于三维重建，而SLAM侧重于实时定位，并预测下一时刻的位置。SfM可以分为视觉前端与优化后端，前端涉及相机模型、坐标转换、特征提取与匹配以及多视图几何等相关内容，最后得到粗略的相机位姿与3D点坐标，后端涉及优化前端得到的相机位姿（即精炼相机位姿）与3D点坐标、去除误差大的3D点。点云稠密使用的技术为Multi-View Stereo（MVS），MVS的实现方式多种多样，本文采用基于像素区块的深度图计算方法，对每张图片寻找参考图，根据参考图与SfM恢复的相机位姿得到该张图片的深度图，最后融合所有图片的深度图形成稠密点云。
\section{相机模型}

三维地图构建复杂，但可以提供更多的地理空间信息，必将是未来UGV地图发展的方向。由于本文将基于无人机视觉传感器构建三维场景地图，因为无人机飞行高度较高，此时双目相机的基线基本上可以忽略，两个双目相机的输出基本相同，无法解算得到景深图，因此在三维重建中，选择单目相机作为本文的传感器。

使用相机拍摄物体的过程实际是一个光学成像过程，这涉及到摄像机最基本的元件――透镜――的成像原理，如图~\ref{len}所示，其透镜成像原理为：$Z$ 是物体距透镜光心的距离，简称物距，$f$是焦距，$b$ 是像距，即成像平面与透镜光心的距离，三者满足等式~\ref{len-equation}。
\begin{equation}
\frac{1}{f} = \frac{1}{Z} + \frac{1}{b}
\label{len-equation}
\end{equation}

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=4.8in]{chapter3/focal.png}
	\caption{透镜成像原理}
	\label{len}
\end{figure}
在机器视觉中，利用摄像机可以将三维场景记录在二维平面上，不同的相机模型产生不同的成像效果，比如第二章中Ladybug使用的球面成像模型，使多个相机的图像拼接简化为绕球面坐标的光心旋转。但常见的相机模型还是小孔成像模型，如图~\ref{pinhole}所示。数码相机中，镜头近似为一个凸透镜，感光元件位于透镜焦点附近，将焦距近似为凸透镜中心到感光元件的距离时，该模型就变为小孔成像模型。这可以类比为艺术家画一幅画的过程，将眼睛放在图~\ref{pinhole}针孔$C$（光心）处，在物体与眼睛之间放置一个画布（图~\ref{pinhole}中虚像位置），按照光线直线传播的原则，则物体反射的光线与眼睛的连线交画布一点，如此物体可以在画布上成像，只是相机的成像在光心后面。
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter3/pinhole.png}
	\caption{小孔成像模型}
	\label{pinhole}
\end{figure}

为了简化成像模型，假设存在一个位于画布位置的虚像平面，则虚像、物位于光心同一侧，如图~\ref{firstperson}。
在三维空间中，物体反射的光线经过小孔形成倒立二维图像，根据其数学模型，可以得到
\begin{equation}
-\frac{x}{X} = \frac{f}{Z}
\label{pinhole-equation}
\end{equation}
其中，$f$是小孔到成像平面的距离，即焦距；$Z$是物体距光心的距离，简称物距；$x$是物体在成像平面的投影长度；$X$是物体实际长度。为了简化以上模型，用图~\ref{firstperson}表示小孔成像的等价模型，其中$C$是相机中心，$P(X,Y,Z)$为空间中的3D点，$p(x,y,1)$表示3D点成像在感光元件上的点，小孔成像近似后，成像平面与相机中心的距离为焦距$f$。图~\ref{firstperson}展示了三个坐标系，分别为相机坐标系、图像坐标系与像素坐标系，在下节中详细介绍。
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter3/firstperson.png}
	\caption{小孔成像等价模型}
	\label{firstperson}
\end{figure}

\section{相关坐标系}
由上节可知，仅仅相机内部就存在多个坐标系，所以本节将探讨摄像机涉及的坐标系转换问题。相机成像过程是一个3D物体显示在2D成像平面的过程，在此涉及物体、真实空间与相机三者的位置关系。物体在真实空间的位置坐标是物体在世界坐标系中的坐标表达，物体相对于相机中心的位置坐标是物体在相机坐标系中的坐标表达。
%在上图~\ref{firstperson}中，$(X,Y,Z)$为3D点$P$在相机坐标系（$x_c,y_c,z_c$）下的坐标表示，
这里就存在两个三维坐标的转换，即同一个点在世界坐标系与相机坐标系下表达方式的转换关系，如式\ref{R-t-equation}\cite{Hartley-MultipleView}，该变换可以看做三维坐标系的刚体变换。
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
^{c}P = ^{c}R_w \phantom{}^wP + ^c T_w
\label{R-t-equation}
\end{equation}
其中$c$表示相机坐标系，$w$表示世界坐标系，$^w P$表示世界坐标系下$P$点3D坐标，$^c P$是相机坐标系下$P$点3D坐标，$^{c}R_w$和$^c T_w$分别表示由世界坐标系到相机坐标系的旋转矩阵和平移矩阵。

将世界坐标系下的3D点表示为齐次坐标$(X,Y,Z,1)$，式\ref{R-t-equation}可以改写为：
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
^{c}P=\left[\begin{array}{ccc}
^{c}R_w & ^c T_w \\
0_{1\times 3} & 1 \\
\end{array}
\right]_{4\times 4}
\left[\begin{array}{ccc}
X_w \\
Y_w \\
Z_w \\
1 \\
\end{array}
\right]
\label{Rt-matrix-equation}
\end{equation}
其中$^{c}R_w$是$3\times 3$矩阵，三个列向量两两正交；$^c T_w$是$3\times 1$矩阵，代表从相机原点指向世界坐标系原点的向量，$^{c}P$坐标表示为$(x_c,y_c,z_c)$。

由上图\ref{firstperson}中可以看到成像平面内的二维坐标系$x_i ,y_i$，该二维坐标系称为图像坐标系，可以推导某点在相机坐标系与图像坐标系转换的公式表示为式~\ref{camera2img1-equation}和式~\ref{camera2img2-equation}，其中$x_c,y_c,z_c$表示相机坐标系下的坐标。
\begin{equation}
x_i = f\frac{x_c}{z_c}
\label{camera2img1-equation}
\end{equation}
\begin{equation}
y_i = f\frac{y_c}{z_c}
\label{camera2img2-equation}
\end{equation}
改为矩阵表达形式为
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
z_c \left[\begin{array}{ccc}
x_i \\
y_i \\
1 \\
\end{array}
\right]=\left[\begin{array}{cccc}
f & 0 & 0 & 0 \\
0 & f & 0 & 0 \\
0 & 0 & 1 & 0 \\
\end{array}
\right]
\left[\begin{array}{ccc}
x_c \\
y_c \\
z_c \\
1 \\
\end{array}
\right]
\label{camera2img-matrix-equation}
\end{equation}

相机在生成图像的过程中，由于成像平面由许多CCD感光元件组成，所以成像的横纵坐标是不连续的，由此引入像素概念，同时也产生从图像坐标系到像素坐标系的转换关系如式~\ref{camera2pixel-equation}。
\begin{equation}
\left[\begin{array}{ccc}
u \\
v \\
1 \\
\end{array}
\right]=\left[\begin{array}{ccc}
\frac{1}{s_x} & s & p_x \\
0 & \frac{1}{s_y} & p_y \\
0 & 0 & 1 \\
\end{array}
\right]
\left[\begin{array}{ccc}
x_i \\
y_i \\
1 \\
\end{array}
\right]
\label{camera2pixel-equation}
\end{equation}
其中，$s_x$是u轴方向单像素的宽度，$s_y$是v轴方向单像素的宽度，当前大部分CCD的像素宽度，$dx=dy$，但老式电视机使用的是矩形ccd，导致$s_x\neq s_y$。$p_x$是主点（图~\ref{firstperson} Z 轴与成像平面的交点）与图像左边界的距离，$p_y$ 是主点与图像上边界的距离。$s$是衡量主光轴与成像平面的倾斜程度，如果主光轴垂直于成像平面，则$s = 0$。这些参数被称作相机内参，一般可以通过查询相机手册得到包括焦距在内的相机内参。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=4.5in]{chapter3/transform.png}
	\caption{坐标转换流程图}
	\label{transform}
\end{figure}
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter3/transform-coordinate.png}
	\caption{坐标转换关系图}
	\label{transform-coordinate}
\end{figure}
至此我们可以得到从世界坐标系到像素坐标系转换过程，如图~\ref{transform}和图~\ref{transform-coordinate}所示。
图~\ref{transform}过程用公式表达为式~\ref{transform-equation}所示。
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
z_c \left[\begin{array}{ccc}
u \\
v \\
1 \\
\end{array}
\right]=\left[\begin{array}{ccc}
\frac{1}{s_x} & s & p_x \\
0 & \frac{1}{s_y} & p_y \\
0 & 0 & 1 \\
\end{array}
\right]
\left[\begin{array}{cccc}
f & 0 & 0 & 0 \\
0 & f & 0 & 0 \\
0 & 0 & 1 & 0 \\
\end{array}
\right]
\left[\begin{array}{ccc}
^{c}R_w & ^c T_w \\
0_{1\times 3} & 1 \\
\end{array}
\right]_{4\times 4}
\left[\begin{array}{cccc}
X_w \\
Y_w \\
Z_w \\
1 \\
\end{array}
\right]
\label{transform-equation}
\end{equation}

至此我们得到相机成像过程某点在不同坐标系下转换关系，将在真实空间表达的3D物体成像为相机CCD上的2D图像，完成摄像过程。式~\ref{transform-equation} 涉及坐标系变换的相关参数:$^cR_w,^cT_w$，这些参数称为相机的外参，其与相机的空间位置有关，与相机本身无关；而其他参数，如$f,p_x,p_y,s_x,x_y$称为相机内参，这些参数与相机的空间位置无关，只与相机本身有关。实际上在相机成像过程中，除了这些可线性建模的相机内参外，还存在无法线性建模的相机内参，如Gopro 相机的鱼眼镜头中，我们可以看到世界中的直线不再是直线，如图~\ref{distortion}(a)所示，
这些弯曲的线有一些共同特性：它们都是关于中心扭曲的，我们称这种畸变为径向畸变，这意味着像素点是沿着从中心放射的径向成比例扭曲的。
\begin{figure}[!htbp]
	\centering
    \subfigure[]{
	\includegraphics[width=2.4in]{chapter3/radial-distortion.PNG}
    }
    \subfigure[]{
	\includegraphics[width=2.5in]{chapter3/non-radial-distortion.PNG}
    }
	\caption{径向畸变的校正}
	\label{distortion}
\end{figure}

为了去除相机的畸变得到图~\ref{distortion}(b)所示图像，需要对畸变进行建模，表示为多项式~\ref{radial-equation}。
\begin{equation}
\left\{
\begin{aligned}
&u^{rdist} = u(1+k_1 r+k_2 r^2+ k_3 r^3 + k_n r^n) \\
&v^{rdist} = v(1+k_1 r+k_2 r^2+ k_3 r^3 + k_n r^n) \\
&r^2 = u^2 + v^2 \\
\end{aligned}
\right.
\label{radial-equation}
\end{equation}
%\begin{subequations}
%\label{radial-equation}
%\begin{align}
%u^{rad-dist} &= u(1+k_1 r+k_2 r^2+ k_3 r^3 + ...)  \\
%v^{rad-dist} &= v(1+k_1 r+k_2 r^2+ k_3 r^3 + ...)  \\
%where \quad & r^2 = u^2 + v^2
%\end{align}
%\end{subequations}
其中，$k_1,k_2,k_3$是未知参数，需要标定才可以得到，而实际校正径向畸变一般也只用到2个或3个参数，如图~\ref{distortion}所示，(a)为存在径向畸变的图片，(b)为经过校正后的图片。

在相机校正的过程中，还需要校正图像的切向失真，这是由于摄像机安装时成像平面与透镜没有平行导致的，如果说径向畸变是坐标点距离中心点的长度发生变化，那么切向畸变就是坐标点与中心点的水平夹角发生变化。对切向畸变的建模如式~\ref{tangential-equation}，一般使用$p_1,p_2$两个参数。
\begin{equation}
\left\{
\begin{aligned}
&u^{tdist} = u + 2p_1 uv + p_2(r^2 + 2u^2) \\
&v^{tdist} = v + 2p_2 uv + p_1(r^2 + 2v^2) \\
&r^2 = u^2 + v^2 \\
\end{aligned}
\right.
\label{tangential-equation}
\end{equation}
%\begin{subequations}
%\label{tangential-equation}
%\begin{align}
%u^{tag-dist} &= u + 2p_1 uv + p_2(r^2 + 2u^2) \\
%v^{tag-dist} &= v + 2p_2 uv + p_1(r^2 + 2v^2) \\
%where \quad & r^2 = u^2 + v^2
%\end{align}
%\end{subequations}

以上介绍了相机模型与相机相关坐标系转换关系，以及相机成像去畸变方法，这部分属于图像处理的基础，为下文介绍多视图几何作铺垫。以下3.4 - 3.7 节是本文SfM的主要内容，可以概括为流程图~\ref{flow-sparse}，输入为无序单目航拍图像，输出为稀疏点云与相机位姿，以下将分步介绍各个环节。

\begin{figure}[!htbp]
\centering
\includegraphics[width=5.4in]{chapter3/flow-sparse.png}
\caption{SfM流程图}
\label{flow-sparse}
\end{figure}

\section{特征点提取}
SfM方法首先需要找到图片集中每两张图片间的几何关系，这样才能使无序图像集“有序化”，从而估算相机间的运动与特征点的三维坐标。图像间几何关系的确定方法有多种，例如光流法，特征点法等，光流法适用于匀速运动的视频流中，要求帧间运动比较小，从而找出帧间运动关系；特征点法是提取并存储图像中比较有代表性的特征。由于本文使用的数据集为无序的航拍图像，特征点法较为适宜使用。特征点提取过程可以通俗地理解为将图像由二维矩阵描述变为一维特征向量描述，突出图像信息的重点，去除冗余信息。

最简单的图像特征为图像的角点，角点就是图像中线的交点，例如大厦最高处的顶点。两幅图像的角点不会随着图像移动而改变，但是角点会随着离相机的距离太近而变得无法识别，所以许多研究者设计了精巧而互有优劣的特征检测与描述方法，比较著名的有SIFT\cite{Lowe-DistinctiveImageFeatures}，SURF\cite{Bay-Speeded-UpRobustFeatures}，ORG\cite{Rublee-ORB:Anefficient}等。相对于简单的角点，这些特征检测具有以下优点\cite{slam14}:

%\begin{itemize}
$\bullet$ 可重复，即相同的特征可以在不同的图片中找到。

$\bullet$ 可区别，即不同的特征有不同的数值表示。

$\bullet$ 高效率，即特征点数远远小于像素数。
%\end{itemize}

实际上，特征点可以分为局部特征点和全局特征点，如以上提及的三种特征点实际上是局部特征点，局部特征点顾名思义就是基于图像的突出区域，可明显区分图像的特征，一般而言，局部特征需要具有旋转不变性，对光线强度变化的鲁棒性等，使用局部特征检测算法可以从图像中提取一系列称为感兴趣区域的特征，由一系列向量组成。相反，全局特征是将图像表示为一个向量，向量的值反映图像特定属性，可以是颜色、纹理或形状等不同特征。当区分图像是海洋还是森林时，使用全局颜色描述子可以很好地将图像归类，二者的区别如图~\ref{global-local}所示。
\begin{figure}[!htbp]
\centering
\includegraphics[width=4.8in]{chapter3/global-local.jpg}
\caption{全局特征与局部特征描述}
\label{global-local}
\end{figure}

选用哪种特征描述子取决于图像处理的情景，由于关于地理空间的航拍图像在颜色、纹理等全局特征方面相似度极高，不易辨识，难以找到图片之间的匹配关系，所以本文选用局部特征描述子：SIFT（Scale-Invariant Feature Transform）。

特征点提取包括特征点检测与特征点描述两部分，在此以SIFT特征点为例，简单介绍一下检测子与描述子的使用方法。检测子有很多，使用较多的有Harris Detector\cite{Harris-corner}，FAST Detector\cite{Rosten-FAST}。Harris检测子结合边缘与角点检测方法得到图像不同方向的自相关系数变化率，例如强度变化率，该方法将局部特征描述为对称的自相关系数矩阵。FAST检测子考虑某像素周围一定大小的圆，该圆中像素强度与该像素作比较，如果大于或小于某一阈值则定义该像素为角点，这两种检测子对尺度变化不具有不变性。由于航拍图像拍摄高度不同，导致图像尺度变化较大，因此这两种检测子无法用于对航拍图像的特征检测。而SIFT 作为一种具有尺度、放缩不变，对光线明暗鲁棒的特定，适合应用于环境复杂的航拍图形的特征特取与描述中。SIFT算法由四个步骤组成：尺度空间极值检测，兴趣点（关键点）定位、梯度方向计算以及兴趣点描述。第一个阶段是使用DoG（Difference of Gaussian）算子与图像做卷积确认潜在的兴趣点，这里使用DoG 算子代替LoG（Laplacian of Gaussian）算子以提高计算速度。

LoG是二阶导数的线性组合，也是用来检测斑点(Blob)的检测子，给出一张图片$I(x,y)$，则尺度空间$L(x,y,\sigma)$是由图片$I(x,y)$与不同大小的高斯核$G(x,y,\sigma)$卷积得到的，表示为

\begin{equation}
L(x,y,\sigma) = G(x,y,\sigma)*I(x,y)
%\label{laplacian}
\end{equation}
其中，
\begin{equation}
G(x,y,\sigma) = \frac{1}{2\pi \sigma ^2}e^{\frac{-(x^2 + y^2)}{2 {\sigma}^2}}
%\label{laplacian}
\end{equation}
据此可计算Laplacian算子为
\begin{equation}
{\nabla}^2 L(x,y,\sigma)=L_{xx}(x,y,\sigma)+L_{yy}(x,y,\sigma)
%\label{laplacian}
\end{equation}
上式对大小为$\sqrt{2\sigma}$的斑点有最强的响应，然而该算子严重依赖图像中斑点大小与用来平滑图像的高斯核大小，即大小不同的高斯核函数与大小相近的斑点卷积有最强的响应。为了在一幅图像中自动检测不同大小的斑点，多尺度的方法应用而生\cite{Lindeberg-Feature-detection}，通过尺度不同的归一化Laplacian算子对图片进行平滑从而在不同尺度空间找到大小不同的斑点，尺度归一化的高斯核函数为式~\ref{nor-lap-equation}。

\begin{equation}
{\nabla}^{2}_{norm} L(x,y,\sigma)={\sigma}^2 (L_{xx}(x,y,\sigma)+L_{yy}(x,y,\sigma))
\label{nor-lap-equation}
\end{equation}

$\sigma$较大时对应图片的粗略特征，较小时对应图片的精细特征。同时LoG算子是对称的，所以斑点旋转对检测没有影响，但是LoG算子耗费计算资源。为了加速计算并在尺度空间检测到稳定的兴趣点，Lowe\cite{Lowe-DistinctiveImageFeatures}提出了高斯差分尺度空间，简称DoG Scale Space。如式~\ref{gaussian} 所示，利用不同尺度的高斯差分核函数与图像卷积生成。
\begin{equation}
\begin{split}
D(x,y,\sigma) &= (G(x,y,k\sigma)-G(x,y,\sigma))*I(x,y) \\
&= L(x,y,k\sigma)-L(x,y,\sigma)
\label{gaussian}
\end{split}
\end{equation}

计算DoG的关键步骤是构建图像金字塔：对于某个图像，使用大小不同的高斯核与该图像卷积，可以得到图像模糊程度不同但大小一致的图像，这些图像构成了一个八度（Octave），而后对这些图像进行降采样，降采样就是对Octave中的图像隔行隔列采集像素，最后图像尺度（Scale）变为原来大小的$1/4$，构成下一个Octave。这样就避免了对不同尺度空间下的图像进行卷积，减少计算增加效率。为了寻找尺度空间的极值点，在DoG尺度空间中，每个采样点要与其周围所有相邻点（8邻域中，一个像素点周围共有26个像素点）比较，若该点是极大值或者极小值则定位为兴趣点。整个过程可以用图~\ref{octave}描述。

检测到特征点后，需要定量地表示各个特征点的强度、方向等特征，为特征点匹配做准备。Lowe提出用一个128维度的向量表示每个特征点，首先对每个关键点周围的$16\times 16$ 邻域内所有像素计算其梯度的幅值与方向，用直方图统计邻域像素的梯度方向，梯度直方图将$0-360^{\circ}$分为8个区间，对得到的直方图进行高斯平滑，以减少突变带来的影响。选取直方图的峰值作为该关键点处邻域梯度的主方向。所以在每个$4\times 4$的象限内，将每个像素的主方向加权到直方图的8个方向区间中的一个，计算一个梯度方向直方图，这样对于每个特征点可以形成$4\times 4 \times 8 = 128$ 维的描述子，计算描述子的过程如图~\ref{siftDescriptor}。

\begin{figure}[!htbp]
\centering
\includegraphics[width=5.4in]{chapter3/octave.jpg}
\caption{图像金字塔的构建}
\label{octave}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/siftDescriptor.jpg}
\caption{SIFT描述子的构建}
\label{siftDescriptor}
\end{figure}

%%在此需要插入4张sift检测的图片，甚至是sift与surf特征点的比较框
借助开源库OpenCV实现SIFT特征点检测，结果如图~\ref{sift-result}所示。在使用OpenCV构建SIFT 检测子时，需要设置两个参数：峰值阈值（Peak threshold）与边缘阈值（Edge threshold）。峰值阈值用来消除DoG尺度空间中绝对值较小的峰值，该值越大，得到的SIFT特征点越少。边缘阈值用来消除DoG尺度空间中曲率较小的峰值，该值越大，得到的SIFT特征点越多。
图~\ref{sift-result}中圆圈的大小代表兴趣点（斑点）的大小（size），圆圈半径越大，代表特征点的强度（response）越小，圆圈中的线段代表SIFT特征点的主方向。
\begin{figure}[!htbp]
\centering
 \subfigure{
	\includegraphics[width=2.9in]{chapter3/testPho/1_res.png}
    }
 \subfigure{
	\includegraphics[width=2.9in]{chapter3/testPho/2_res.png}
    }
 \subfigure{
	\includegraphics[width=2.9in]{chapter3/testPho/3_res.png}
    }
 \subfigure{
	\includegraphics[width=2.9in]{chapter3/testPho/4_res.png}
    }
%\includegraphics[width=5.0in]{chapter3/siftDescriptor.jpg}
\caption{SIFT特征点检测结果}
\label{sift-result}
\end{figure}

\section{特征点匹配}
上面提到SIFT作为特征点检测与描述的方法，特征点提取并描述后，就需要找出图像间通过特征点联系的关系，即每张图像中的特征点如何与其他图像的特征点进行匹配，找到相近的特征点并去除错误匹配的特征点。特征点匹配解决航拍图像之间的数据关联问题，将无序的图片数据集建立关联关系，为下文计算相机位姿并还原特征点的三维坐标提供基础。一对匹配点指的是真实空间里同一个点，如果将不同图片中对应同一个真实点的特征点连在一起，将使数据集中所有无序图片“有序化”。同样如果匹配点中有大量的错误匹配，将对后续的位姿估计与恢复三维结构产生不利影响，所以如何去除错误匹配同样至关重要。下面就这两方面展开讨论。

%为了找出数据集中所有图像之间的匹配关系，需要每张图像与其他图像做比较，而对于大范围的图像数量较多的数据集，图像间的匹配
假设有图片集$I = \{I_i | i=1...N_I\}$，对于每张图片$I_i$，可以检测到的局部特征点为$F_i=\{(x_j,f_j) | j=1...N_{F_i}\}$，其中$x_j$表示特征点位置坐标，$f_j$ 表示特征点描述子，如果是SIFT特征点，则$f_j$是一个128维向量。对于两图像的特征点$F_i$与$F_j$匹配，最简单的是使用暴力匹配，即$F_i$每个特征点与$F_j$中特征点测量两个向量之间的距离，需要进行$N_{F_i} \times N_{F_j}$次距离比较。对于像SIFT这样的浮点描述子，一般采用欧拉距离作为衡量依据，而对于像BRIEF（ORB特征描述子）二进制描述子，则一般使用汉明距离作为度量依据。

%插入一组SIFT匹配的图片，对应欧拉距离那句话

但是当图片长宽较大，分辨率较高时，图片特征点会很多，这时采用暴力匹配会导致效率很低，一般采用近似最近邻（Approximate Nearest Neighbor, ANN）匹配方法。事实上，无论SIFT特征匹配还是数据库检索本质上是相同的，都是使用距离函数在高维矢量空间中寻找相近对象的过程。常用的方法是K近邻查询，设置查询点与正整数K，从另一个数据集中根据距离公式找到距离最近的K个数据，如果K=1，该方法变为最近邻查询算法。K近邻查询算法是通过构建KD-Tree或者R-Tree等实现的。

在此简单介绍本文采用的最近邻匹配库FLANN（Fast Library for Approximate Nearest Neighbors），FLANN具有一种内部机制，该机制可以根据数据本身选择合适的算法来处理数据集，基于FLANN的匹配准确、快速。FLANN需要配置两个参数：indexParams和searchParams。indexParams 可以选择为LinearIndex、KTreeIndex、KMeansIndex、CompositeIndex和AutotuneIndex，其中KTreeIndex灵活且可被并行处理。对于searchParams，用来指定索引树被遍历的次数，即check值，该值越大则匹配的时间越长，当然也越准确。

即使最近邻匹配也无法保证所有匹配的特征点正确，Lowe\cite{Lowe-ObjectRecognition}在1999提出一种简单的方法可以剔除大部分的错误匹配。对于某个特征点向量，如果查询数据集中与其距离最近的特征点向量与次近的特征点向量距离之比小于0.8，可以避免约90\%的错误匹配，但正确匹配也因此变少。使用FLANN并采用Lowe提出的方法可以去掉大部分的错误匹配，图~\ref{matches-single}显示760对匹配特征点（最近邻筛选，系数取0.8）以及红点表示的未匹配特征点。
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/matches/matches.jpg}
\caption{FLANN匹配}
\label{matches-single}
\end{figure}

从上图可以看出，使用Lowe提出的方法刷选最近邻匹配结果后仍然存在错误匹配点，这是因为刚刚的匹配方法仅基于特征描述子的向量值，没有考虑图片的几何关系，也就无法保证匹配特征点对应相同的场景，因此本文利用投影几何相关知识再次过滤错误的匹配点。

不同投影描述图片间不同的几何关系，例如，单应性变换描述相机拍摄的二维图片之间纯旋转和平移变换。对极几何通过本质矩阵（Essential matrix）$E$和基本矩阵（Fundamental matrix）$F$ 描述移动相机的关系，并可通过三焦张量扩展到三视图间的关系。无论哪种变换关系，有效的变换可以满足大部分匹配点的几何关系，那么这种变换或者匹配点被认为是有效的。本文在去除错误匹配时，使用了对极约束关系，这将在三维重建的初始化时详细阐述，以下首先介绍一下单应性变换，因为单应性变换不仅可以用来筛选错误匹配点，也可以用来计算两视图纯旋转关系，从而避免使用纯旋转关系的两视图用于三维重建的初始化，造成对极约束永远满足。

单应性又称射影变换、保线变换\cite{Richard-MultipleView}，与坐标系无关。映射$h$是射影变换的充要条件是：存在一个$3\times 3$非奇异矩阵$H$，使得任意一个矢量$x$表示的坐标点都满足$h(x)=Hx$。即
\begin{equation}
\lambda
\left[\begin{array}{ccc}
u_2 \\
v_2 \\
1 \\
\end{array}
\right]=\left[\begin{array}{ccc}
h_{11} & h_{12} & h_{13} \\
h_{21} & h_{22} & h_{23} \\
h_{31} & h_{32} & h_{33} \\
\end{array}\right]
\left[\begin{array}{ccc}
u_1 \\
v_1 \\
1 \\
\end{array}\right]
\label{homography-equation}
\end{equation}

由上式可知，单应性变换表示两个平面的空间位置关系，或两视图纯旋转的位置关系，如图~\ref{homography}所示。
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/homography.png}
\caption{单应性变换}
\label{homography}
\end{figure}

矩阵$H$是一个齐次矩阵，在$H$的九个元素中有八个独立比率，即交比（Cross Radio）$h_{11}:h_{12}:h_{13}:h_{21}:h_{22}:h_{23}:h_{31}:h_{32}:h_{33}$不会因为$H$乘以非零因子而变化，所以单应性变化有八个自由度，而每个二维特征点可提供2个方程，故需要解算$H$需要至少4对匹配点。已知两视图的4对匹配特征点坐标$x_1,x_2$，
单应性变换表示为$x_2=Hx_1$，这是齐次矢量方程。三维矢量$x_2$和$Hx_1$不相等，实际可以写为$\lambda x_2 = Hx_1$。单应性变换可以变形为$x_2 \times Hx_1=0$（矢量与自身外积为零），记为$[x_2]_\times Hx_1=0$，其中$[x_2]_\times$表示3维矢量$x_2=(u_2,v_2,1)$的$3\times 3$的反对称矩阵，如式~\ref{inverse-sym-equation}。
\begin{equation}
[a]_\times=\left[\begin{array}{ccc}
0 & -a_3 & a_2 \\
a_3 & 0 & -a_1 \\
-a_2 & a_1 & 0 \\
\end{array}\right]
\label{inverse-sym-equation}
\end{equation}

设$h_i$为$H$的第$i$ 行，计算单应性矩阵的过程可以写成下式~\ref{homography-tuidao-equation}。
\begin{equation}
\begin{split}
\left[\begin{array}{ccc}
u_2 \\
v_2 \\
1 \\
\end{array}
\right]_\times
\left[\begin{array}{ccc}
h_1 \\
h_2 \\
h_3 \\
\end{array}\right]x_1 &=
\left[\begin{array}{ccc}
u_2 \\
v_2 \\
1 \\
\end{array}
\right]_\times
\left[\begin{array}{ccc}
h_1 x_1 \\
h_2 x_1 \\
h_3 x_1 \\
\end{array}\right]=
\left[\begin{array}{ccc}
u_2 \\
v_2 \\
1 \\
\end{array}
\right]_\times
\left[\begin{array}{ccc}
x_1^T h_1^T \\
x_1^T h_2^T \\
x_1^T h_3^T \\
\end{array}\right] \\
&=
\left[\begin{array}{ccc}
0 & -1 & v_2 \\
1 & 0 & -u_2 \\
-v_2 & u_2 & 0 \\
\end{array}\right]
\left[\begin{array}{ccc}
x_1^T & 0_{1\times 3} & 0_{1\times 3} \\
0_{1\times 3} & x_1^T & 0_{1\times 3} \\
0_{1\times 3} & 0_{1\times 3} & x_1^T \\
\end{array}\right]
\left[\begin{array}{ccc}
h_1^T \\
h_2^T \\
h_3^T \\
\end{array}\right]\\
&=\left[\begin{array}{ccc}
0_{1\times 3} & -x_1^T & v_2 x_1^T \\
x_1^T & 0_{1\times 3} & -u_2 x_1^T \\
-v_2 x_1^T & u_2 x_1^T & 0_{1\times 3} \\
\end{array}\right]
\left[\begin{array}{ccc}
h_1^T \\
h_2^T \\
h_3^T \\
\end{array}\right]=0
\end{split}
\label{homography-tuidao-equation}
\end{equation}

上式简写为$Ab=0$，其中$A$为$3\times 9$矩阵，$rank(A)=2$，这是因为每对匹配点仅提供两个量：$u$和$v$，也可以直接从$A$矩阵看出，将$A$的第一行乘以$u_2$与第二行乘以$v_2$相加得到第三行。$b$为$9\times 1$ 矩阵，由于只需要维持$b$ 各项的交比不变即可，所以有8 个自由度，故需要4 对匹配点即可在这个线性系统上获得足够的条件计算得到单应性矩阵$H$的各项。由以上分析可知当给定的匹配点为4个时，方程有精确解。但是大部分的匹配问题中匹配点的数目远不止4 对，如果多于4对，那么$Ab=0$的解是超定的，如果所有的匹配点的位置是精确的，那么$Ab=0$的解仍然是精确的，但是通常情况匹配点中有噪声，存在错误匹配等问题，所以这时候方程存在最小二乘解（超定解）。除了零解可以忽略外，方程解可以通过奇异值分解找到近似解。

%实际上SVD可以看做特征值分解的推广，SVD在主成分分析、求解方程的最小二乘解中广泛应用。
奇异值分解（Singular Value Decomposition, SVD）可以将一个矩阵，无论是否满秩，均可以分解为$A=UDV^T$形式，其中$UU^T=I$和$V V^T=I$，即$U$和$V$ 是正交矩阵，$D$是对角阵，对角元素非负。根据SVD求解单应性变换的最小二乘解，可以得到：最小二乘解是$A^T A$的最小特征值的特征向量。具体而言，假设有n 个匹配点，则$A$为$2n\times 9$ 的矩阵，且$A=UDV^T$，且$D$对角阵元素按降序排列，那么$b$是$V$的最后一列。因为$Ab=0$的解是$A$的零空间，而且$rank(A)=n-1$，所以$A$的零空间只有一个基向量，即SVD 中最小特征值对应的特征向量$V$的最后一列。

奇异值分解是广泛用于图像处理中的一种算法，不仅在求单应性矩阵时使用，下文亦有涉及。但是在这里介绍一种本文采用的另一种方法――随机抽样一致性（Random Sample Consensus, RANSAC），该方法基于统计理论，在图像领域中也广泛应用，可以通过多次迭代的方法估计含有噪声数据模型的参数。RANSAC一般过程为从一组含有外点（噪声）的数据中随机选取数个数据，估计模型参数，然后将估计得到的模型应用于剩余的其他数据，从而得到该模型的误差，重复以上过程，直到误差达到某设定阈值或迭代次数达到设定最大次数，停止迭代，选取最优的一组模型作为最终结果。根据最终结果与事先设定的阈值可以过滤一部分外点并得到较为精确的模型。由此可知，估计单应性变换需要4对匹配点即可计算一个单应性变换，所以RANSAC 每次选取4 对匹配点迭代计算得到单应性矩阵，而后将单应性矩阵应用于剩余的匹配点，计算误差，过滤外点。如图~\ref{Homography+RANSAC1}所示，其中$(a)$与$(c)$是仅根据SIFT 描述子通过最近邻匹配过滤的结果。$(b)$与$(d)$是使用RANSAC方法通过单应性变换过滤的结果，从图中可以直观看出使用RANSAC可以过滤掉一部分错误的匹配。

\begin{figure}[!htbp]
\centering
 \subfigure[KITTI-0400与KITTI-0415最近邻匹配点]{
	\includegraphics[width=6.0in]{chapter3/matches/000400_H_000415.png}
    }
\subfigure[KITTI-0400与KITTI-0415单应性变换过滤匹配点]{
	\includegraphics[width=6.0in]{chapter3/matches/000400_U_000415.png}
    }
\subfigure[KITTI-0400与KITTI-0424最近邻匹配点]{
	\includegraphics[width=6.0in]{chapter3/matches/000400_H_000424.png}
    }
\subfigure[KITTI-0400与KITTI-0424单应性变换过滤匹配点]{
	\includegraphics[width=6.0in]{chapter3/matches/000400_U_000424.png}
    }
    \caption{两组匹配图像的单应性刷选结果}
    \label{Homography+RANSAC1}
\end{figure}

%\begin{figure}[!htbp]
% \subfigure[KITTI-0400与KITTI-0424最近邻匹配点]{
%	\includegraphics[width=6.0in]{chapter3/matches/000400_H_000424.png}
%    }
%\subfigure[KITTI-0400与KITTI-0424单应性变换过滤匹配点]{
%	\includegraphics[width=6.0in]{chapter3/matches/000400_U_000424.png}
%    }
% \subfigure[]{
%	\includegraphics[width=2.95in]{chapter3/matches/000415_H_000424.png}
%   }
% \subfigure[]{
%	\includegraphics[width=2.95in]{chapter3/matches/000415_U_000424.png}
%    }
%\includegraphics[width=5.0in]{chapter3/siftDescriptor.jpg}
%\caption{KITTI-0400与KITTI-0424单应性刷选结果}
%\label{Homography+RANSAC2}
%\end{figure}

同样从表~\ref{table:matches}中也可以看出，其中对角线上的数字表示图~\ref{Homography+RANSAC1}中三张原始图片（名称分别为KITTI-0400、KITTI-0415、KITTI-0424）提取到的SIFT特征点数量，其他表格中的数值，例如$55/32$分别表示使用RANSAC方法前得到的匹配点数目，以及使用RANSAC后过滤剩下的匹配点数目。

\begin{table}[!htbp]
	\caption{匹配点过滤}
	\centering
	\begin{tabular}{c|ccc}
		\hline
		\diagbox[dir=SE]{\bfseries 图片名}{\bfseries 特征点}{\bfseries 图片名} &  KITTI-0400 &  KITTI-0415 &  KITTI-0424\\
		\hline
%		\hline
		 KITTI-0400&  $861$&  $55/32$ & $75/46$\\
		%\hline
		 KITTI-0415 & $ \cdots $ & $370$ & $19/10$ \\
		%\hline
         KITTI-0424 & $ \cdots $ & $ \cdots $ & $599$ \\
		\hline
	\end{tabular}
	\label{table:matches}
\end{table}

经过FLANN匹配特征点，使用对极约束去除错误匹配，得到的图片匹配点基本上是正确的，基于此我们可以对数据集中无序图片构建它们的关联结构，对于多张图片对应的同一个匹配点，设置统一标号，称为track。例如图片一中第125个特征点，图片二中第259 个特征点，图片三中第6个特征点匹配，实际上它们对应实际地理空间的同一个点，所以可以使用同一个track序号标记这些匹配点。当然同一个track序号在某张图片所有匹配特征点中只能出现一次，这很容易理解，一张图片里不可能出现与自身匹配的特征点，据此也可以过滤一部分错误匹配。如此可以使整个数据集通过特征点匹配构成关联关系，为下面的多视图重建打下基础。

\section{多视图重建}
整个数据集通过特征点匹配构成关联关系，至此可以采用多视图几何知识，根据图像间的匹配关系得到每张图像对应相机的位姿关系，并恢复匹配特征点的三维空间坐标，增量式的SfM方法应用而生。首先从track点最多的两张图片入手，恢复两视图匹配特征点的三维空间位置与两视图对应的摄像机的相对关系，而后加入第三张图片，计算第三张图片与前两张图片得到的3D 点的关系，使用三角测量方法还原第三张图片的特征点3D位置坐标，如此循环。其中每加入一张图片使用光束平差法优化3D点与相机位姿。最终得到较为理想的点云数据。

\subsection{两视图重建}
在式~\ref{transform-equation}中，$z_c$为每个像素的深度，在单目成像时，$z_c$是未知的。所以已知某点在世界坐标系中的坐标，很容易得到该点的像素坐标，但是反之，已知该点像素坐标无法得到其世界坐标。所以从单张图像中无法还原场景的三维结构，所以下文介绍从两张存在特征点匹配的图片中恢复场景的三维结构。两视图几何最核心的原理是对极约束，在阐述对极约束相关知识之前，我们先简述3.3节中关于摄像机成像过程涉及的相关坐标系。如果三维空间的一个点$X$，其在相机所成的像坐标为$x$，则有$\lambda x = PX=k[R\ t]X$，其中$\lambda$表示每个像素的深度，$[R\ t]$表示摄像机坐标系与世界坐标系的刚体变换。

对极约束基础是两视图同时观察到真实空间的同一个物体，分别用$C_1,C_2$ 表示两视图的相机光心坐标，两相机同时观察三维世界中同一点，该点在$C_1$ 相机坐标系的三维坐标为$X_1$，在$C_2$ 相机坐标系的三维坐标为$X_2$。对极几何中，世界坐标系原点选为$C_1$，其坐标轴方向与$C_1$相机坐标系相同，所以$C_1$ 相机成像矩阵为$P=K[I\ 0]$，而$C_2$相机成像矩阵为$P=k[R\ t]$，所以$X_2=RX_1 +t$，表示$C_1$ 相机坐标系中的点经过$R,t$ 可以转换到$C_2$ 相机坐标系中表示的过程。从$C_2$为原点坐标系中，我们可以得到如图~\ref{epipolor}中对极平面（Epipolar plane，图中橙色三角形表示）三边对应的的矢量为$t,X_2,X_2 -t$，其中$X_2 -t = RX_1$。
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/epipolor.png}
\caption{对极几何约束}
\label{epipolor}
\end{figure}

由于三边共面有向量的混合积为零，如式~\ref{epipolor-equation}所示。式中$[t]_\times X_2 = t\times X_2$表示垂直于对极平面的向量（绿色箭头）。
\begin{equation}
\begin{split}
(X_2-t)^T\cdot [t]_\times X_2 &= 0\\
(RX_1)^T\cdot [t]_\times X_2 &= 0\\
-X_2^T[t]_\times RX_1 &= 0 \\
\end{split}
\label{epipolor-equation}
\end{equation}

式~\ref{epipolor-equation}用简洁形式表示为$-X_2^T EX_1 = 0$，其中$E=[t]_\times R$，Longuet Higgins首先发现的这一关系并称$E$为本质矩阵。本质矩阵是$3\times 3$的矩阵，它将旋转与平移的复杂关系简洁形式地表达出来。

这里还有几个概念需要说明，所有由3D点、相机中心$C_1,C_2$组成的平面（对极平面）与成像平面相交于两条直线，称为极线（Epipolar line，图~\ref{epipolor} 中蓝色直线$Fx_1$，$Fx_2$），$C_1$成像平面中一点$\lambda X_1$对应的极线是对极平面与$C_2$ 成像平面的交线（图~\ref{epipolor}中蓝线$Fx_1$）。如果改变3D 点的位置形成另外一个对极平面，其与成像平面的交线为另外一条极线，所有这些极线交于一点，该点称为极点（图~\ref{epipolor} 中$e_1,e_2$）。已知点$x$ 在直线上的充要条件是$x^T L=0$，从极线约束$-X_2^TEX_1 = 0$与齐次坐标的性质，可以得到$EX_1$是过$X_2$的直线，也就是与$C_2$成像平面的极线平行的直线，由于所有的极线$EX_1$都通过同一个点（极点$e_2$），所以$e_2^T E = 0, E e_1=0$。极点与极线的关系由图~\ref{epipolor-line}可以证明，图(a) 中的点匹配图(b)中同颜色的点，对应的极线为图(b)中同颜色的直线，可以看出图(b)中每条直线经过图(a)匹配的特征点，而这些极线交于一点，即极点。

\begin{figure}[!htbp]
\centering
\subfigure[左视图中的特征点]{
\includegraphics[width=5.4in]{chapter3/epipolorLine/points.png}
}
\subfigure[右视图中的特征点与对应极线]{
\includegraphics[width=5.4in]{chapter3/epipolorLine/lines.png}
}
\caption{极点与极线的关系}
\label{epipolor-line}
\end{figure}

针对极点与极线的关系，本文基于开源库OpenCV使用对极约束过滤错误匹配点。在使用OpenCV库函数时，需要设置距离阈值。具体方法是在两视图A、B 中，A 视图的特征点对应在B 视图中的极线与其在B 视图匹配的特征点的欧拉距离小于9 个像素，则认为该对特征点的匹配是有效的。按照这一原则，在使用上文提到的最近邻方法去除错误匹配后，通过极点与极线距离又可以去除一部分错误匹配。相比最近邻方法，这种过滤错误匹配的方法考虑了两视图的几何关系。

已知$KX_1 = x_1$，其中$K$为内参矩阵，$X_1$表示$C_1$为原点的坐标系中三维点坐标，$x_1$表示$C_1$的像素坐标系中二维点坐标。这里和上文相同都是其次坐标，所以是在齐次意义上相等。带入$-X_2^TEX_1=0$中有式~\ref{fundunmental-equation}，其中$F$称为基本矩阵，$rank(F) = 2$，$F$的自由度为8。

\begin{equation}
\begin{split}
X_2^TEX_1 &=0 \\
x_2^Tk^{-T} E k^{-1} x_1 &=0 \\
x_2^TFx_1 &=0 \\
\end{split}
\label{fundunmental-equation}
\end{equation}

基于式~\ref{fundunmental-equation}可以得到下式~\ref{fundunmental-tuidao-equation1}。
\begin{equation}
\left[\begin{array}{ccc}
u_i^2 & v_i^2 & 1
\end{array}\right]
\left[\begin{matrix}
f_{11} & f_{12} & f_{13} \\
f_{21} & f_{22} & f_{23} \\
f_{31} & f_{32} & f_{33}
\end{matrix}\right]
\left[\begin{array}{c}
u_i^1 \\
v_i^1 \\
1
\end{array}\right] =0
\label{fundunmental-tuidao-equation1}
\end{equation}


根据式~\ref{fundunmental-tuidao-equation1}，一对匹配点可以得到一个方程，所以至少需要8对匹配点才能计算得到矩阵$F$。如式~\ref{fundunmental-tuidao-equation2}所示，可以简写为$AX=0$，与上文计算单应性矩阵的方法相同，由于匹配点远不止8对，所以此时方程存在最小二乘解，使用SVD对左侧矩阵$A$分解得$UDV^T$，则基本矩阵即为$V$的最后一列重新排列成$3\times 3$的形式，当然最后还需要保证$rank(F)=2$，这可以通过将$F$ 进行SVD分解并使最小奇异值设为零得到秩为2的$F$。至此我们可以根据至少8对匹配点解算出基本矩阵$F$，该方法称为八点法，在两视图重建中被广泛使用。另外一种方法是使用RANSAC代替SVD分解，避免噪声带来的影响，需要注意八点法选取的8对匹配点不能共面。

本文八点法的实现是基于开源库OpenGV，在使用OpenGV库函数计算本质矩阵时，需要设置角度阈值\cite{Kneip-OpenGV}。使用库函数计算的本质矩阵可以恢复3D 点坐标，该3D点与光心所成方向向量记为$f_{repr}$，与该3D点对应的2D点与光心所成的方向向量记为$f_{meas}$，二者夹角记为$\theta$，该角度就是需要设置的阈值。由于归一化这两个向量，所以向量$f_{meas}$ 与$f_{repr}$ 的数量积为$cos\theta$，为了将所有向量数量积限制为正数，则重投影误差可以表示为$\epsilon=1-f_{meas}^Tf_{repr}=1-cos\theta$，本文设置该角度$\theta \leq 0.004$。

\begin{equation}
\left[\begin{matrix}
u_1^1u_1^2 & u_1^1v_1^2 & u_1^1 & v_1^1u_1^2 & v_1^1v_1^2 & v_1^1 & u_1^2 & v_1^2 & 1 \\
u_2^1u_2^2 & u_2^1v_2^2 & u_2^1 & v_2^1u_2^2 & v_2^1v_2^2 & v_2^1 & u_2^2 & v_2^2 & 1 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
u_8^1u_8^2 & u_8^1v_8^2 & u_8^1 & v_8^1u_8^2 & v_2^1v_2^2 & v_8^1 & u_8^2 & v_8^2 & 1 \\
\end{matrix}\right]
\left[\begin{array}{c}
f_{11} \\
f_{21} \\
f_{31} \\
f_{12} \\
f_{22} \\
f_{32} \\
f_{31} \\
f_{32} \\
f_{33}
\end{array}\right] =0
\label{fundunmental-tuidao-equation2}
\end{equation}

在得到基本矩阵$F$后，如果已知相机内参矩阵$K$，则可以得到本质矩阵$E$，由示意图~\ref{essentialMatrix}所示，从本质矩阵可以恢复相机之间的旋转平移关系。
\begin{figure}[!htbp]
\centering
\includegraphics[width=4.0in]{chapter3/essentialMatrix.png}
\caption{本质矩阵恢复旋转平移关系}
\label{essentialMatrix}
\end{figure}

由于$P_2[0,0,0,1]^T=[R |\ t][0,0,0,1]^T = t$，且$e_2^T E=0$，故有$t^TE=0$，由SVD知$t$是$E$的左零空间，所以$t=U[:\ ,-1]$，即$E$奇异值分解后$U$的最后一列，因此$t=-u_3,u_3$，故有式~\ref{Rt-fromE-equation}，其中$U = \left[\begin{array}{ccc} u_1 & u_2 & t\end{array}\right]$。

\begin{equation}
E = U \left[\begin{matrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{matrix}\right]
V^T=[t]_\times |\ R=U
\left[\begin{matrix}
0 & 1 & 0 \\
-1 & 0 & 0 \\
0 & 0 & 0 \\
\end{matrix}\right]
U^T |\ UYV^T
\label{Rt-fromE-equation}
\end{equation}

据式~\ref{Rt-fromE-equation}可以得到下式~\ref{result-y-equation}:
\begin{equation}
\left[\begin{matrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{matrix}\right]
=
\left[\begin{matrix}
0 & 1 & 0 \\
-1 & 0 & 0 \\
0 & 0 & 0 \\
\end{matrix}\right]
Y
\label{result-y-equation}
\end{equation}

因此可以得到矩阵$Y$的值，$Y\,$ or $\, Y^T=\left[\begin{matrix}
0 & 1 & 0 \\
-1 & 0 & 0 \\
0 & 0 & 0 \\
\end{matrix}\right]$，所以$R$有两种可能值，这是因为$R=UYV^T$。

综上通过$E$恢复旋转矩阵$R$和平移矩阵$t$有四种可能的解，图~\ref{essentialfromRT}形象地展示了分解本质矩阵得到四个解的过程。保持成像平面内的点（红点）不变的情况下，可以画出四种可能情况，但只有(a)是正确的，因为其有正深度，所以恢复旋转与平移后需要将3D点带入图四个解中，检测该点在两个相机中的深度，即可确定$R$和$t$。实际上利用$E$的内在性质，使用“五点法”可以解算得到$E$，其只有五个自由度：旋转(3)，平移(3)，缩放因子(-1)。

\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/essentialfromRT.png}
\caption{分解本质矩阵得到的四个解}
\label{essentialfromRT}
\end{figure}

\subsection{三角测量法}
在得到相机的位姿后，需要恢复匹配点的三维坐标信息，该方法称为三角测量（Triangulation），三角测量指不同视角观察同一个点的夹角，确定该点的位置。三角测量最早由高斯提出并应用于天文学中。在SfM中，我们使用三角测量估计匹配点的像素深度。

小孔相机成像模型简化为$\lambda x_1=P_1 X_1$，即$[x_1]_\times P_1 X_1 = 0$，由于一个视图可以提供两个方程，所以只需要两个视图即可计算得到3D点坐标，但由于噪声影响，空间中的多视图射线不一定交于一点，所以仅由两视图恢复得到的三维点坐标存在不准确的问题，一般使用多视图进行三角测量恢复多视图匹配的3D点坐标。如图~\ref{triangulation}所示，仅由视图$c_1$和$c_2$得到的3D坐标（灰点）与多视图确定的3D坐标（红点）存在位置误差，工程中普遍采用多视图同时进行三角测量。
\begin{figure}[!htbp]
\centering
\includegraphics[width=4.0in]{chapter3/triangulation.png}
\caption{三角测量示意图}
\label{triangulation}
\end{figure}

多个视图匹配的同一个3D点在不同视图成像平面的投影为$x_1,x_2,...x_n$，这些投影点具有相同的track序号（第三章第二节末提到的定义），当从多视图中恢复3D位置坐标，该问题又归为对式~\ref{triangulation-equation}进行奇异值分解，变为求解最小二乘解问题。将式~\ref{triangulation-equation}简写为$A_{3n\times 4}X=0$，由于$rank(A_{3n\times 4})=2$ 所以对$A$进行SVD分解，$V$的最后一列就是$A$的右零空间，也就是3D点的位置坐标。
\begin{equation}
\centering
\left[\begin{array}{c}
\left[\begin{array}{c}
u_1\\
v_1\\
1
\end{array}\right]_\times P_1 \\
\left[\begin{array}{c}
u_2\\
v_2\\
1
\end{array}\right]_\times P_2 \\
\vdots
\end{array}\right]_{3n\times 4}
\left[\begin{array}{c}
x\\
y\\
z\\
1
\end{array}\right]_{4\times 1}=0
\label{triangulation-equation}
\end{equation}

三角测量的多视图必须存在平移关系，否则单纯的旋转无法使用三角测量，因为此时对极约束永远满足。在平移存在的情况下，三角测量存在不确定性，当平移很小时，射线夹角很小，计算得到的像素深度不确定性很大；但是平移太大说明图像变化较大，容易导致匹配失效。所以三角测量时，需要考虑多视图夹角，也就是多视图平移大小的影响。故本文在三角测量时，计算track序号对应的所有特征点两两之间的夹角，多视图中至少存在一对匹配特征点，其夹角大于$2^\text{o}$，而后采用上文提到SVD分解的方法恢复3D点坐标，否则舍去该track序号对应的所有特征点，保证得到的3D点尽可能准确。

\subsection{2D-3D位姿求解}
经过以上计算，我们可以得到初始的两视图匹配点的3D位置坐标，也就是我们通过两视图匹配关系得到场景的特征点3D位置坐标，也称为稀疏点云，以及两视图的空间几何关系。至此加入第三张匹配的图片，这时就存在如何计算第三张图片对应的相机坐标系与刚刚三角测量得到的点云的空间位置关系，完成这一步才能继续增加其他图片，完成对数据集的增量式重建过程。

PnP（Perspective-n-Point）是用来求解3D到2D匹配点的几何关系。新图片能通过解决PnP问题与当前的点云模型（两视图或者多视图得到的）配准。PnP问题被用来估计相机位姿，包括未矫正相机的内参矩阵。我们知道对极几何估计2D-2D位置关系，一般使用“八点法”。但若两张图像中一张图像的特征点3D位置已知，那么最少需要三对匹配点就可以估计相机运动，该方法称为P3P。 特征点的3D位置可以由上文的三角测量或者深度传感器确定。3D-2D方法不需要使用对极约束，又可以使用很少的匹配点进行运动估计，其有多种解法，包括直接线性方法（DLT, Direct Linear Transform）、P3P、EPnP（Efficient PnP）、非线性优化构建最小二乘问题求解，本文选用直接线性方法求解PnP问题。

\begin{equation}
\begin{split}
&\left[\begin{array}{ccc}
u \\
v \\
1 \\
\end{array}
\right]_\times
\left[\begin{array}{ccc}
P_1 \\
P_2 \\
P_3 \\
\end{array}\right]X =
\left[\begin{array}{ccc}
u \\
v \\
1 \\
\end{array}
\right]_\times
\left[\begin{array}{ccc}
P_1 X \\
P_2 X \\
P_3 X \\
\end{array}\right] \\
&=
\left[\begin{array}{ccc}
0 & -1 & v \\
1 & 0 & -u \\
-v & u & 0 \\
\end{array}\right]
\left[\begin{array}{ccc}
X^T & 0_{1\times 4} & 0_{1\times 4} \\
0_{1\times 4} & X^T & 0_{1\times 4} \\
0_{1\times 4} & 0_{1\times 4} & X^T \\
\end{array}\right]
\left[\begin{array}{ccc}
P_1^T \\
P_2^T \\
P_3^T \\
\end{array}\right]\\
&=
\left[\begin{array}{ccc}
0_{1\times 4} & -X^T & vX^T \\
X^T & 0_{1\times 4} & -uX^T \\
-vX^T & uX^T & 0_{1\times 4} \\
\end{array}\right]
\left[\begin{array}{ccc}
P_1^T \\
P_2^T \\
P_3^T \\
\end{array}\right]=0
\end{split}
\label{pnp-tuidao1-equation}
\end{equation}

本文仅介绍使用直接线性方法结合SVD分解，其他方法不一一展开，首先回忆最经典的小孔成像模型$\lambda x_1=P_1 X_1$，即$[x_1]_\times P_1 X_1 = 0$，其中$P$为未知量，2D坐标$x$与3D坐标$X$都是已知量，参照求单应性矩阵的方法，我们可以把推导写成如式~\ref{pnp-tuidao1-equation}所示，可以记为$A_1b=0$，其中每对匹配点仅能提供两个方程，所以$rank(A_1)=2$，$A_1$ 为$3\times 12$矩阵，$b$为$12\times 1$的矩阵，所以至少需要6 对匹配点才能计算得到位置关系矩阵$b$。此时等式简写为~\ref{pnp-svd-equation}的组合矩阵，其中每对3D-2D匹配点可以写成类似$A_1$的形式。此时可以对式~\ref{pnp-svd-equation}左侧的组合矩阵运用SVD求解。对于实际中匹配点数量大于6对时，SVD求得的是最小二乘解（超定解）。

\begin{equation}
\left[\begin{array}{c}
A_1\\
A_2\\
\cdots\\
A_6
\end{array}\right]b = 0
\label{pnp-svd-equation}
\end{equation}

通过以上方法，我们可以得到矩阵$P$，由于$P=K[R|\ t]$，所以旋转矩阵$R=K^{-1}P_{[1:3]}$（$P_{[1:3]}$表示$P$的前三列），由于旋转阵$R$是正交矩阵，为了保证这一性质，所以旋转矩阵取式~\ref{main-orthogonal-equation}中$R_+$
\begin{equation}
\begin{split}
&R = UDV^T\\
&R_+ = UV^T\\
\label{main-orthogonal-equation}
\end{split}
\end{equation}
在得到正交旋转矩阵后，需要恢复平移矩阵$t$，方法如式~\ref{main-scale-equation}所示，最后$P=K[R_+ \ t]$
\begin{equation}
\begin{split}
&R=UDV^T\\
&D=diag(\sigma_1,\sigma_2,\sigma_3)\\
& \sigma_1>\sigma_2>\sigma_3\\
&t=K^{-1}\frac{P_4}{\sigma_1}
\end{split}
\label{main-scale-equation}
\end{equation}
%但是这两个过程是相互影响的，分开计算必然导致一定误差
\section{三维重建中的优化处理}
使用多视图几何知识计算相机位姿与特征点的3D坐标时，一般先计算相机位姿，后计算特征点3D坐标。而本章采用的非线性优化问题正是将相机位姿与特征点3D 位置同时优化，实际上Structure from Motion中Structure指特征点3D坐标，而Motion指相机位姿，但是图像配准（Image Registration）和三角测量（Triangulation）分别独立计算必然导致一定的误差，因为二者是紧密关联的：相机姿态的不确定性将增加三角测量的不确定性，反之亦然。没有对二者计算结果的优化，SfM将很快离散到难以恢复的状态。下面将分析说明同时优化相机位姿与3D点坐标的常用方法，称为光束平差法，主要实现方式是最小化重投影误差。
\subsection{光束平差法}
光束平差法是对相机位姿矩阵$P_c$与3D点$X_k$的非线性优化方法，其最小化重投影误差如式~\ref{repro-equation}，其中函数$f(x)$表示3D点的重投影误差，函数$\tau$是3D点在成像平面的投影的像坐标，$\rho_j$是损失函数，用来降低外点带来的影响。
\begin{equation}
x^*=arg\mathop{{min}}\limits_{x} \sum_{j=1}^{k}\left \|f_j(x) \right \|^2
=arg\mathop{{min}}\limits_{x} \sum _{j=1}^{k}\left \|\tau_j(x)-b \right \|^2
%f=\sum_{j}\rho_j(\left \| \pi(P_c,X_k)-x_j \right \|_2^2)
\label{repro-equation}
\end{equation}

为解决该问题，使用最多的方法是梯度下降法，该算法是解决非线性最小二乘问题的通用方法，BA实际上就是非线性最小二乘问题。我们一般使用Levenberg-Marquardt（LM）\cite{Hartley-MultipleView}近似梯度下降法，LM将非线性问题转变为一系列正则线性问题。设$J(x)$ 是$f(x)$或$\tau$ 的雅各比矩阵，最小化重投影误差如式~\ref{repro-tuidao-equation}所示。
\begin{equation}
\mathop{{min}}\limits_{x} \left \|\tau(x)-b\right\|^2=\mathop{{min}}\limits_{x} (\tau(x)-b)^T(\tau(x)-b)=\mathop{{min}}\limits_{x} \tau(x)^T\tau(x)-2b^T\tau(x)
\label{repro-tuidao-equation}
\end{equation}
对式~\ref{repro-tuidao-equation}求导数有
\begin{equation}
2\frac{\partial \tau(x)}{\partial x}^T\tau(x)-2\frac{\partial \tau(x)}{\partial x}b=0
\label{repro-tuidao2-equation}
\end{equation}
其中
\begin{equation}
J=\frac{\partial \tau(x)}{\partial x}=\frac{\partial f(x)}{\partial x}=
\left[\begin{matrix}
\frac{\partial f_1}{\partial x_1} & \vdots & \frac{\partial f_1}{\partial x_n}\\
\frac{\partial f_2}{\partial x_1} & \vdots & \frac{\partial f_2}{\partial x_n}\\
\cdots & \cdots & \cdots\\
\frac{\partial f_n}{\partial x_1} & \vdots & \frac{\partial f_m}{\partial x_n}\\
\end{matrix}
\right]
\label{repro-tuidao3-equation}
\end{equation}
雅可比矩阵反映存在$m$个约束条件的误差函数的变化率，雅可比矩阵的$m$一般不等于$n$，$m$表示组成$f(x)$的等式个数，$n$表示与$f(x)$相关的变量个数，下文会详细解释在光束平差法中这些变量代表的意义。对式\label{repro-tuidao3-equation}中的$f(x)$泰勒展开如式~\ref{repro-tuidao4-equation}，最终结果可简写为$J^TJ\Delta x = -J^Tf$，其中$J^TJ$称为Hessian矩阵。
\begin{equation}
\begin{split}
2\frac{\partial \tau(x)}{\partial x}^T(\tau(x) & +\frac{\partial \tau(x)}{\partial x}\Delta x)-2\frac{\partial \tau(x)}{\partial x}b=0\\
\frac{\partial \tau(x)}{\partial x}^T\frac{\partial \tau(x)}{\partial x}\Delta x &= \frac{\partial \tau(x)}{\partial x}^T(b-\tau(x))\\
\Delta x &=(J^TJ)^{-1}J^T(b-\tau(x))
\label{repro-tuidao4-equation}
\end{split}
\end{equation}

式~\ref{repro-tuidao4-equation}的推导结果称为高斯牛顿法，而LM是基于高斯牛顿法得到的，只是在迭代步长时不使用$\Delta x=(J^TJ)^{-1}J^Tf$，而改为$\Delta x=(J^TJ+\lambda D(x)^TD(x))^{-1}J^Tf$，其中$D(x)$是非负对角矩阵，是$J^TJ$的对角线元素的平方根，$J^TJ+\lambda D(x)^TD(x)$ 称为增广Hessian矩阵。相较高斯牛顿法，LM的优点是可以动态调节$\Delta x$，当下降太快时，使用较小的$\lambda$，反之亦然。经过以上推导可知，实际上光束平差法是一种非线性最小二乘法，关键是如何计算$f(x)$的雅可比矩阵$J$，即$f(x)$ 的梯度，并最终计算步长$\Delta x$，直至重投影误差收敛至最小。

BA过程中雅可比矩阵的计算至关重要，雅可比矩阵随着相机与3D点的数目增加而维度增加。当一个相机内仅可见一个3D点时雅可比矩阵的形式如图~\ref{jagebi}所示，其中$R,C,X,q$分别表示旋转、相机中心（即平移）、3D点和四元数表示的旋转。
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/ba.png}
\caption{雅可比矩阵的组成}
\label{jagebi}
\end{figure}

BA 中针对多个相机多个3D点的雅可比矩阵形式如下所述，最小化误差函数存在$6(F-1)$个运动约束和$3N-1$ 个结构约束，$F$ 指相机个数，$N$ 指3D 点的个数，这可以理解为第一个相机为基准，所以不计入考虑，而每个相机有6 个约束条件，即旋转（3）、相机中心（3），每个3D 点都是3个约束条件，但是缺失一个尺度，所以减去$1$。Hessian 矩阵$J^TJ$ 的维数为$(6F+3N-7)\times (6F+3N-7)$。 既然如此，由于3D 点与相机数量非常庞大，所以BA 得到的雅可比矩阵与Hessian 矩阵维数也很大。如图~\ref{jagebi-shiyi}所示，当两视图可见同一个3D点时，雅可比矩阵如式~\ref{jagebi-shiyi}所示，其中颜色与图~\ref{jagebi}相同的部分有相同物理意义。
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/baall.png}
\caption{一个3D点和两视图的雅可比矩阵示意图}
\label{jagebi-shiyi}
\end{figure}

当得到雅可比矩阵后，需要计算迭代步长，以最小化误差函数。为了下文分析方便，我们设$U=J_c^TJ_c$，$V=J_p^TJ_p$，$U_\lambda=U+\lambda D_c^TD_c$，$V_\lambda=V+\lambda D_p^TD_p$，$W=J_c^TJ_p$，下标$c$表示与相机参数有关的向量，下标$p$表示与3D坐标点参数有关的向量，可以将式$\Delta x=(J^TJ+\lambda D(x)^TD(x))^{-1}J^T(b-f(x))$写为分块矩阵形式，如式~\ref{simple-j-equation} 所示。其中，$U_\lambda$和$V_\lambda$ 是分块对角矩阵，对此可以采用舒尔补方法（Schur Complement）高效求解该方程。
\begin{equation}
\left[\begin{matrix}
U_\lambda & W \\
W^T & V_\lambda
\end{matrix}\right]
\left[\begin{matrix}
\Delta x_c \\
\Delta x_p
\end{matrix}\right]=-
\left[\begin{matrix}
J_c^Tf\\
J_p^Tf
\end{matrix}\right]
\label{simple-j-equation}
\end{equation}
考虑求解线性系统$M[x_1, x_2]^T=[b_1, b_2]^T$，式~\ref{schur-equation}表示分块矩阵$M$的分解过程。其中，$\bar{D}=D-CA^{-1}B$，且$A$应是非奇异方阵，$\bar{D}$被称为舒尔补。
\begin{equation}
M = \left[\begin{matrix}
A & B\\
C & D
\end{matrix}\right]=
\left[\begin{matrix}
1 & 0\\
CA^{-1} & 1
\end{matrix}\right]\left[\begin{matrix}
A & 0\\
0 & \bar{D}
\end{matrix}\right]\left[\begin{matrix}
1 & A^{-1}B\\
0 & 1
\end{matrix}\right]
\label{schur-equation}
\end{equation}
利用矩阵$\left[\begin{matrix}
1 & 0\\
-CA^{-1} & 1
\end{matrix}\right]$左乘该线性系统有下式，其中$\bar{b}_2=b_2-CA^{-1}b_1$。

\begin{equation}
\left[\begin{matrix}
A & B\\
0 & \bar{D}
\end{matrix}\right]\left[\begin{matrix}
x_1\\
x_2
\end{matrix}\right]=\left[\begin{matrix}
b_1\\
\bar{b}_2
\end{matrix}\right]
\end{equation}
至此我们可以得到一个降阶系统$\bar{D}x_2=\bar{b}_2$，求解$x_2$后，回带解算$x_1$。
回归光束平差法本身，由~\ref{simple-j-equation}可以得到降阶系统~\ref{reduce-system-equation}和~\ref{reduce-system2-equation}，矩阵$S=(U_\lambda-WV_\lambda^{-1}W^T)$是舒尔补。
\begin{equation}
(U_\lambda-WV_\lambda^{-1}W^T)\Delta x_c=-J_cf+W^TV_\lambda ^{-1}J_p^Tf
\label{reduce-system-equation}
\end{equation}
\begin{equation}
\Delta x_p=-V_\lambda^{-1}(J_p^Tf+W^T\Delta x_c)
\label{reduce-system2-equation}
\end{equation}
$S$是对称正定矩阵，使用Cholesky分解可以求解式~\ref{reduce-system-equation}。以上求解BA问题的方法之所以有效，是由于相机数量要远小于3D点数量，所以可以先求解$\Delta x_c$，再求解$\Delta x_p$。
\subsection{最小化相机中心位置误差}
上节介绍了使用舒尔补的方法简化线性系统，求解使重投影误差最小的$\Delta x_c,\Delta x_p$，其中涉及到的变量有旋转、相机中心（平移）、3D点坐标，而本节将介绍一种优化方法：最小化相机中心位置误差。通过图片我们可以提取相机的EXIF（Exchangeable Image File Format）信息，从而得到相机的经纬度坐标。EXIF是由数码相机制造商在图像、音频中标记的相机、频率等标准格式文件。EXIF包含的信息丰富，可以从中找到相机拍摄时所处的经纬高信息，也可以从中提取相机模型、相机位姿等。

同一个3D点在两视图中的左视图的相机坐标系中坐标为$X_1$，在右视图的相机坐标系中坐标为$X_2$，已知两视图相机位姿关系$R,t$，则坐标可以表示为$RX_1+t=X_2$。对于右视图相机中心这一3D点，有$X_2=0$，则该点在左视图的相机坐标系中的坐标为$X_1=-R^Tt$，旋转矩阵是正交矩阵有$R^T=R^{-1}$。 所以在多视图中每个视图与第一个视图的位姿关系为$R_i,t_i$，则这些视图对应的相机中心坐标在第一视图的相机坐标系中的坐标为$-R_i^Tt_i$。这样我们就建立每个视图的经纬高$pos=(Lon_i,Lat_i,Hig_i)$与其相对于第一视图的位姿关系$-R_i^Tt_i$，基于此我们可以建立最小二乘问题如式~\ref{min-position-equation}。
\begin{equation}
(R^*,t^*)=arg\mathop{{min}}\limits_{R,t} \sum_{i=1}\left \|f_i(x) \right \|
=arg\mathop{{min}}\limits_{R,t} \sum_{i}(\left \| -R_i^Tt_i-pos \right \|_2^2)
\label{min-position-equation}
\end{equation}
相对于最小化重投影误差的过程，该过程只调整相机相关参数，没有涉及3D点相关的调整，所以只作为BA的辅助步骤，也可以认为这属于光束平差法的一部分。光束平差法的实现依赖开源库Ceres Solver，无论是最小化重投影误差或最小化相机中心位置，其在Ceres Solver 中实现的方式是相同的。该库通过C++结构体设置输入输出参数，通过C++模板设置残差函数，库函数根据指定的输入、输出参数个数等信息自动完成差分求解雅可比矩阵过程。

\begin{figure}[!htbp]
\centering
\subfigure[BA前重投影效果图]{
\includegraphics[width=3.0in]{chapter3/BA/before_small.jpg}}
\subfigure[BA后重投影效果图]{\includegraphics[width=3.0in]{chapter3/BA/after_small.jpg}
}
\caption{BA对比结果图}
\label{BA}
\end{figure}
本节介绍了稀疏点云生成过程，包括多视图几何知识从匹配点恢复结构，而后增量式的重建其他图片；在每加入一张图片时，使用BA优化相机位姿与3D 点的位置，同时最小化相机中心位置误差。图~\ref{BA}(b)是使用CeresSolver库迭代三次优化重投影误差的结果，同时BA后移除重投影误差大于14个像素的3D点（外点）得到以下结果。
初始重投影误差代价为$34.83778$平方像素，BA优化后的代价为$17.32320$平方像素，优化后不存在重投影误差大于14个像素的外点，故没有移除3D点。将BA前的3D 点与BA 优化后的3D 点重新投回到成像平面上，结果如~\ref{BA}所示。两张图对比可以看出BA 最小化重投影误差的作用，(a)仅使用多视图几何计算相机位姿并使用三角测量恢复3D 点位置，而(b) 增加BA 环节，图中橙色空心圆点是提取的2D特征点坐标，蓝色实心圆点是3D 点根据相机$R,t$ 关系投影到成像平面的点，从中可以看出BA 对重投影误差有明显改善作用，可以去除误差较大的3D 点并且可以将误差均摊到所有3D点上。

\section{稀疏点云的稠密化}
通过SfM方法，不仅可以得到相机位姿，也可以恢复特征点的三维坐标得到稀疏点云，但是由于点云过于稀疏难以用于UGV导航与规划，所以本文使用Multiple View Stereo（MVS）技术将点云稠密化，MVS可以分为基于体素（voxel），基于表面演化（surface evolution）、基于特征点生长（feature point growing）和基于深度图融合（depth-map merging）四种类型\cite{Shen-Accurate}。本文采用深度图融合的方法\cite{Furukawa-Accurate}得到稠密点云，该方法包括深度图计算与深度图融合两部分，可以用图~\ref{flow-dense-whole}表示整个过程。

\begin{figure}[!htbp]
\centering
\includegraphics[width=4.9in]{chapter3/flow-dense.png}
\caption{点云稠密化流程图}
\label{flow-dense-whole}
\end{figure}

\subsection{深度图计算}
基于SfM方法，我们可以得到图像数据集下所有图像对应的相机位姿关系与特征点3D坐标，利用这些位姿关系，可以完成接下来的深度图计算过程。对于单个图像无法计算深度图，深度图的计算首先需要为每张图像选取其参考图（Reference Images），而后根据参考图与自身的旋转平移关系（通过SfM得到），计算每张图像的深度图。双目相机是基于双目成像互为参考图从而计算得到深度图；然而对于无序的图像数据集，参考图的选取至关重要。本文参考Shen\cite{Shen-Accurate}的方法设置参考图阈值，假设有$n$张图像，$\theta_{ij}, j=1,...,n$表示第$i$张图像与其他图像$1,..,n$的相机主轴夹角，$\theta_{ij}$用$i$ 与$j$ 图片匹配的特征点之间夹角的平均值表示。同时每张图像与其参考图对应相机的距离称为基线，记为$d_{ij}$，基线的距离太长则两张图像重叠区域太小，太短则使重建精度降低，选择满足$5^{\circ}<\theta_{ij}<60^\circ$，$0.05\bar{d}<d_{ij}<2\bar{d}$（$\bar{d}$ 表示所有图像平均基线距离）的图像作为参考图，按照$\theta_{ij}\dot d_{ij}$ 降序排序参考图，取前10张（如果有）作为参考图。

根据Bleyer\cite{Bleyer-PatchMatch}的方法，使用上文刷选的参考图计算深度图，主要思想是遍历图像的每个像素，为其找到一个支持面（Patch），使该支持面在参考图中有最小汇总匹配代价，整个过程如图~\ref{patch}所示。其中支持面$f$由法线$n_i$与3D点$X_i$决定，两视图几何中，以一个相机的相机坐标系作为世界坐标系，则有$P=[I_{3\times3} | \ 0_{3\times 1}]$，$P^\prime=[R|\ t]$，空间平面的齐次表达式为$\pi^T X=0$，其中$\pi=(V^T,1)^T,V^T=(V_1,V_2,V_3)$，由于$X_i$在面$\pi$上，所以有$\left|V^T\right|=\left|\frac{1}{X_i}\right|$，而方向与$n_i$相反，故$V^T=-\frac{n_i^T}{n_i^TX_i}$。

相机$i$由${K_i,R_i,C_i}$表示，参考图对应的相机用${K_j,R_j,C_j}$表示，现在设世界坐标系中心为相机$i$，则相机坐标系间的旋转平移关系变为$P_i=K_i[I_{3\times3} | 0_3]$，$P_j=K_j[R_jR_i^{-1}|R_j(C_i-C_j)]$
根据吴福朝\cite{wu-jisuanji} 有两视图间的单应性可以用式~\ref{homodouble-equation} 表示。
\begin{equation}
H=K_j(R-tV^T)K_i^{-1}=K_j(R_jR_i^{-1}+R_j\frac{(C_i-C_j)n_i^T)}{n_i^TX_i}K_i^{-1}
\label{homodouble-equation}
\end{equation}

\begin{figure}[!htbp]
\centering
\includegraphics[width=4.0in]{chapter3/mvs/patch.png}
\caption{最小汇总匹配代价计算过程}
\label{patch}
\end{figure}

\begin{figure}[!htbp]
\centering
    \subfigure[]{
    \label{depth-1}
    \includegraphics[width=1.9in]{chapter3/mvs/DJI_0056.JPG}
    }
    \subfigure[]{
    \label{depth-2}
    \includegraphics[width=1.9in]{chapter3/mvs/DJI_0056_depth.jpg}
    }
    \subfigure[]{
    \label{depth-3}
    \includegraphics[width=1.9in]{chapter3/mvs/DJI_0056_score.jpg}
    }
    \subfigure[]{
    \label{depth-4}
    \includegraphics[width=1.9in]{chapter3/mvs/DJI_0100.JPG}
    }
    \subfigure[]{
    \label{depth-5}
    \includegraphics[width=1.9in]{chapter3/mvs/DJI_0100_depth.jpg}
    }
\subfigure[]{
\label{depth-6}
    \includegraphics[width=1.9in]{chapter3/mvs/DJI_0100_score.jpg}
}
\caption{深度图与NCC值}
\label{depth}
\end{figure}

图~\ref{patch}中支持面$f$以$p$为中心，大小为$7\times 7$像素的正方形块，对于Patch中的每个像素$q$通过式~\ref{homodouble-equation}转换到参考图中，记为$H_{ij}$，此时可以求出$7\times7$所有像素$q$的代价和$m(p,f_p)$，称为去均值的归一化互相关（NCC, Normalized Cross Correlation），如式~\ref{matchingCost-equation}所示，其中上划线表示所有像素点的平均值。

\begin{equation}
m(p,f_p)=1-\frac{\sum\limits_{q\in patch}(q-\bar{q})(H_{ij}(q)-\overline{H_{ij}(q)})}{\sqrt{\sum\limits_{q\in patch}(q-\bar{q})^2\sum\limits_{q\in patch}(H_{ij}(q)-\overline{H_{ij}(q)})^2}}
\label{matchingCost-equation}
\end{equation}

据此可计算数据集中每张图像的深度图与图像中每个像素对应的NCC值，如图~\ref{depth}所示，图$(b),(e)$为深度图，图$(c),(f)$为NCC值图，图中使用冷暖色表示像素值的大小，颜色越暖表示像素值越大。图~\ref{depth-3} 较冷（蓝色）的区域表示用来计算深度图的图像与参考图对应Patch区域像素误差较大，当大于一定阈值时，证明该区域的深度计算是不准确的，故对应图~\ref{depth-5}的深度设为零，为最浅色（蓝色）部分。对于深度图而言颜色越暖证明深度越深，图~\ref{depth-5}很好地反映了一面竖直墙导致图像深度的变化情况。

\subsection{深度图融合}
在得到每张图像的深度图之后，需要将各个深度图融合得到完成的场景图，由于图像存在重叠区域，所以直接融合深度图会在重叠区域产生深度冗余，所以深度图的融合就是去除冗余深度的过程。将图$i$每个像素点$x$通过上文计算得到的深度重新投影到3D空间为$X=\lambda R_i^TK_i^{-1}x + C_i$，而$i$ 的参考图$j=1,..h$也可以得到对应图$i$中每个像素的深度，这个深度可以通过参考图的深度图计算得到，记为$\lambda(X,j)$，对于依赖图$i$的深度图得到的坐标$X$可以转换到参考图的相机坐标系中得到$X$对应的像素深度$d(X,i)$，如果$d(X,i)<\lambda(X,j)$或$\frac{\left|d(X,i)-\lambda(X,j)\right|}{\lambda(X,j)}$，则移除参考图中该像素的深度。
\section{本章小结}
鉴于三维地图可以提供更丰富的地理空间信息，在UGV仿真、实验中有着广阔的应用前景，本章提出一种基于航拍图像的三维地图构建算法。在特征点提取与匹配过程中，采用基于SIFT特征点提取和描述的方法，结合对极约束算法与特征点track序号的逻辑关系，有效地过滤了错误匹配点。在算法的位姿优化过程中，相对于传统的最小化重投影误差，增加了基于相机地理位置的最小化相机中心距离，优化相机的旋转与平移矩阵。在稀疏点云的稠密化中，针对航拍图像分辨率较高的特点，采用基于块的点云稠密方法，以去均值的NCC作为块匹配代价函数，得到数据集中每张图像中每个像素的深度，由于图像存在重叠区域，所以深度图的融合也存在冗余深度的问题，通过精炼深度图，从而去除冗余或错误的深度值，从而完成深度图的融合，得到稠密点云。
