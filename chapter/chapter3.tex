\chapter{基于无人机航拍图像的三维构建}

\section{引言}
大部分无人车辆行驶依赖环境地图导航与感知\cite{Mattyus-Enhancing-road-maps}，这限制了无人车在未知环境中的行驶。为了解决该问题，当前许多技术利用车载传感器可以构建很大范围的地图，例如SLAM(Simultaneous Localization and Mapping) 和第二章提到的用车载Ladybug构建地图的方法。然而车载传感器仅能获取车辆周边环境信息且易被阻挡，而且在野外或灾害地区等环境中，车辆可通行区域变少。然而无人机拥有比无人车更广阔的视野，不受地面障碍物的影响，可以灵活地从空中俯视地面环境。本章介绍了应用无人机构建三维环境地图，为无人车导航提供先验信息。

三维重建涉及到的内容可以分为三部分：稀疏点云的获取，点云稠密化与点云的网格渲染。其中稀疏点云的获取使用的是Structure from Motion(SfM)，在技术层面上与当前热门的Visual-SLAM(VSLAM)比较类似，只是SfM 侧重于三维重建，而SLAM侧重于实时定位，并需要预测下一时刻的位置。SfM可以分为视觉前端与优化后端，前端涉及到的是相机模型，坐标转换，特征提取与匹配以及多视图几何相关的内容，最后得到粗略的相机位姿与3D点坐标，后端涉及到的是优化前端得到的相机位姿与3D点坐标，去除错误的结果等。点云稠密使用的技术称为Multi-View Stereo(MVS)，MVS的方法很多，本文使用的是深度图融合的方法。网格渲染主要用的技术为三角剖分，网格渲染后的三维重建结果比稠密点云存储量小，更易于后续的仿真与处理。
\section{相机模型}
数码相机拍摄的过程中，实际上是一个光学成像的过程，这涉及到摄像机最基本的原件――透镜――的成像原理，如图~\ref{len}所示，这是最基本的透镜成像原理：$Z$是物体距透镜光心的距离，简称物距；$f$是焦距；$b$是相距，即成像平面与透镜光心的距离。三者满足式~\ref{len-equation}
\begin{equation}
\frac{1}{f} = \frac{1}{Z} + \frac{1}{b}
\label{len-equation}
\end{equation}

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter3/focal.png}
	\caption{透镜成像原理}
	\label{len}
\end{figure}
在机器视觉中，利用摄像机可以将三维场景记录在二维图形上，不同的相机模型导致不同的成像效果，比如第二章中Ladybug使用的球面成像模型，使不同相机的拼接简化为绕光心的旋转。但是常见的相机使用的模型还是小孔成像模型，如图~\ref{pinhole}所示。数码相机的镜头相当于一个凸透镜，感光元件就处在这个凸透镜的焦点附近，将焦距近似为凸透镜中心到感光元件的距离时就成为小孔成像模型。
这可以类比为艺术家画一幅画的过程，将眼睛放在图~\ref{pinhole}光心$C$处，在物体与眼睛之间放置一个画布（图~\ref{pinhole}中虚像位置），按照光线直线传播的原则，则物体反射的光线与眼睛的连线交画布一点，如此物体可以在画布上成像。只是相机的成像在光心后面，所以说存在一个虚拟成像平面。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter3/pinhole.png}
	\caption{小孔成像模型}
	\label{pinhole}
\end{figure}

在三维空间中，物体反射的光线经过小孔形成倒立二维图像，根据其数学模型，可以得到
\begin{equation}
\frac{-x}{X} = \frac{f}{Z}
\label{pinhole-equation}
\end{equation}

其中，$f$是小孔到成像平面的距离，即焦距；$Z$是物体距光心的距离，简称物距；$x$是物体在成像平面的投影长度；$X$是物体实际长度。为了简化以上模型，用图~\ref{firstperson}表示小孔成像的等价模型，其中$C$是相机中心，$P(X,Y,Z)$为空间中的3D点，$p(x,y,1)$表示3D点成像在感光元件上的点，小孔成像近似后，成像平面与相机中心的距离为焦距$f$。该图也表示了两个坐标系，分别为图像坐标系和像素坐标系，在下节中详细介绍。
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter3/firstperson.png}
	\caption{小孔成像等价模型}
	\label{firstperson}
\end{figure}

\section{相关坐标系}
从上一节知道，仅仅相机内部就存在多个坐标系，那么摄像机在空间中涉及到的坐标系转换问题将在这节详细探讨。相机成像的过程是一个3D物体显示在2D成像平面的过程，在此涉及物体，真实空间与相机三者的位置关。物体在真实空间的位置即是物体在世界坐标系中的位置坐标表达，在图~\ref{firstperson}中，$(X,Y,Z)$为3D点$P$在相机坐标系（$X_c,y_c,Z_c$）下的坐标表示，这里就存在两个三维坐标的转换，即同一个点在世界坐标系与相机坐标系下表达方式的转换关系，如式\ref{R-t-equation}，该变换可以看做三维坐标系的刚体变换。
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
^{c}P = ^{c}R_w ^w P + ^c T_w
\label{R-t-equation}
\end{equation}
其中$c$表示相机坐标系，$w$表示世界坐标系，$^w P$表示世界坐标系下$P$点3D坐标，$^c P$是相机坐标系下$P$点3D坐标，$^{c}R_w$和$^c T_w$分别表示由世界坐标系到相机坐标系的旋转矩阵和平移矩阵。

将世界坐标系下的3D点表示为齐次坐标$(X,Y,Z,1)$，式\ref{R-t-equation}可以改写为：
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
^{c}P=\left[\begin{array}{ccc}
^{c}R_w & ^c T_w \\
0^{1\times 3} & 1 \\
\end{array}
\right]_{4\times 4}
\left[\begin{array}{ccc}
X_w \\
Y_w \\
Z_w \\
1 \\
\end{array}
\right]
\label{Rt-matrix-equation}
\end{equation}
其中$^{c}R_w$是$3\times 3$矩阵，三个列向量俩俩正交；$^c T_w$是$3\times 1$矩阵，代表两个坐标系原点连线的向量，$^{c}P$坐标表示为$(x_c,y_c,z_c)$。

从图\ref{firstperson}中可以看到成像平面内的二维坐标系$x^\prime ,y^\prime$，该二维坐标系称为图像坐标系，可以推导相机坐标系与图像坐标系（3D到2D）变换的公式表示：
\begin{equation}
x\prime = f\frac{x}{z}
\label{camera2img-equation}
\end{equation}
\begin{equation}
y\prime = f\frac{y}{z}
\label{camera2img-equation}
\end{equation}
改为矩阵表达形式为
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
z_c \left[\begin{array}{ccc}
x\prime \\
y\prime \\
1 \\
\end{array}
\right]=\left[\begin{array}{cccc}
f & 0 & 0 & 0 \\
0 & f & 0 & 0 \\
0 & 0 & 1 & 0 \\
\end{array}
\right]
\left[\begin{array}{ccc}
x_c \\
y_c \\
z_c \\
1 \\
\end{array}
\right]
\label{camera2img-matrix-equation}
\end{equation}

相机在生成图片的过程中，由于成像平面由许多CCD感光元件组成，所以成像的横纵坐标是不连续的，引入像素的概念，同时也存在从图像坐标系到像素坐标系（2D到2D）的转换关系：
\begin{equation}
\left[\begin{array}{ccc}
u \\
v \\
1 \\
\end{array}
\right]=\left[\begin{array}{ccc}
\frac{1}{s_x} & s & p_x \\
0 & \frac{1}{s_y} & p_y \\
0 & 0 & 1 \\
\end{array}
\right]
\left[\begin{array}{ccc}
x\prime \\
y\prime \\
1 \\
\end{array}
\right]
\label{camera2pixel-equation}
\end{equation}
其中，$dx$是u轴方向单像素的宽度，$dy$是v轴方向单像素的宽度，当前大部分ccd的像素宽度，$dx=dy$，只有以前老式电视机使用的是矩形ccd，导致$dx\neq dy$。$p_x,p_y$是主点（图~\ref{firstperson}）与图像左边界的距离与$p_y$ 是主点与图像上边界的距离。$s$是衡量主光轴与成像平面的倾斜程度，如果主光轴垂直于成像平面，则$s = 0$。这些参数被称作相机内参，一般可以通过查询相机手册得到包括焦距在内的相机内参。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=4.0in]{chapter3/transform.png}
	\caption{坐标转换流程图}
	\label{transform}
\end{figure}
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter3/transform-coordinate.png}
	\caption{坐标转换关系图}
	\label{transform-coordinate}
\end{figure}
至此我们可以得到从世界坐标系到像素坐标系变化过程，如图~\ref{transform}和~\ref{transform-coordinate}。
图~\ref{transform}过程用公式表达为~\ref{transform-equation}
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
z_c \left[\begin{array}{ccc}
u \\
v \\
1 \\
\end{array}
\right]=\left[\begin{array}{ccc}
\frac{1}{s_x} & s & p_x \\
0 & \frac{1}{s_y} & p_y \\
0 & 0 & 1 \\
\end{array}
\right]
\left[\begin{array}{cccc}
f & 0 & 0 & 0 \\
0 & f & 0 & 0 \\
0 & 0 & 1 & 0 \\
\end{array}
\right]
\left[\begin{array}{ccc}
^{c}R_w & ^c T_w \\
0^{1\times 3} & 1 \\
\end{array}
\right]_{4\times 4}
\left[\begin{array}{cccc}
X_w \\
Y_w \\
Z_w \\
1 \\
\end{array}
\right]
\label{transform-equation}
\end{equation}
至此我们知道了相机成像过程中某点如何在不同坐标系下转换，从而完成摄像过程。式~\ref{transform-equation}涉及到相机参数$f,p_x,p_y$以及空间变换相关的参数（外参$R,t$），实际上在相机成像过程中除了这些参数，我们还会遇到无法线性建模的参数：像Gopro等鱼眼镜头中，我们可以看到世界中的直线不再是直线，如图。。。。
这些弯曲的线有一些特别的特性：他们都是被关于中心扭曲的 我们称这种畸变为径向畸变。这意味着像素点是沿着从中心放射的径向成比例扭曲的。为了去除相机的畸变，需要对畸变用多项式进行建模，如式~\ref{radial-equation}。

\begin{subequations}
\label{radial-equation}
\begin{align}
u^{dist} &= u(1+k_1 r+k_2 r^2+ k_3 r^3 + ...)  \\
v^{dist} &= v(1+k_1 r+k_2 r^2+ k_3 r^3 + ...)  \\
where \quad & r^2 = u^2 + v^2
\end{align}
\end{subequations}
其中，$k_1,k_2,k_3$是未知参数，需要标定才可以得到，而实际校正径向畸变时一般也只用到2个或3个参数，如图~\ref{distortion}所示，左图为存在径向畸变的图片，右图为经过校正后的图片。
\begin{figure}[!htbp]
	\centering
    \subfigure{
	\includegraphics[width=2.5in]{chapter3/radial-distortion.PNG}
    }
    \subfigure{
	\includegraphics[width=2.5in]{chapter3/non-radial-distortion.PNG}
    }
	\caption{径向畸变的校正}
	\label{distortion}
\end{figure}

在相机校正的过程中，实际上还需要校正图像的切向失真，这是由于摄像机安装时成像平面与透镜没有平行导致的，如果说径向畸变是坐标点距离中心点的长度放生了变化，那么切向畸变就是坐标点与中心点的水平夹角发生了变化。对切向畸变的建模如式~\ref{tangential-equation}，一般使用$p_1,p_2$两个参数。
\begin{subequations}
\label{tangential-equation}
\begin{align}
u^{dist} &= u + 2p_1 uv + p_2(r^2 + 2u^2) \\
u^{dist} &= u + 2p_2 uv + p_1(r^2 + 2v^2) \\
where \quad & r^2 = u^2 + v^2
\end{align}
\end{subequations}

综上所述，本节详细介绍了相机模型与相机相关坐标系转换关系，以及相机成像的校正等相关内容，这部分属于图像处理的基础，熟悉该部分内容将为下面几节多视图几何相关内容作铺垫。
\section{特征点提取}
SfM首先需要找到图片集中俩俩图片间的几何关系，这样才能估算相机间的运动与成像的三维点坐标。为了找到图片之间的几何关系，现今有许多方法，例如光流法，特征点法等，光流法适用于匀速运送的视频流中，要求帧间运动比较小，从而找出帧间运动关系；特征点法指对图像特征的提取与存储，是图像中比较有代表性的特征。由于本文使用的数据集为无序的航拍图像，特征点法较为适宜使用。特征点提取更通俗的理解为将二维矩阵描述的图像降维为一维特征点描述的图像，突出图像的重点，去除冗余信息的过程。

最简单的图像特征为图像的角点，角点就是图像中线的交点，例如大厦最高处的顶点。两幅图像的角点不会随着图像移动而改变，但是角点会随着离相机的距离太近而变得无法识别，所以许多研究者设计了精巧而互有优劣的特征检测与描述方法，比较著名的有SIFT(Scale Invariant Feature Transform)\cite{Lowe-DistinctiveImageFeatures}，SURF\cite{Bay-Speeded-UpRobustFeatures}，ORG\cite{Rublee-ORB:Anefficient}等。相对于简单的角点，这些特征检测具有以下优点\cite{slam14}:
\begin{itemize}
\item 可重复：相同的特征可以在不同的图片中找到
\item 可区别：不同的特征有不同的表示
\item 高效率：特征点数远远小于像素数
\end{itemize}

实际上，特征点可以分为局部特征点和全局特征点，像上面举例的三个特征点实际上是局部特征点，局部特征点顾名思义就是基于图像的突出区域明显的区分图像的特征，一般而言，局部特征需要具有旋转不变性，光线明暗的鲁棒性等。因此图像可以被提取到的一系列称为感兴趣区域的特征描述。相反全局特征是将图像表示为一个向量，向量的值为图像的各个方面，如颜色，纹理或形状。例如像区分图像，一些是海洋，一些是森林，一个全局颜色描述子可以很好的对其归类。在这种情况下，全局描述子涉及图像所有像素的特定属性，这个属性可以是颜色直方图，纹理，边缘等。如图~\ref{global-local}所示，使用哪种特征描述子取决于图像处理的情景。由于地理空间的航拍图像在颜色或者纹理等全局特征上，图片的相似度极高，不易辨识，难以找到图片之间的匹配关系，所以此文选用局部特征描述子：SIFT。
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/global-local.jpg}
\caption{全局特征与局部特征描述}
\label{global-local}
\end{figure}

特征点提取包括特征点检测与特征点描述两部分，在此以SIFT特征点为例，简单介绍一下检测子与描述子的使用方法。检测子有很多，使用较多的有Harris Detector\cite{Harris-corner}，FAST Detector\cite{Rosten-FAST}。Harris检测子结合边缘与角点检测方法得到图像不同方向的自相关系数变化率，例如强度变化率等，该方法将局部特征描述为对称的自相关系数矩阵。Fast检测子考虑某像素周围一定大小的圆，院上像素强度与该像素比较，如果大于或小于某一阈值则定义该像素为角点，这两种检测子对尺度变化都不具有不变性，不适合直接应用在拍摄高度不同的航拍图形的特征检测中。而SIFT作为一种具有尺度、放缩不变，对不同光线鲁棒的特定，很适合应用于环境复杂的航拍图形的特征特取与描述中。SIFT一般使用的是Difference of Gaussian作为检测子，使用Lowe提出的SIFT方法作为描述子。

Laplacian of Gaussian (LoG)是二阶导数的线性组合，也是检测斑点(blob)的检测子，给出一个图片$I(x,y)$,尺度空间$L(x,y,\sigma)$是由图片$I(x,y)$与不同大小的高斯核$G(x,y,\sigma)$卷积得到的，表示为

\begin{equation}
L(x,y,\sigma) = G(x,y,\sigma)*I(x,y)
\label{laplacian}
\end{equation}
其中，
\begin{equation}
G(x,y,\sigma) = \frac{1}{2\pi \sigma ^2}e^{1}
\label{laplacian}
\end{equation}
Laplacian算子使用下式
DoG(Difference of Gaussian)是Lowe提出的有效改善LoG算子耗时问题的特性


\section{特征点匹配}
\section{多视图重建}
在式~\ref{transform-equation}中，$z_c$为每个像素的深度，在单目成像时，$z_c$是未知的。所以当知道某点的世界坐标系时，可以很容易得到该点的像素坐标，但是反之无法已知像素坐标得到世界坐标。这将为我们下面
\subsection{两视图重建}
\subsection{2D-3D位姿求解}
\subsection{三角定位法}


\section{三维重建中的优化}
\subsection{光束平差法}
\subsection{最小化相机中心位置误差}

\section{稀疏电云的渲染}
\subsection{点云稠密化}
\subsection{点云网格渲染（三角剖分）}

\section{本章小结}
