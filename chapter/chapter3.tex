\chapter{基于无人机航拍图像的三维构建}

\section{引言}
大部分无人车辆行驶依赖环境地图导航与感知\cite{Mattyus-Enhancing-road-maps}，这限制了无人车在未知环境中的行驶。为了解决该问题，当前许多技术利用车载传感器可以构建很大范围的地图，例如SLAM(Simultaneous Localization and Mapping) 和第二章提到的用车载Ladybug构建地图的方法。然而车载传感器仅能获取车辆周边环境信息且易被阻挡，而且在野外或灾害地区等环境中，车辆可通行区域变少。然而无人机拥有比无人车更广阔的视野，不受地面障碍物的影响，可以灵活地从空中俯视地面环境。本章介绍了应用无人机构建三维环境地图，为无人车导航提供先验信息。

三维重建涉及到的内容可以分为三部分：稀疏点云的获取，点云稠密化与点云的网格渲染。其中稀疏点云的获取使用的是Structure from Motion(SfM)，在技术层面上与当前热门的Visual-SLAM(VSLAM)比较类似，只是SfM 侧重于三维重建，而SLAM侧重于实时定位，并需要预测下一时刻的位置。SfM可以分为视觉前端与优化后端，前端涉及到的是相机模型，坐标转换，特征提取与匹配以及多视图几何相关的内容，最后得到粗略的相机位姿与3D点坐标，后端涉及到的是优化前端得到的相机位姿与3D点坐标，去除错误的结果等。点云稠密使用的技术称为Multi-View Stereo(MVS)，MVS的方法很多，本文使用的是深度图融合的方法。网格渲染主要用的技术为三角剖分，网格渲染后的三维重建结果比稠密点云存储量小，更易于后续的仿真与处理。
\section{相机模型}
数码相机拍摄的过程中，实际上是一个光学成像的过程，这涉及到摄像机最基本的原件――透镜――的成像原理，如图~\ref{len}所示，这是最基本的透镜成像原理：$Z$是物体距透镜光心的距离，简称物距；$f$是焦距；$b$是相距，即成像平面与透镜光心的距离。三者满足式~\ref{len-equation}
\begin{equation}
\frac{1}{f} = \frac{1}{Z} + \frac{1}{b}
\label{len-equation}
\end{equation}

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter3/focal.png}
	\caption{透镜成像原理}
	\label{len}
\end{figure}
在机器视觉中，利用摄像机可以将三维场景记录在二维图形上，不同的相机模型导致不同的成像效果，比如第二章中Ladybug使用的球面成像模型，使不同相机的拼接简化为绕光心的旋转。但是常见的相机使用的模型还是小孔成像模型，如图~\ref{pinhole}所示。数码相机的镜头相当于一个凸透镜，感光元件就处在这个凸透镜的焦点附近，将焦距近似为凸透镜中心到感光元件的距离时就成为小孔成像模型。
这可以类比为艺术家画一幅画的过程，将眼睛放在图~\ref{pinhole}光心$C$处，在物体与眼睛之间放置一个画布（图~\ref{pinhole}中虚像位置），按照光线直线传播的原则，则物体反射的光线与眼睛的连线交画布一点，如此物体可以在画布上成像。只是相机的成像在光心后面，所以说存在一个虚拟成像平面。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter3/pinhole.png}
	\caption{小孔成像模型}
	\label{pinhole}
\end{figure}

在三维空间中，物体反射的光线经过小孔形成倒立二维图像，根据其数学模型，可以得到
\begin{equation}
\frac{-x}{X} = \frac{f}{Z}
\label{pinhole-equation}
\end{equation}

其中，$f$是小孔到成像平面的距离，即焦距；$Z$是物体距光心的距离，简称物距；$x$是物体在成像平面的投影长度；$X$是物体实际长度。为了简化以上模型，用图~\ref{firstperson}表示小孔成像的等价模型，其中$C$是相机中心，$P(X,Y,Z)$为空间中的3D点，$p(x,y,1)$表示3D点成像在感光元件上的点，小孔成像近似后，成像平面与相机中心的距离为焦距$f$。该图也表示了两个坐标系，分别为图像坐标系和像素坐标系，在下节中详细介绍。
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter3/firstperson.png}
	\caption{小孔成像等价模型}
	\label{firstperson}
\end{figure}

\section{相关坐标系}
从上一节知道，仅仅相机内部就存在多个坐标系，那么摄像机在空间中涉及到的坐标系转换问题将在这节详细探讨。相机成像的过程是一个3D物体显示在2D成像平面的过程，在此涉及物体，真实空间与相机三者的位置关。物体在真实空间的位置即是物体在世界坐标系中的位置坐标表达，在图~\ref{firstperson}中，$(X,Y,Z)$为3D点$P$在相机坐标系（$X_c,y_c,Z_c$）下的坐标表示，这里就存在两个三维坐标的转换，即同一个点在世界坐标系与相机坐标系下表达方式的转换关系，如式\ref{R-t-equation}，该变换可以看做三维坐标系的刚体变换。
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
^{c}P = ^{c}R_w ^w P + ^c T_w
\label{R-t-equation}
\end{equation}
其中$c$表示相机坐标系，$w$表示世界坐标系，$^w P$表示世界坐标系下$P$点3D坐标，$^c P$是相机坐标系下$P$点3D坐标，$^{c}R_w$和$^c T_w$分别表示由世界坐标系到相机坐标系的旋转矩阵和平移矩阵。

将世界坐标系下的3D点表示为齐次坐标$(X,Y,Z,1)$，式\ref{R-t-equation}可以改写为：
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
^{c}P=\left[\begin{array}{ccc}
^{c}R_w & ^c T_w \\
0^{1\times 3} & 1 \\
\end{array}
\right]_{4\times 4}
\left[\begin{array}{ccc}
X_w \\
Y_w \\
Z_w \\
1 \\
\end{array}
\right]
\label{Rt-matrix-equation}
\end{equation}
其中$^{c}R_w$是$3\times 3$矩阵，三个列向量俩俩正交；$^c T_w$是$3\times 1$矩阵，代表两个坐标系原点连线的向量，$^{c}P$坐标表示为$(x_c,y_c,z_c)$。

从图\ref{firstperson}中可以看到成像平面内的二维坐标系$x^\prime ,y^\prime$，该二维坐标系称为图像坐标系，可以推导相机坐标系与图像坐标系（3D到2D）变换的公式表示：
\begin{equation}
x\prime = f\frac{x}{z}
\label{camera2img-equation}
\end{equation}
\begin{equation}
y\prime = f\frac{y}{z}
\label{camera2img-equation}
\end{equation}
改为矩阵表达形式为
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
z_c \left[\begin{array}{ccc}
x\prime \\
y\prime \\
1 \\
\end{array}
\right]=\left[\begin{array}{cccc}
f & 0 & 0 & 0 \\
0 & f & 0 & 0 \\
0 & 0 & 1 & 0 \\
\end{array}
\right]
\left[\begin{array}{ccc}
x_c \\
y_c \\
z_c \\
1 \\
\end{array}
\right]
\label{camera2img-matrix-equation}
\end{equation}

相机在生成图片的过程中，由于成像平面由许多CCD感光元件组成，所以成像的横纵坐标是不连续的，引入像素的概念，同时也存在从图像坐标系到像素坐标系（2D到2D）的转换关系：
\begin{equation}
\left[\begin{array}{ccc}
u \\
v \\
1 \\
\end{array}
\right]=\left[\begin{array}{ccc}
\frac{1}{s_x} & s & p_x \\
0 & \frac{1}{s_y} & p_y \\
0 & 0 & 1 \\
\end{array}
\right]
\left[\begin{array}{ccc}
x\prime \\
y\prime \\
1 \\
\end{array}
\right]
\label{camera2pixel-equation}
\end{equation}
其中，$dx$是u轴方向单像素的宽度，$dy$是v轴方向单像素的宽度，当前大部分ccd的像素宽度，$dx=dy$，只有以前老式电视机使用的是矩形ccd，导致$dx\neq dy$。$p_x,p_y$是主点（图~\ref{firstperson}）与图像左边界的距离与$p_y$ 是主点与图像上边界的距离。$s$是衡量主光轴与成像平面的倾斜程度，如果主光轴垂直于成像平面，则$s = 0$。这些参数被称作相机内参，一般可以通过查询相机手册得到包括焦距在内的相机内参。

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=4.0in]{chapter3/transform.png}
	\caption{坐标转换流程图}
	\label{transform}
\end{figure}
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=5.0in]{chapter3/transform-coordinate.png}
	\caption{坐标转换关系图}
	\label{transform-coordinate}
\end{figure}
至此我们可以得到从世界坐标系到像素坐标系变化过程，如图~\ref{transform}和~\ref{transform-coordinate}。
图~\ref{transform}过程用公式表达为~\ref{transform-equation}
\begin{equation}
%\sideset{^a_b}{^c_d}\prod^{n}_{k=1}
z_c \left[\begin{array}{ccc}
u \\
v \\
1 \\
\end{array}
\right]=\left[\begin{array}{ccc}
\frac{1}{s_x} & s & p_x \\
0 & \frac{1}{s_y} & p_y \\
0 & 0 & 1 \\
\end{array}
\right]
\left[\begin{array}{cccc}
f & 0 & 0 & 0 \\
0 & f & 0 & 0 \\
0 & 0 & 1 & 0 \\
\end{array}
\right]
\left[\begin{array}{ccc}
^{c}R_w & ^c T_w \\
0^{1\times 3} & 1 \\
\end{array}
\right]_{4\times 4}
\left[\begin{array}{cccc}
X_w \\
Y_w \\
Z_w \\
1 \\
\end{array}
\right]
\label{transform-equation}
\end{equation}
至此我们知道了相机成像过程中某点如何在不同坐标系下转换，从而完成摄像过程。式~\ref{transform-equation}涉及到相机参数$f,p_x,p_y$以及空间变换相关的参数（外参$R,t$），实际上在相机成像过程中除了这些参数，我们还会遇到无法线性建模的参数：像Gopro等鱼眼镜头中，我们可以看到世界中的直线不再是直线，如图。。。。
这些弯曲的线有一些特别的特性：他们都是被关于中心扭曲的 我们称这种畸变为径向畸变。这意味着像素点是沿着从中心放射的径向成比例扭曲的。为了去除相机的畸变，需要对畸变用多项式进行建模，如式~\ref{radial-equation}。

\begin{subequations}
\label{radial-equation}
\begin{align}
u^{dist} &= u(1+k_1 r+k_2 r^2+ k_3 r^3 + ...)  \\
v^{dist} &= v(1+k_1 r+k_2 r^2+ k_3 r^3 + ...)  \\
where \quad & r^2 = u^2 + v^2
\end{align}
\end{subequations}
其中，$k_1,k_2,k_3$是未知参数，需要标定才可以得到，而实际校正径向畸变时一般也只用到2个或3个参数，如图~\ref{distortion}所示，左图为存在径向畸变的图片，右图为经过校正后的图片。
\begin{figure}[!htbp]
	\centering
    \subfigure{
	\includegraphics[width=2.5in]{chapter3/radial-distortion.PNG}
    }
    \subfigure{
	\includegraphics[width=2.5in]{chapter3/non-radial-distortion.PNG}
    }
	\caption{径向畸变的校正}
	\label{distortion}
\end{figure}

在相机校正的过程中，实际上还需要校正图像的切向失真，这是由于摄像机安装时成像平面与透镜没有平行导致的，如果说径向畸变是坐标点距离中心点的长度放生了变化，那么切向畸变就是坐标点与中心点的水平夹角发生了变化。对切向畸变的建模如式~\ref{tangential-equation}，一般使用$p_1,p_2$两个参数。
\begin{subequations}
\label{tangential-equation}
\begin{align}
u^{dist} &= u + 2p_1 uv + p_2(r^2 + 2u^2) \\
u^{dist} &= u + 2p_2 uv + p_1(r^2 + 2v^2) \\
where \quad & r^2 = u^2 + v^2
\end{align}
\end{subequations}

综上所述，本节详细介绍了相机模型与相机相关坐标系转换关系，以及相机成像的校正等相关内容，这部分属于图像处理的基础，熟悉该部分内容将为下面几节多视图几何相关内容作铺垫。
\section{特征点提取}
SfM首先需要找到图片集中俩俩图片间的几何关系，这样才能估算相机间的运动与成像的三维点坐标。为了找到图片之间的几何关系，现今有许多方法，例如光流法，特征点法等，光流法适用于匀速运送的视频流中，要求帧间运动比较小，从而找出帧间运动关系；特征点法指对图像特征的提取与存储，是图像中比较有代表性的特征。由于本文使用的数据集为无序的航拍图像，特征点法较为适宜使用。特征点提取更通俗的理解为将二维矩阵描述的图像降维为一维特征点描述的图像，突出图像的重点，去除冗余信息的过程。

最简单的图像特征为图像的角点，角点就是图像中线的交点，例如大厦最高处的顶点。两幅图像的角点不会随着图像移动而改变，但是角点会随着离相机的距离太近而变得无法识别，所以许多研究者设计了精巧而互有优劣的特征检测与描述方法，比较著名的有SIFT(Scale Invariant Feature Transform)\cite{Lowe-DistinctiveImageFeatures}，SURF\cite{Bay-Speeded-UpRobustFeatures}，ORG\cite{Rublee-ORB:Anefficient}等。相对于简单的角点，这些特征检测具有以下优点\cite{slam14}:
\begin{itemize}
\item 可重复：相同的特征可以在不同的图片中找到
\item 可区别：不同的特征有不同的表示
\item 高效率：特征点数远远小于像素数
\end{itemize}

实际上，特征点可以分为局部特征点和全局特征点，像上面举例的三个特征点实际上是局部特征点，局部特征点顾名思义就是基于图像的突出区域明显的区分图像的特征，一般而言，局部特征需要具有旋转不变性，光线明暗的鲁棒性等。因此图像可以被提取到的一系列称为感兴趣区域的特征描述。相反全局特征是将图像表示为一个向量，向量的值为图像的各个方面，如颜色，纹理或形状。例如像区分图像，一些是海洋，一些是森林，一个全局颜色描述子可以很好的对其归类。在这种情况下，全局描述子涉及图像所有像素的特定属性，这个属性可以是颜色直方图，纹理，边缘等。如图~\ref{global-local}所示，使用哪种特征描述子取决于图像处理的情景。由于地理空间的航拍图像在颜色或者纹理等全局特征上，图片的相似度极高，不易辨识，难以找到图片之间的匹配关系，所以此文选用局部特征描述子：SIFT。
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/global-local.jpg}
\caption{全局特征与局部特征描述}
\label{global-local}
\end{figure}

特征点提取包括特征点检测与特征点描述两部分，在此以SIFT特征点为例，简单介绍一下检测子与描述子的使用方法。检测子有很多，使用较多的有Harris Detector\cite{Harris-corner}，FAST Detector\cite{Rosten-FAST}。Harris检测子结合边缘与角点检测方法得到图像不同方向的自相关系数变化率，例如强度变化率等，该方法将局部特征描述为对称的自相关系数矩阵。Fast检测子考虑某像素周围一定大小的圆，院上像素强度与该像素比较，如果大于或小于某一阈值则定义该像素为角点，这两种检测子对尺度变化都不具有不变性，不适合直接应用在拍摄高度不同的航拍图形的特征检测中。而SIFT作为一种具有尺度、放缩不变，对不同光线鲁棒的特定，很适合应用于环境复杂的航拍图形的特征特取与描述中。SIFT算法由四个步骤组成：尺度空间极值检测，兴趣点（关键点）定位，方向计算以及兴趣点描述。第一个阶段是使用Difference of Gaussian(DoG)确认潜在的兴趣点，这里使用DoG代替Laplacian of Gaussian (LoG)算子以提高计算速度。

Laplacian of Gaussian (LoG)是二阶导数的线性组合，也是检测斑点(blob)的检测子，给出一个图片$I(x,y)$,尺度空间$L(x,y,\sigma)$是由图片$I(x,y)$与不同大小的高斯核$G(x,y,\sigma)$卷积得到的，表示为

\begin{equation}
L(x,y,\sigma) = G(x,y,\sigma)*I(x,y)
\label{laplacian}
\end{equation}
其中，
\begin{equation}
G(x,y,\sigma) = \frac{1}{2\pi \sigma ^2}e^{\frac{-(x^2 + y^2)}{2 {\sigma}^2}}
\label{laplacian}
\end{equation}

据此可计算Laplacian算子为
\begin{equation}
{\nabla}^2 L(x,y,\sigma)=L_{xx}(x,y,\sigma)+L_{yy}(x,y,\sigma)
\label{laplacian}
\end{equation}

这对于大小为$\sqrt{2\sigma}$的斑点有最强的响应，然而该算子严重依赖图像中斑点大小与用来平滑图像的高斯核大小，即大小不同的高斯核函数被用来寻找对应大小的blob。为了在一幅图像中自动检测不同大小的斑点，一种多尺度的方法被提出来\cite{Lindeberg-Feature detection}，其通过尺度不同的归一化的Laplacian算子对图片进行平滑从而在不同尺度空间找到大小不同的blob，尺度归一化的高斯核函数为式~\ref{nor-lap-equation}。

\begin{equation}
{\nabla}^{2}_{norm} L(x,y,\sigma)={\sigma}^2 L_{xx}(x,y,\sigma)+L_{yy}(x,y,\sigma)
\label{laplacian}
\end{equation}
大的$\sigma$对应图片的粗略特征，小$sigma$对应图片的精细特征。同时LoG算子是对称的，所以它具有blob旋转对其没有影响，但是LoG算子耗费计算资源，为了加速计算并在尺度空间检测到稳定的兴趣点，Lowe\cite{Lowe-DistinctiveImageFeatures}提出了高斯差分尺度空间，简称DoG scale-space。如式~\ref{gaussian}所示，利用不同尺度的搞死差分核函数与图像卷积生成。
\begin{equation}
\begin{split}
D(x,y,\sigma) &= (G(x,y,k\sigma)-G(x,y,\sigma)*I(x,y)) \\
&= L(x,y,k\sigma)-L(x,y,\sigma)
\label{gaussian}
\end{split}
\end{equation}

计算DoG的关键步骤是构建图像金字塔：对于某个图像，使用大小不同的高斯核与该图像卷积，可以得到模糊程度不同的但是图像长宽一致的图像，这些图像构成了一个八度(octave)，而后对这一系列图像进行降采样，降采样就是对octave中的图像隔行隔列采集像素，最后图像尺度(scale)变为原来大小的$\frac{1}{4}$，构成下一个octave。这样就避免了对不同尺度空间下的图像进行卷积，减少了计算。为了寻找尺度空间的极值点，在DoG尺度空间中，每个采样点要和其周围所有相邻点（8邻域中，一个像素点周围共有26个像素点）比较，若该点是极大值或者极小值则定位为兴趣点。整个过程可以用图~\ref{octave}描述。
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.4in]{chapter3/octave.jpg}
\caption{图像金字塔的构建}
\label{octave}
\end{figure}

检测到特征点后，需要定量的表示各个特征点，以为特征点匹配做准备。Lowe在其论文中提出用一个128维度的向量表示每个特征点，具体做法为：如图~\ref{siftDescriptor}，首先对每个关键点周围的$16\times 16$ 邻域内所有像素计算其梯度的幅值与方向，用直方图统计邻域像素的梯度方向，梯度直方图将$0~360^{\circ}$分为8个区间，对得到的直方图进行高斯平滑，以减少突变带来的影响。选取直方图的峰值作为该关键点处邻域梯度的主方向。所以在每个$4\times 4$的象限内，将每个像素的主方向加权到直方图的8个方向区间中的一个，计算一个梯度方向直方图，这样对于每个特征点可以形成$4\times 4 \times 8 = 128$ 维的描述子。

%%在此需要插入4张sift检测的图片，甚至是sift与surf特征点的比较框

至此基本介绍了本文使用的SIFT特征点提取的方法，详细介绍了特征的提取与描述方法，以上是使用SIFT特征点检测多张图像的结果，其中特征点主方向以及特征点的强度分别表示在图中的。。。
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/siftDescriptor.jpg}
\caption{SIFT描述子的构建}
\label{siftDescriptor}
\end{figure}


\begin{figure}[!htbp]
\centering
 \subfigure{
	\includegraphics[width=2.6in]{chapter3/testPho/1_res.png}
    }
 \subfigure{
	\includegraphics[width=2.6in]{chapter3/testPho/2_res.png}
    }
 \subfigure{
	\includegraphics[width=2.6in]{chapter3/testPho/3_res.png}
    }
 \subfigure{
	\includegraphics[width=2.6in]{chapter3/testPho/4_res.png}
    }
%\includegraphics[width=5.0in]{chapter3/siftDescriptor.jpg}
\caption{SIFT特征点检测结果}
\label{sift-result}
\end{figure}

\section{特征点匹配}
上面提到本文使用SIFT特征点作为关键点检测的方法，特征点提取并描述后，就应该找出各个图像中以特征点为依据的关系，即每张图像中的特征点如何与其他图像的特征点进行匹配，找到相近的特征点并去除错误的匹配的特征点。特征点匹配解决航拍图像之间的数据关联问题，将无序的图片数据集建立关联关系，为下文通过图像之间的关系，计算相机位姿与还原特征点的三维坐标提供基础。很好理解，一对匹配点就是实际空间的一个点，如果将所有图片对应同一个真实点的特征点连在一起，将使数据集中所有无序图片“有序化”。相同的，如果匹配点中有大量的错误匹配，那对后续的位姿估计与还原三维坐标都产生不利影响，所以如果去除错误匹配更是至关重要的问题。下面就这两方面展开讨论。

假设在图片集$I = \{I_i | i=1...N_I\}$中找到的局部特征点为$F_i=\{(x_j,f_j) | j=1...N_{F_i}\}$，其中$x_j$表示特征点位置坐标，$f_j$表示特征点描述子，如果是SIFT特征点，则$f_j$是128维向量。对于$F_i$与$F_j$的特征点匹配的方法，最简单的是使用暴力匹配，即每个$(x_j,f_j) | j=1...N_{F_i}$与$(x_j,f_j) | j=1...N_{F_j}$中特征点测量两个向量之间的距离，需要进行$N_{F_i} \times N_{F_j}$次距离比较。对于像SIFT这样的浮点描述子，一般采用欧拉距离作为衡量依据，而对于像BRIEF（ORB特征使用的描述子）二进制描述子，则一般使用汉明距离作为度量依据。

%插入一组SIFT匹配的图片，对应欧拉距离那句话

但是当图片长宽较大，分辨率较高时，图片特征点会很多，这时采用暴力匹配会导致效率很低，一般实际使用中采用近似最近邻(Approximate Nearest Neighbor, ANN)匹配方法。事实上，无论SIFT特征匹配还是数据库检索本质上是相同的，都是使用距离函数在高维矢量空间中寻找相近对象的过程。常用的方法试K近邻查询，设置查询点与正整数K，从另一个数据集中根据距离公式找到距离最近的K个数据，如果K=1，改方法变为最近邻查询算法。K近邻查询算法是通过构建KD-Tree或者R-Tree等实现的。在此不做详细介绍。

在此简单介绍近似最近邻的快速库（Fast Library for Approximate Nearest Neighbors, FLANN），FLANN具有一种内部机制，该机制可以根据数据本身选择合适的算法来处理数据集，基于FLANN的匹配非常准确、快速。FLANN在使用的时候需要配置两个参数：indexParams和searchParams。使用FLANN时，indexParams可以选择为LinearIndex、KTreeIndex、KMeansIndex、CompositeIndex和AutotuneIndex，其中KTreeIndex灵活且可被并行处理。对于searchParams，用来指定索引树被遍历的次数，即check值，该值越大则匹配的时间越长，当然也越准确。

当然即使用上面的方法，也无法保证所有的匹配都是正确的，Lowe\cite{Lowe-ObjectRecognition}在1999提出一种简单的方法可以剔除大部分的错误匹配。对于某个特征点向量，如果查询数据集中与其距离最近的特征点向量与次近的特征点向量的距离之比大于0.7，可以避免接近90\%的错误匹配，但是正确的匹配也因此变少。使用FLANN并采用Lowe提出的方法可以去掉大部分的错误匹配，如图~\ref{matches}所示。
%\begin{figure}
%\includegraphics[width=5.0in]{chapter3/matches.jpg}
%\caption{FLANN匹配}
%\label{matches}
%\end{figure}

以上提到的方法，匹配后仍然存在错误的匹配点，由于刚刚的匹配是单纯基于特征描述子的向量值，无法保证匹配的两个特征点对应相同的场景点。因此SfM使用投影几何的知识估计两张图像的特征点变换。依赖图像对的空间配置，不同投影描述图片间不同的几何关系。例如，单应性变换（又称射影变换\cite{Richard-MultipleView}）描述相机拍摄的二维图片之间纯旋转和平移变换。对极几何通过基本矩阵（Essential matrix）$E$和基础矩阵（Fundamental matrix）$F$ 描述移动相机的关系，并可以通过三焦张量扩展到三视图的关系。无论哪种变换关系，有效的变换可以满足大部分匹配点的几何关系，那么这种变换或者匹配点被认为是有效的。本文在删除错误匹配时，使用的是单应性变换，一下简单介绍一下单应性变换的详细内容。

单应性又称射影变换、保线变换，与坐标系无关。映射$h$是射影变换的充要条件是：存在一个$3\times 3$非奇异矩阵$H$，使得任意一个矢量$X$表示的坐标点都满足$h(x)=HX$。即
\begin{equation}
\left[\begin{array}{ccc}
x_{1}^{\prime} \\
x_{2}^{\prime} \\
x_{3}^{\prime} \\
\end{array}
\right]=\left[\begin{array}{ccc}
h_{11} & h_{12} & h_{13} \\
h_{21} & h_{22} & h_{23} \\
h_{31} & h_{32} & h_{33} \\
\end{array}\right]
\left[\begin{array}{ccc}
x_{1} \\
x_{2} \\
x_{3} \\
\end{array}\right]
\label{homography-equation}
\end{equation}

矩阵$H$是一个齐次矩阵，在$H$的九个元素中有八个独立比率，即交比(cross radio)$h_{11}:h_{12}:h_{13}:h_{21}:h_{22}:h_{23}:h_{31}:h_{32}:h_{33}$不会因为$H$因为乘以非零因子而变化，所以单应性变化有八个自由度，而每个二维特征点可提供2个方程，故需要解算$H$需要至少4对匹配点。已知4对匹配点，使用直接线性法解算$H$，示意图如图~\ref{homography}，已知两个视图四对匹配的特征点坐标$X_1,X_2$，求H 的过程。
\begin{figure}[!htbp]
\centering
\includegraphics[width=5.0in]{chapter3/homography.png}
\caption{单应性变换}
\label{homography}
\end{figure}
线性变换表示为$X_2=HX_1$，这是齐次矢量方程，三维矢量$X_2$和$HX_1$不相等，实际可以写为$\lambda X_2 = HX_1$，线性变换可以变形为$X_2 \times HX_1=0$（矢量与自身外积为零），记为$[X_2]_\times HX_1=0$，设$h_i$为$H$的第$i$ 行，

\begin{equation}
\begin{split}
\left[\begin{array}{ccc}
u_2 \\
v_2 \\
1 \\
\end{array}
\right]_\times
\left[\begin{array}{ccc}
h_1 \\
h_2 \\
h_3 \\
\end{array}\right]X_1 &=
\left[\begin{array}{ccc}
u_2 \\
v_2 \\
1 \\
\end{array}
\right]_\times
\left[\begin{array}{ccc}
h_1 X_1 \\
h_2 X_1 \\
h_3 X_1 \\
\end{array}\right]=
\left[\begin{array}{ccc}
u_2 \\
v_2 \\
1 \\
\end{array}
\right]_\times
\left[\begin{array}{ccc}
X_1^T h_1^T \\
X_1^T h_2^T \\
X_1^T h_3^T \\
\end{array}\right] \\
&=
\left[\begin{array}{ccc}
0 & -1 & v_2 \\
1 & 0 & -u_2 \\
-v_2 & u_2 & 0 \\
\end{array}\right]
\left[\begin{array}{ccc}
X_1^T & 0_{1\times 3} & 0_{1\times 3} \\
0_{1\times 3} & X_1^T & 0_{1\times 3} \\
0_{1\times 3} & 0_{1\times 3} & X_1^T \\
\end{array}\right]
\left[\begin{array}{ccc}
h_1^T \\
h_2^T \\
h_3^T \\
\end{array}\right]
\end{split}
\label{homography-tuidao-equation}
\end{equation}
最后一步是由线性代数的知识得到，即对3维矢量$x=(x_1,x_2,x_3)$的$3\times 3$的反对称矩阵为式~\ref{inverse-sym-equation}
\begin{equation}
[a]_\times=\left[\begin{array}{ccc}
0 & -a_3 & a_2 \\
a_3 & 0 & -a_1 \\
-a_2 & a_1 & 0 \\
\end{array}\right]
\label{inverse-sym-equation}
\end{equation}

合并~\ref{homography-tuidao-equation}最后一步的前两项得到下式为
\begin{equation}
=\left[\begin{array}{ccc}
0_{1\times 3} & -X_1^T & v_2 X_1^T \\
X_1^T & 0_{1\times 3} & -u_2 X_1^T \\
-v_2 X_1^T & u_2 X_1^T & 0_{1\times 3} \\
\end{array}\right]
\left[\begin{array}{ccc}
h_1^T \\
h_2^T \\
h_3^T \\
\end{array}\right]=0
\label{inverse-sym-equation}
\end{equation}

上式简写为$Ab=0$，其中$A$为$3\times 9$矩阵，$rank(A)=2$，这是因为每队匹配点只提供两个量：$u$和$v$，也可以从$A$矩阵的值看出，将$A$的第一行乘以$u_2$与第二行乘以$v_2$相加得到第三行。$b$为$9\times 1$ 矩阵，由于只需要维持$b$ 各项的交比不变即可，所以有8 个自由度，故需要4 对匹配点即可在这个线性系统上获得足够的条件计算得到单应性矩阵$H$的各项。由以上分析我们可以知道当给定的匹配点为4个时，并将$||H||=1$，方程有精确解，。但是大部分的匹配问题匹配点的数目远不止4 对，如果多于4对，那么$Ab=0$是超定的，如果所有的匹配点的位置是精确的，那么$Ab=0$的解仍然是精确的，但是通常情况匹配点中有噪声，存在错误匹配等问题，所以这时候方程存在最小二乘解（超定解）。除了零解可以忽略外，方程解可以使用SVD找到其近似解。

SVD又称奇异值分解，在此不详细介绍，SVD可以将一个矩阵，无论是否满秩，均可以分解为$A=UDV^T$形式，其中$UU_T=I$和$V V_T=I$，即$U$和$V$是正交矩阵，$D$是对角阵，对角元素非负。实际上SVD可以看做特征值分解的推广，SVD在主成分分析，求解方程的最小二乘解中广泛应用。回到求解单应性变换的问题来，根据SVD求解最小二乘解的原理，可以得到：最小二乘解是$A^T A$的最小特征值的特征向量。具体讲，假设有n个匹配点，则$A$为$2n\times 9$的矩阵，且$A=UDV^T$，且$D$对角阵元素按降序排列，那么$b$是$V$的最后一列。这是由于$Ab=0$的解，即$A$的零空间，又由于$rank(A)=n-1$，所以$A$的零空间只有一个基向量，即SVD中最小特征值对应的特征向量$V$的最后一列。

奇异值分解是广泛用于图像处理中的一种算法，不仅在求单应性矩阵时使用，下文亦有涉及。但是这里要介绍一种本文使用到的另一种方法――RANSAC(Random Sample Consensus)，该方法在图像领域中也广泛应用，其根本原理是使用统计原理，可以通过多次迭代估计含有噪声的数据模型的参数。RANSAC中文译为随机抽样一致性，其一般过程为从一组含有外点（噪声）的数据中随机选取数个数据，估计模型参数，然后将估计得到的模型应用于剩余的其他数据，从而得到该模型的误差，重复以上过程，直到误差达到某设定阈值或迭代次数达到设定最大次数，停止迭代，选取最优的一组模型作为最终结果。根据最终结果与事先设定的阈值可以过滤一部分外点并得到较为精确的模型。由以上内容知，估计单应性变换需要4对匹配点即可计算一个单应性变换，所以RANSAC每次选取4对匹配点迭代计算得到单应性矩阵，而后将单应性矩阵应用于剩余的匹配点，计算误差，过滤外点。如图~\ref{Homography+RANSAC}所示，其中$a,b$是仅根据SIFT描述子通过最近邻匹配过滤的结果。$c,d$是使用RANSAC方法通过单应性变换过滤的结果，从图中可以直观看出使用RANSAC可以过滤掉一部分错误的匹配，同样从表~\ref{table:matches}中也可以看出，其中对角数字表示图~\ref{Homography+RANSAC}中三张图片提取到的SIFT特征点数量，其他表格数目，例如$55/32$表示使用RANSAC方法前得到的匹配点数目，以及使用RANSAC后过滤剩下的匹配点。

\begin{figure}[!htbp]
\centering
 \subfigure[]{
	\includegraphics[width=2.8in]{chapter3/matches/000400_H_000415.png}
    }
 \subfigure[]{
	\includegraphics[width=2.8in]{chapter3/matches/000400_H_000424.png}
    }
 \subfigure[]{
	\includegraphics[width=2.8in]{chapter3/matches/000415_H_000424.png}
    }
 \subfigure[]{
	\includegraphics[width=2.8in]{chapter3/matches/000400_U_000415.png}
    }
 \subfigure[]{
	\includegraphics[width=2.8in]{chapter3/matches/000400_U_000424.png}
    }
 \subfigure[]{
	\includegraphics[width=2.8in]{chapter3/matches/000415_U_000424.png}
    }
%\includegraphics[width=5.0in]{chapter3/siftDescriptor.jpg}
\caption{RANSAC筛选的单应性变换结果}
\label{Homography+RANSAC}
\end{figure}

\begin{table}[!htbp]
	\caption{匹配点过滤}
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		\diagbox[dir=SE]{\bfseries 图片名}{\bfseries 特征点}{\bfseries 图片名} & \bfseries 400 & \bfseries 415 & \bfseries 424\\
		\hline
%		\hline
		\bfseries 400&  $861$&  $55/32$ & $75/46$\\
		\hline
		\bfseries 415 & $ 空 $ & $370$ & $19/10$ \\
		\hline
        \bfseries 424 & $空$ & $空$ & $599$ \\
		\hline
	\end{tabular}
	\label{table:matches}
\end{table}

经过各种方法，得到的图片匹配点基本上是正确的，基于此我们可以对数据集中无序图片构建它们的关联结构，对于多张图片对应的同一个匹配点，设置统一标号，称为track。例如图片一中第125个特征点，图片二中第259个特征点，图片三中第6个特征点...匹配，实际上它们对应实际地理空间的同一个点，所以可以使用同一个track序号标记这些匹配点，这样做使整个数据集通过特征点匹配构成关联关系，为下面的多视图重建打下基础。

\section{多视图重建}
在式~\ref{transform-equation}中，$z_c$为每个像素的深度，在单目成像时，$z_c$是未知的。所以当知道某点的世界坐标系时，可以很容易得到该点的像素坐标，但是反之无法已知像素坐标得到世界坐标。所以从单张图像中无法还原场景的三维结构，所以下文介绍从两张存在特征点匹配的图片中恢复场景的三维结构。两视图几何最核心的原理是对极约束，下面将详细阐述对极约束的相关知识。如图~\ref{epipolor}所示，其中$C_1,C_2$表示两个摄像头同时看到三维世界中同一点$P$，在$C_1$成像平面的坐标为$X_1$，在在$C_2$成像平面的坐标为$X_2$，

\begin{figure}[!htbp]
\centering
\includegraphics[width=5.4in]{chapter3/epipolor.png}
\caption{对极几何约束}
\label{epipolor}
\end{figure}


\subsection{两视图重建}
\subsection{2D-3D位姿求解}
\subsection{三角定位法}


\section{三维重建中的优化}
\subsection{光束平差法}
\subsection{最小化相机中心位置误差}

\section{稀疏电云的渲染}
\subsection{点云稠密化}
\subsection{点云网格渲染（三角剖分）}

\section{本章小结}
